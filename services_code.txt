import os
import logging
from typing import Dict, Any
from ..utils.core_utils import ConfigValidator


class ConfigManager:
    """Centralized configuration management"""
    
    _instance = None
    _config = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._config is None:
            self._load_config()
    
    def _load_config(self):
        """Load and validate all configuration"""
        # Required environment variables
        required_env_vars = [
            'GOOGLE_SHEETS_ID',  # ‚Üê ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
            'LINE_CHANNEL_ACCESS_TOKEN',
            'LINE_CHANNEL_SECRET',
            'LINE_USER_ID'
        ]
        
        try:
            self._config = ConfigValidator.validate_required_env_vars(required_env_vars)
            
            # Add optional configs with defaults
            self._config.update({
                'DEBUG': os.getenv('DEBUG', 'false').lower() == 'true',
                'PORT': int(os.getenv('PORT', '8080')),
                'BINANCE_BASE_URL': 'https://api.binance.com/api/v3',
                'VERSION': os.getenv('VERSION', '2.0-refactored'),
                'GOOGLE_APPLICATION_CREDENTIALS': os.getenv('GOOGLE_APPLICATION_CREDENTIALS', '/app/credentials.json')
            })
            
            # Validate configuration
            self._validate_config()
            
            logging.info("‚úÖ Configuration loaded successfully")
            
        except Exception as e:
            logging.error(f"‚ùå Configuration error: {e}")
            raise
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value"""
        return self._config.get(key, default)
    
    def get_all(self) -> Dict[str, Any]:
        """Get all configuration"""
        return self._config.copy()
    
    def _validate_config(self):
        """Validate configuration values"""
        # Validate port range
        port = self._config.get('PORT')
        if not (1024 <= port <= 65535):
            raise ValueError(f"Invalid port: {port}. Must be between 1024-65535")
        
        # Validate Google Sheets ID format
        sheets_id = self._config.get('GOOGLE_SHEETS_ID')
        if not sheets_id or len(sheets_id) < 20:
            raise ValueError("Invalid Google Sheets ID")
        
        # Validate LINE tokens
        line_token = self._config.get('LINE_CHANNEL_ACCESS_TOKEN')
        if not line_token or len(line_token) < 50:
            raise ValueError("Invalid LINE Channel Access Token")
        
        logging.info("‚úÖ Configuration validation passed")
    
    def is_debug_mode(self) -> bool:
        """Check if debug mode is enabled"""
        return self._config.get('DEBUG', False)
    
    def get_binance_config(self) -> Dict[str, str]:
        """Get Binance API configuration"""
        return {
            'base_url': self._config['BINANCE_BASE_URL'],
            'timeout': 30,
            'rate_limit': 1200
        }
    
    def get_google_config(self) -> Dict[str, str]:
        """Get Google API configuration"""
        return {
            'sheets_id': self._config['GOOGLE_SHEETS_ID'],
            'credentials_path': self._config['GOOGLE_APPLICATION_CREDENTIALS']
        }
    
    def get_line_config(self) -> Dict[str, str]:
        """Get LINE Bot configuration"""
        return {
            'access_token': self._config['LINE_CHANNEL_ACCESS_TOKEN'],
            'secret': self._config['LINE_CHANNEL_SECRET'],
            'user_id': self._config.get('LINE_USER_ID')
        }import logging
import requests
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
import pandas as pd
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

from ..utils.core_utils import JSONManager, ErrorHandler
from ..utils.data_types import DataConverter
from .config_manager import ConfigManager

class DataManager:
    """Centralized data management - ‡∏£‡∏ß‡∏° DataUpdater + PriceFetcher"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.config = ConfigManager()
        self.json_manager = JSONManager()
        self.data_converter = DataConverter()
        
        # Cache management
        self.cache = {}
        self.price_cache = {}
        self.last_requests = {}
        
        # Rate limiting
        self.min_request_interval = 0.2  # 200ms between requests
        self.price_cache_timeout = 30    # 30 seconds for price cache
        
        # Setup requests session with connection pooling
        self._setup_session()
        
        self.logger.info("‚úÖ DataManager initialized")
    
    def _setup_session(self):
        """Setup requests session with connection pooling"""
        self.session = requests.Session()
        
        # Retry strategy
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=retry_strategy
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    @ErrorHandler.api_error_handler
    def get_current_prices(self, symbols: List[str]) -> Dict[str, float]:
        """Get current prices for multiple symbols"""
        try:
            binance_config = self.config.get_binance_config()
            symbols_param = '["' + '","'.join(symbols) + '"]'
            url = f"{binance_config['base_url']}/ticker/price"
            
            response = self.session.get(
                url, 
                params={'symbols': symbols_param}, 
                timeout=binance_config['timeout']
            )
            response.raise_for_status()
            
            prices = {}
            for item in response.json():
                price = float(item['price'])
                if self.data_converter.validate_price_data(price):
                    prices[item['symbol']] = price
            
            # Update cache
            now = time.time()
            for symbol, price in prices.items():
                self.price_cache[f"price_{symbol}"] = {
                    'price': price,
                    'timestamp': now
                }
            
            self.logger.info(f"Fetched {len(prices)} current prices")
            return prices
            
        except Exception as e:
            self.logger.error(f"Error fetching current prices: {e}")
            return {}
    
    def get_current_prices_cached(self, symbols: List[str]) -> Dict[str, float]:
        """Get current prices with intelligent caching"""
        now = time.time()
        fresh_prices = {}
        symbols_to_fetch = []
        
        # Check cache first
        for symbol in symbols:
            cache_key = f"price_{symbol}"
            if cache_key in self.price_cache:
                cached_data = self.price_cache[cache_key]
                if now - cached_data['timestamp'] < self.price_cache_timeout:
                    fresh_prices[symbol] = cached_data['price']
                else:
                    symbols_to_fetch.append(symbol)
            else:
                symbols_to_fetch.append(symbol)
        
        # Fetch missing prices
        if symbols_to_fetch:
            fetched_prices = self.get_current_prices(symbols_to_fetch)
            fresh_prices.update(fetched_prices)
        
        return fresh_prices
    
    def get_single_price(self, symbol: str) -> Optional[float]:
        """Get single symbol price with rate limiting"""
        now = time.time()
        last_request = self.last_requests.get(symbol, 0)
        
        if now - last_request < self.min_request_interval:
            time.sleep(self.min_request_interval - (now - last_request))
        
        try:
            binance_config = self.config.get_binance_config()
            url = f"{binance_config['base_url']}/ticker/price"
            
            response = self.session.get(
                url, 
                params={'symbol': symbol}, 
                timeout=binance_config['timeout']
            )
            response.raise_for_status()
            
            self.last_requests[symbol] = time.time()
            price = float(response.json()['price'])
            
            if self.data_converter.validate_price_data(price):
                # Update cache
                self.price_cache[f"price_{symbol}"] = {
                    'price': price,
                    'timestamp': time.time()
                }
                return price
            
            return None
            
        except Exception as e:
            self.logger.error(f"Error fetching price for {symbol}: {e}")
            return None
    
    def get_klines(self, symbol: str, interval: str, limit: int = 500) -> Optional[pd.DataFrame]:
        """Get klines data with caching"""
        cache_key = f"{symbol}_{interval}"
        
        # Check cache first
        if cache_key in self.cache:
            cached_data = self.cache[cache_key]
            if self._is_cache_valid(cached_data, interval):
                return cached_data['df']
        
        try:
            binance_config = self.config.get_binance_config()
            url = f"{binance_config['base_url']}/klines"
            params = {
                'symbol': symbol,
                'interval': interval,
                'limit': limit
            }
            
            response = self.session.get(
                url, 
                params=params, 
                timeout=binance_config['timeout']
            )
            response.raise_for_status()
            
            data = response.json()
            
            df = pd.DataFrame(data, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_asset_volume', 'number_of_trades',
                'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
            ])
            
            # Convert data types
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()
            
            # Validate DataFrame
            if not self.data_converter.validate_dataframe(df):
                self.logger.warning(f"Invalid DataFrame for {symbol} {interval}")
                return None
            
            # Update cache
            self.cache[cache_key] = {
                'df': df,
                'timestamp': datetime.now()
            }
            
            # Save to file for persistence
            self._save_to_file(symbol, interval, df)
            
            self.logger.debug(f"Loaded {len(df)} candles for {symbol} {interval}")
            return df
            
        except Exception as e:
            self.logger.error(f"Error fetching klines for {symbol}: {e}")
            return self._load_from_file(symbol, interval)
    
    def _is_cache_valid(self, cached_data: Dict, interval: str) -> bool:
        """Check if cached data is still valid"""
        now = datetime.now()
        cache_time = cached_data['timestamp']
        
        # Cache validity periods
        validity_periods = {
            '1m': timedelta(minutes=1),
            '5m': timedelta(minutes=5),
            '15m': timedelta(minutes=15),
            '1h': timedelta(hours=1),
            '4h': timedelta(hours=2),
            '1d': timedelta(hours=6)
        }
        
        validity_period = validity_periods.get(interval, timedelta(minutes=30))
        return (now - cache_time) < validity_period
    
    def _save_to_file(self, symbol: str, interval: str, df: pd.DataFrame):
        """Save data to JSON file"""
        try:
            month_str = datetime.now().strftime("%Y-%m")
            filename = f"data/candles/{symbol}_{interval}_{month_str}.json"
            
            data = {
                'symbol': symbol,
                'interval': interval,
                'timestamp': datetime.now().isoformat(),
                'data': df.to_dict('records')
            }
            
            # Convert numpy types before saving
            data = self.data_converter.convert_numpy_types(data)
            self.json_manager.save_json(data, filename)
            
        except Exception as e:
            self.logger.error(f"Error saving data to file: {e}")
    
    def _load_from_file(self, symbol: str, interval: str) -> Optional[pd.DataFrame]:
        """Load data from JSON file as fallback"""
        try:
            month_str = datetime.now().strftime("%Y-%m")
            filename = f"data/candles/{symbol}_{interval}_{month_str}.json"
            
            data = self.json_manager.load_json(filename)
            if not data or 'data' not in data:
                return None
            
            df = pd.DataFrame(data['data'])
            if df.empty:
                return None
                
            # Convert timestamp column
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Validate loaded data
            if self.data_converter.validate_dataframe(df):
                self.logger.info(f"Loaded fallback data for {symbol} {interval}")
                return df
            
            return None
            
        except Exception as e:
            self.logger.error(f"Error loading data from file: {e}")
            return None
    
    def clear_cache(self):
        """Clear all caches"""
        self.cache.clear()
        self.price_cache.clear()
        self.logger.info("Cache cleared")
    
    def get_cache_stats(self) -> Dict:
        """Get cache statistics"""
        return {
            'klines_cache_size': len(self.cache),
            'price_cache_size': len(self.price_cache),
            'last_requests_count': len(self.last_requests)
        }"""
Data Updater Service
‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• candle data ‡πÅ‡∏ö‡∏ö real-time ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ memory cache
"""

import json
import os
import time
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from threading import Thread, Lock
import requests

from config.data_config import DataConfig

logger = logging.getLogger(__name__)

class DataUpdater:
    """
    ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• candle data ‡πÅ‡∏ö‡∏ö real-time
    - ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å Binance API
    - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ memory cache ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß
    - ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö batch
    """
    
    def __init__(self):
        self.config = DataConfig
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Python/Trading-Bot-Updater'
        })
        
        # Memory cache: symbol -> timeframe -> candles
        self.cache = {}
        self.cache_lock = Lock()
        
        # Update tracking
        self.last_update = {}  # symbol_timeframe -> timestamp
        self.update_threads = {}  # timeframe -> thread
        self.running = False
        
        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        self.stats = {
            'total_updates': 0,
            'successful_updates': 0,
            'failed_updates': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        logger.info("DataUpdater initialized")
    
    def get_latest_candles(self, symbol: str, timeframe: str, limit: int = 100) -> List[Dict]:
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• candles ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å API
        
        Args:
            symbol: Trading pair
            timeframe: Timeframe
            limit: Number of candles to fetch
            
        Returns:
            Latest candle data
        """
        url = f"{self.config.BINANCE_BASE_URL}{self.config.KLINES_ENDPOINT}"
        
        params = {
            'symbol': symbol,
            'interval': timeframe,
            'limit': limit
        }
        
        try:
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            raw_data = response.json()
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
            formatted_data = []
            for candle in raw_data:
                formatted_candle = {
                    'open_time': int(candle[0]),
                    'open': float(candle[1]),
                    'high': float(candle[2]),
                    'low': float(candle[3]),
                    'close': float(candle[4]),
                    'volume': float(candle[5]),
                    'close_time': int(candle[6]),
                    'quote_volume': float(candle[7]),
                    'count': int(candle[8]),
                    'taker_buy_volume': float(candle[9]),
                    'taker_buy_quote_volume': float(candle[10])
                }
                formatted_data.append(formatted_candle)
            
            return formatted_data
            
        except Exception as e:
            logger.error(f"Failed to get latest candles for {symbol} {timeframe}: {e}")
            return []
    
    def load_cache_from_files(self, symbol: str, timeframe: str) -> List[Dict]:
        """
        ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Ç‡πâ‡∏≤ cache
        
        Args:
            symbol: Trading pair  
            timeframe: Timeframe
            
        Returns:
            Loaded candle data
        """
        candles = []
        current_date = datetime.now()
        
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        for i in range(3):
            target_date = current_date - timedelta(days=30 * i)
            file_path = self.config.get_file_path(symbol, timeframe, target_date)
            
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r') as f:
                        data = json.load(f)
                        file_candles = data.get('candles', [])
                        candles.extend(file_candles)
                        
                except Exception as e:
                    logger.error(f"Failed to load file {file_path}: {e}")
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° timestamp ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        candles.sort(key=lambda x: x['open_time'])
        max_candles = self.config.ANALYSIS_CANDLES.get(timeframe, 300)
        
        return candles[-max_candles:] if len(candles) > max_candles else candles
    
    def update_cache(self, symbol: str, timeframe: str, force_reload: bool = False):
        """
        ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô cache
        
        Args:
            symbol: Trading pair
            timeframe: Timeframe  
            force_reload: Force reload from files
        """
        cache_key = f"{symbol}_{timeframe}"
        
        with self.cache_lock:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á update ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if not force_reload:
                last_update_time = self.last_update.get(cache_key, 0)
                update_interval = self.config.UPDATE_INTERVALS.get(timeframe, 300)
                
                if time.time() - last_update_time < update_interval:
                    return  # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á‡πÄ‡∏ß‡∏•‡∏≤ update
            
            try:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á cache structure ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
                if symbol not in self.cache:
                    self.cache[symbol] = {}
                
                # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô cache)
                if timeframe not in self.cache[symbol] or force_reload:
                    self.cache[symbol][timeframe] = self.load_cache_from_files(symbol, timeframe)
                    self.stats['cache_misses'] += 1
                else:
                    self.stats['cache_hits'] += 1
                
                # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å API
                latest_candles = self.get_latest_candles(symbol, timeframe, limit=50)
                
                if latest_candles:
                    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï cache ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
                    existing_candles = self.cache[symbol][timeframe]
                    updated_candles = self.merge_candles(existing_candles, latest_candles)
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏Ñ‡πà‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
                    max_candles = self.config.ANALYSIS_CANDLES.get(timeframe, 300)
                    self.cache[symbol][timeframe] = updated_candles[-max_candles:]
                    
                    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï timestamp
                    self.last_update[cache_key] = time.time()
                    self.stats['successful_updates'] += 1
                    
                    logger.debug(f"Updated cache for {symbol} {timeframe}: {len(self.cache[symbol][timeframe])} candles")
                    
                else:
                    self.stats['failed_updates'] += 1
                    
                self.stats['total_updates'] += 1
                    
            except Exception as e:
                logger.error(f"Failed to update cache for {symbol} {timeframe}: {e}")
                self.stats['failed_updates'] += 1
    
    def merge_candles(self, existing: List[Dict], new: List[Dict]) -> List[Dict]:
        """
        ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• candles ‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡∏°‡πà
        
        Args:
            existing: Existing candle data
            new: New candle data
            
        Returns:
            Merged candle data
        """
        if not existing:
            return new
            
        if not new:
            return existing
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á dict ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö lookup ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß
        existing_dict = {candle['open_time']: candle for candle in existing}
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        for candle in new:
            existing_dict[candle['open_time']] = candle
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô list ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á
        merged = list(existing_dict.values())
        merged.sort(key=lambda x: x['open_time'])
        
        return merged
    
    def get_candles(self, symbol: str, timeframe: str, limit: int = None) -> List[Dict]:
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• candles ‡∏à‡∏≤‡∏Å cache
        
        Args:
            symbol: Trading pair
            timeframe: Timeframe
            limit: Number of candles to return (default: all)
            
        Returns:
            Candle data from cache
        """
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï cache ‡∏Å‡πà‡∏≠‡∏ô
        self.update_cache(symbol, timeframe)
        
        with self.cache_lock:
            try:
                candles = self.cache[symbol][timeframe]
                
                if limit:
                    return candles[-limit:]
                return candles
                
            except KeyError:
                logger.warning(f"No cache data for {symbol} {timeframe}")
                return []
    
    def get_latest_candle(self, symbol: str, timeframe: str) -> Optional[Dict]:
        """
        ‡∏î‡∏∂‡∏á candle ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        
        Args:
            symbol: Trading pair
            timeframe: Timeframe
            
        Returns:
            Latest candle or None
        """
        candles = self.get_candles(symbol, timeframe, limit=1)
        return candles[0] if candles else None
    
    def save_cache_to_files(self):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å cache ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        logger.info("Saving cache to files...")
        saved_count = 0
        
        with self.cache_lock:
            for symbol in self.cache:
                for timeframe in self.cache[symbol]:
                    try:
                        candles = self.cache[symbol][timeframe]
                        
                        if not candles:
                            continue
                        
                        # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° month
                        monthly_data = {}
                        
                        for candle in candles:
                            candle_date = datetime.fromtimestamp(candle['open_time'] / 1000)
                            month_key = f"{candle_date.year}-{candle_date.month:02d}"
                            
                            if month_key not in monthly_data:
                                monthly_data[month_key] = []
                            monthly_data[month_key].append(candle)
                        
                        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
                        for month_key, month_candles in monthly_data.items():
                            year, month = month_key.split('-')
                            file_date = datetime(int(year), int(month), 1)
                            file_path = self.config.get_file_path(symbol, timeframe, file_date)
                            
                            # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°
                            existing_data = []
                            if os.path.exists(file_path):
                                try:
                                    with open(file_path, 'r') as f:
                                        data = json.load(f)
                                        existing_data = data.get('candles', [])
                                except:
                                    pass
                            
                            # ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
                            merged_data = self.merge_candles(existing_data, month_candles)
                            
                            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
                            data = {
                                'symbol': symbol,
                                'timeframe': timeframe,
                                'updated_at': datetime.now().isoformat(),
                                'count': len(merged_data),
                                'candles': merged_data
                            }
                            
                            os.makedirs(os.path.dirname(file_path), exist_ok=True)
                            with open(file_path, 'w') as f:
                                json.dump(data, f, separators=(',', ':'))
                            
                            saved_count += 1
                            
                    except Exception as e:
                        logger.error(f"Failed to save cache for {symbol} {timeframe}: {e}")
        
        logger.info(f"Saved {saved_count} cache files")
    
    def start_auto_update(self, symbols: List[str] = None, timeframes: List[str] = None):
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        
        Args:
            symbols: List of symbols to update
            timeframes: List of timeframes to update
        """
        if symbols is None:
            symbols = self.config.PRIORITY_SYMBOLS
        if timeframes is None:
            timeframes = self.config.TIMEFRAMES
        
        self.running = True
        logger.info(f"Starting auto update for {len(symbols)} symbols, {len(timeframes)} timeframes")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ timeframe
        for timeframe in timeframes:
            thread = Thread(
                target=self._update_worker,
                args=(symbols, timeframe),
                daemon=True,
                name=f"UpdateWorker-{timeframe}"
            )
            thread.start()
            self.update_threads[timeframe] = thread
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        save_thread = Thread(
            target=self._save_worker,
            daemon=True,
            name="SaveWorker"
        )
        save_thread.start()
        
        logger.info("Auto update started")
    
    def _update_worker(self, symbols: List[str], timeframe: str):
        """Worker thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï timeframe ‡∏´‡∏ô‡∏∂‡πà‡∏á"""
        interval = self.config.UPDATE_INTERVALS.get(timeframe, 300)
        
        while self.running:
            try:
                for symbol in symbols:
                    if not self.running:
                        break
                    
                    self.update_cache(symbol, timeframe)
                    time.sleep(1)  # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á symbol
                
                # ‡∏£‡∏≠‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏£‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡πÑ‡∏õ
                time.sleep(interval)
                
            except Exception as e:
                logger.error(f"Error in update worker {timeframe}: {e}")
                time.sleep(30)  # ‡∏£‡∏≠ 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
    
    def _save_worker(self):
        """Worker thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        while self.running:
            try:
                time.sleep(300)  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ
                self.save_cache_to_files()
                
            except Exception as e:
                logger.error(f"Error in save worker: {e}")
                time.sleep(60)  # ‡∏£‡∏≠ 1 ‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
    
    def stop_auto_update(self):
        """‡∏´‡∏¢‡∏∏‡∏î‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
        self.running = False
        logger.info("Stopping auto update...")
        
        # ‡∏£‡∏≠‡πÉ‡∏´‡πâ threads ‡∏´‡∏¢‡∏∏‡∏î
        for thread in self.update_threads.values():
            if thread.is_alive():
                thread.join(timeout=5)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        self.save_cache_to_files()
        
        logger.info("Auto update stopped")
    
    def clear_cache(self, symbol: str = None, timeframe: str = None):
        """‡∏•‡πâ‡∏≤‡∏á cache"""
        with self.cache_lock:
            if symbol and timeframe:
                if symbol in self.cache and timeframe in self.cache[symbol]:
                    del self.cache[symbol][timeframe]
                    logger.info(f"Cleared cache for {symbol} {timeframe}")
            elif symbol:
                if symbol in self.cache:
                    del self.cache[symbol]
                    logger.info(f"Cleared cache for {symbol}")
            else:
                self.cache.clear()
                self.last_update.clear()
                logger.info("Cleared all cache")
    
    def get_cache_info(self) -> Dict:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö cache"""
        with self.cache_lock:
            info = {
                'symbols': list(self.cache.keys()),
                'total_entries': 0,
                'memory_usage': {},
                'last_updates': {}
            }
            
            for symbol in self.cache:
                info['memory_usage'][symbol] = {}
                for timeframe in self.cache[symbol]:
                    candle_count = len(self.cache[symbol][timeframe])
                    info['total_entries'] += candle_count
                    info['memory_usage'][symbol][timeframe] = candle_count
                    
                    cache_key = f"{symbol}_{timeframe}"
                    if cache_key in self.last_update:
                        info['last_updates'][cache_key] = datetime.fromtimestamp(
                            self.last_update[cache_key]
                        ).isoformat()
            
            return info
    
    def get_stats(self) -> Dict:
        """‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        cache_info = self.get_cache_info()
        
        return {
            **self.stats,
            'cache_info': cache_info,
            'running': self.running,
            'active_threads': len([t for t in self.update_threads.values() if t.is_alive()])
        }"""Technical indicators for Squeeze Bot trading system."""

import logging
from typing import Dict, Optional, Tuple

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class TechnicalIndicators:
    """Technical analysis indicators calculator with layer organization."""

    # ================================================================
    # üéØ LAYER 1: Squeeze Momentum Indicator (‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Squeeze ‡πÅ‡∏•‡∏∞ Momentum)
    # ================================================================

    @staticmethod
    def squeeze_momentum(
        df: pd.DataFrame, length: int = 20, mult_bb: float = 2.0, mult_kc: float = 1.5
    ) -> Tuple[bool, str, Dict]:
        """
        Calculate Squeeze Momentum Indicator.

        This indicator identifies periods of low volatility (squeeze) followed by
        directional moves when the squeeze is released.

        Args:
            df: DataFrame with OHLCV data
            length: Period for calculation (default 20)
            mult_bb: Bollinger Bands multiplier (default 2.0)
            mult_kc: Keltner Channel multiplier (default 1.5)

        Returns:
            Tuple[bool, str, Dict]: (squeeze_off, momentum_direction, details)
        """
        try:
            close = df["close"]
            high = df["high"]
            low = df["low"]

            # Bollinger Bands calculation
            basis = close.rolling(length).mean()
            dev = mult_bb * close.rolling(length).std()
            upper_bb = basis + dev
            lower_bb = basis - dev

            # Keltner Channel calculation (using True Range)
            high_low = high - low
            high_close = np.abs(high - close.shift())
            low_close = np.abs(low - close.shift())
            true_range = np.maximum(high_low, np.maximum(high_close, low_close))
            rangema = true_range.rolling(length).mean()

            # Moving average for Keltner Channel
            ma = close.rolling(length).mean()
            upper_kc = ma + rangema * mult_kc
            lower_kc = ma - rangema * mult_kc

            # Squeeze detection: BB inside KC = squeeze ON, BB outside KC = squeeze OFF
            squeeze_off = (lower_bb.iloc[-1] < lower_kc.iloc[-1]) and (
                upper_bb.iloc[-1] > upper_kc.iloc[-1]
            )

            # Momentum calculation using linear regression
            highest = high.rolling(length).max()
            lowest = low.rolling(length).min()
            mid_point = (highest + lowest) / 2

            momentum_direction = "NEUTRAL"
            momentum_value = 0

            if len(close) >= length:
                # Linear regression of (close - midpoint) for momentum
                x = np.arange(length)
                y = (close - mid_point).iloc[-length:].values

                if len(y) == length and not np.isnan(y).any():
                    # Current momentum slope
                    slope = np.polyfit(x, y, 1)[0]
                    momentum_value = slope

                    # Previous momentum slope for comparison
                    if len(close) > length:
                        prev_y = (close - mid_point).iloc[-length - 1 : -1].values
                        if len(prev_y) == length and not np.isnan(prev_y).any():
                            prev_slope = np.polyfit(x, prev_y, 1)[0]

                            # Determine momentum direction
                            if slope > prev_slope:
                                momentum_direction = "UP"
                            elif slope < prev_slope:
                                momentum_direction = "DOWN"
                            else:
                                momentum_direction = "NEUTRAL"

            # Additional details for analysis
            details = {
                "bb_upper": float(upper_bb.iloc[-1]),
                "bb_lower": float(lower_bb.iloc[-1]),
                "kc_upper": float(upper_kc.iloc[-1]),
                "kc_lower": float(lower_kc.iloc[-1]),
                "momentum_value": float(momentum_value),
                "squeeze_intensity": float(upper_bb.iloc[-1] - lower_bb.iloc[-1])
                / float(upper_kc.iloc[-1] - lower_kc.iloc[-1]),
            }

            # Ensure boolean is Python native type
            squeeze_off = bool(squeeze_off)

            logger.debug(
                f"Squeeze analysis: OFF={squeeze_off}, Direction={momentum_direction}"
            )
            return squeeze_off, momentum_direction, details

        except Exception as e:
            logger.error(f"Error calculating squeeze momentum: {e}")
            return False, "NEUTRAL", {}

    # ================================================================
    # üìà LAYER 2: MACD Uncle Cholok (MACD ‡∏•‡∏∏‡∏á‡πÇ‡∏â‡∏•‡∏Å 8,17,9)
    # ================================================================

    @staticmethod
    def macd_uncle_cholok(
        df: pd.DataFrame, fast: int = 8, slow: int = 17, signal: int = 9
    ) -> Tuple[float, float, str, Dict]:
        """
        MACD ‡∏•‡∏∏‡∏á‡πÇ‡∏â‡∏•‡∏Å (Uncle Cholok) calculation with custom periods (8,17,9).
        ‚úÖ UPDATED: More flexible cross detection
        """
        try:
            close = df["close"]

            # Calculate MACD components
            ema_fast = close.ewm(span=fast, adjust=False).mean()
            ema_slow = close.ewm(span=slow, adjust=False).mean()
            macd_line = ema_fast - ema_slow
            signal_line = macd_line.ewm(span=signal, adjust=False).mean()
            histogram = macd_line - signal_line

            # Get current values
            current_macd = macd_line.iloc[-1]
            current_signal = signal_line.iloc[-1]
            current_histogram = histogram.iloc[-1]

            # ‚úÖ UPDATED: Cross detection with momentum
            cross_direction = "NONE"
            if len(macd_line) > 1:
                prev_macd = macd_line.iloc[-2]
                prev_signal = signal_line.iloc[-2]

                # Bullish signal: MACD crosses above Signal OR trending up
                if current_macd > current_signal:
                    if prev_macd <= prev_signal:
                        # Traditional cross
                        cross_direction = "UP"
                        logger.debug("MACD: Traditional bullish cross detected")
                    elif current_macd > prev_macd:
                        # MACD above signal and increasing
                        cross_direction = "UP"
                        logger.debug("MACD: Bullish momentum detected (above signal + rising)")
            
                # Bearish signal: MACD crosses below Signal OR trending down
                elif current_macd < current_signal:
                    if prev_macd >= prev_signal:
                        # Traditional cross
                        cross_direction = "DOWN"
                        logger.debug("MACD: Traditional bearish cross detected")
                    elif current_macd < prev_macd:
                        # MACD below signal and decreasing
                        cross_direction = "DOWN"
                        logger.debug("MACD: Bearish momentum detected (below signal + falling)")

            # Additional details
            details = {
                "ema_fast": float(ema_fast.iloc[-1]),
                "ema_slow": float(ema_slow.iloc[-1]),
                "histogram": float(current_histogram),
                "macd_above_zero": bool(current_macd > 0),
                "signal_above_zero": bool(current_signal > 0),
                "divergence_strength": float(abs(current_macd - current_signal)),
            }

            logger.debug(
                f"MACD Uncle Cholok: {current_macd:.6f}, Signal: {current_signal:.6f}, Cross: {cross_direction}"
            )
            return float(current_macd), float(current_signal), cross_direction, details

        except Exception as e:
            logger.error(f"Error calculating MACD Uncle Cholok: {e}")
            return 0.0, 0.0, "NONE", {}

    # ================================================================
    # üìä LAYER 3: RSI Extreme (RSI ‡πÇ‡∏ï‡πà‡∏á) - ‡∏õ‡∏£‡∏±‡∏ö default threshold 40/60
    # ================================================================

    @staticmethod
    def rsi_extreme(
        df: pd.DataFrame,
        period: int = 14,
        low_threshold: float = 40,   # ‚≠ê ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 30 ‡πÄ‡∏õ‡πá‡∏ô 40
        high_threshold: float = 60,  # ‚≠ê ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 70 ‡πÄ‡∏õ‡πá‡∏ô 60
    ) -> Tuple[float, str, Dict]:
        """
        RSI ‡πÇ‡∏ï‡πà‡∏á (Extreme RSI) calculation for identifying overbought/oversold conditions.
        ‡∏õ‡∏£‡∏±‡∏ö default threshold ‡πÄ‡∏õ‡πá‡∏ô 40/60 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô

        Args:
            df: DataFrame with OHLCV data
            period: RSI calculation period (default 14)
            low_threshold: Oversold threshold (default 40 - ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å 30)
            high_threshold: Overbought threshold (default 60 - ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å 70)

        Returns:
            Tuple[float, str, Dict]: (rsi_value, extreme_level, details)
        """
        try:
            close = df["close"]

            # Calculate price changes
            delta = close.diff()

            # Separate gains and losses
            gain = (
                (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
            )
            loss = (
                (-delta.where(delta < 0, 0))
                .rolling(window=period, min_periods=1)
                .mean()
            )

            # Calculate Relative Strength and RSI
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))

            current_rsi = rsi.iloc[-1]

            # Determine extreme level - ‡πÉ‡∏ä‡πâ threshold ‡πÉ‡∏´‡∏°‡πà
            if current_rsi < low_threshold:
                extreme_level = "LOW"
            elif current_rsi > high_threshold:
                extreme_level = "HIGH"
            else:
                extreme_level = "NORMAL"

            # Calculate RSI tren
            rsi_trend = "NEUTRAL"
            if len(rsi) >= 3:
                recent_rsi = rsi.iloc[-3:]
                if recent_rsi.iloc[-1] > recent_rsi.iloc[-2] > recent_rsi.iloc[-3]:
                    rsi_trend = "RISING"
                elif recent_rsi.iloc[-1] < recent_rsi.iloc[-2] < recent_rsi.iloc[-3]:
                    rsi_trend = "FALLING"

            # Additional details
            details = {
                "rsi_14": float(current_rsi),
                "rsi_trend": rsi_trend,
                "distance_to_oversold": float(current_rsi - low_threshold),
                "distance_to_overbought": float(high_threshold - current_rsi),
                "is_diverging": False,  # TODO: Implement divergence detection
                "threshold_low": float(low_threshold),   # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
                "threshold_high": float(high_threshold), # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
            }

            logger.debug(
                f"RSI Extreme: {current_rsi:.2f}, Level: {extreme_level}, Trend: {rsi_trend}, Thresholds: {low_threshold}/{high_threshold}"
            )
            return float(current_rsi), extreme_level, details

        except Exception as e:
            logger.error(f"Error calculating RSI Extreme: {e}")
            return 50.0, "NORMAL", {}

    # ================================================================
    # üîç LAYER 4: Comprehensive Analysis (‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏ß‡∏°) - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç default values
    # ================================================================

    @staticmethod
    def analyze_all_indicators(df: pd.DataFrame, config: Dict) -> Dict:
        """
        Calculate all indicators and return comprehensive analysis.
        ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ RSI threshold ‡πÉ‡∏´‡∏°‡πà (40/60) ‡πÄ‡∏õ‡πá‡∏ô default

        Args:
            df: DataFrame with OHLCV data
            config: Configuration dictionary with indicator settings

        Returns:
            Dict with all indicator results
        """
        try:
            # Get indicator settings from config
            squeeze_config = config.get("squeeze", {})
            macd_config = config.get("macd", {})
            rsi_config = config.get("rsi", {})

            # Calculate Squeeze Momentum
            squeeze_off, momentum_direction, squeeze_details = (
                TechnicalIndicators.squeeze_momentum(
                    df,
                    length=squeeze_config.get("length", 20),
                    mult_bb=squeeze_config.get("bb_mult", 2.0),
                    mult_kc=squeeze_config.get("kc_mult", 1.5),
                )
            )

            # Calculate MACD Uncle Cholok
            macd_line, signal_line, macd_cross, macd_details = (
                TechnicalIndicators.macd_uncle_cholok(
                    df,
                    fast=macd_config.get("fast", 8),
                    slow=macd_config.get("slow", 17),
                    signal=macd_config.get("signal", 9),
                )
            )

            # Calculate RSI Extreme - ‚≠ê ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç default values ‡πÄ‡∏õ‡πá‡∏ô 40/60
            rsi_value, rsi_extreme, rsi_details = TechnicalIndicators.rsi_extreme(
                df,
                period=rsi_config.get("period", 14),
                low_threshold=rsi_config.get("oversold", 40),   # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 30 ‡πÄ‡∏õ‡πá‡∏ô 40
                high_threshold=rsi_config.get("overbought", 60), # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 70 ‡πÄ‡∏õ‡πá‡∏ô 60
            )

            # Compile comprehensive analysis
            analysis = {
                "timestamp": pd.Timestamp.now().isoformat(),
                "current_price": float(df["close"].iloc[-1]),
                "squeeze": {
                    "squeeze_off": squeeze_off,
                    "momentum_direction": momentum_direction,
                    "details": squeeze_details,
                },
                "macd": {
                    "macd_line": macd_line,
                    "signal_line": signal_line,
                    "cross_direction": macd_cross,
                    "details": macd_details,
                },
                "rsi": {
                    "value": rsi_value,
                    "extreme_level": rsi_extreme,
                    "details": rsi_details,
                },
            }

            logger.info("Comprehensive indicator analysis completed")
            return analysis

        except Exception as e:
            logger.error(f"Error in comprehensive analysis: {e}")
            return {}

    # ================================================================
    # üõ†Ô∏è LAYER 5: Helper Functions (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠)
    # ================================================================

    @staticmethod
    def validate_dataframe(df: pd.DataFrame) -> bool:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á DataFrame
        Validate DataFrame for required columns and data quality.
        """
        required_columns = ["open", "high", "low", "close", "volume"]
        
        if not all(col in df.columns for col in required_columns):
            logger.error(f"Missing required columns. Required: {required_columns}")
            return False
            
        if df.empty:
            logger.error("DataFrame is empty")
            return False
            
        if df.isnull().any().any():
            logger.warning("DataFrame contains null values")
            
        return True

    @staticmethod
    def get_indicator_summary(analysis: Dict) -> Dict:
        """
        ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ indicators ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        Get indicator summary for display purposes.
        """
        try:
            summary = {
                "timestamp": analysis.get("timestamp"),
                "price": analysis.get("current_price"),
                "squeeze_status": "OFF" if analysis.get("squeeze", {}).get("squeeze_off") else "ON",
                "momentum": analysis.get("squeeze", {}).get("momentum_direction", "NEUTRAL"),
                "macd_cross": analysis.get("macd", {}).get("cross_direction", "NONE"),
                "rsi_value": round(analysis.get("rsi", {}).get("value", 50), 2),
                "rsi_level": analysis.get("rsi", {}).get("extreme_level", "NORMAL"),
                "signals_present": {
                    "squeeze_breakout": analysis.get("squeeze", {}).get("squeeze_off", False),
                    "macd_signal": analysis.get("macd", {}).get("cross_direction") != "NONE",
                    "rsi_extreme": analysis.get("rsi", {}).get("extreme_level") != "NORMAL"
                }
            }
            return summary
        except Exception as e:
            logger.error(f"Error creating indicator summary: {e}")
            return {}

    @staticmethod
    def calculate_signal_confluence(analysis: Dict) -> Dict:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏à‡∏≤‡∏Å indicators ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
        Calculate signal confluence between multiple indicators.
        """
        try:
            squeeze = analysis.get("squeeze", {})
            macd = analysis.get("macd", {})
            rsi = analysis.get("rsi", {})
            
            # Count bullish signals
            bullish_signals = 0
            bearish_signals = 0
            
            # Squeeze momentum
            if squeeze.get("momentum_direction") == "UP":
                bullish_signals += 1
            elif squeeze.get("momentum_direction") == "DOWN":
                bearish_signals += 1
                
            # MACD cross
            if macd.get("cross_direction") == "UP":
                bullish_signals += 1
            elif macd.get("cross_direction") == "DOWN":
                bearish_signals += 1
                
            # RSI extreme
            if rsi.get("extreme_level") == "LOW":
                bullish_signals += 1
            elif rsi.get("extreme_level") == "HIGH":
                bearish_signals += 1
                
            # Squeeze breakout (neutral but important)
            squeeze_breakout = squeeze.get("squeeze_off", False)
            
            confluence = {
                "bullish_count": bullish_signals,
                "bearish_count": bearish_signals,
                "total_signals": bullish_signals + bearish_signals,
                "squeeze_breakout": squeeze_breakout,
                "confluence_strength": "STRONG" if (bullish_signals >= 3 or bearish_signals >= 3) else
                                     "MEDIUM" if (bullish_signals >= 2 or bearish_signals >= 2) else
                                     "WEAK",
                "direction": "BULLISH" if bullish_signals > bearish_signals else
                           "BEARISH" if bearish_signals > bullish_signals else
                           "NEUTRAL"
            }
            
            return confluence
            
        except Exception as e:
            logger.error(f"Error calculating signal confluence: {e}")
            return {}

    # ================================================================
    # üìã LAYER 6: Debug ‡πÅ‡∏•‡∏∞ Monitoring Functions
    # ================================================================

    @staticmethod
    def get_indicator_health(analysis: Dict) -> Dict:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicators
        Check the health of indicator calculations.
        """
        try:
            health = {
                "overall_status": "HEALTHY",
                "issues": [],
                "warnings": []
            }
            
            # Check squeeze calculation
            squeeze = analysis.get("squeeze", {})
            if not squeeze.get("details"):
                health["issues"].append("Squeeze calculation incomplete")
                health["overall_status"] = "ERROR"
                
            # Check MACD calculation  
            macd = analysis.get("macd", {})
            if macd.get("macd_line") == 0 and macd.get("signal_line") == 0:
                health["warnings"].append("MACD values are zero - check data quality")
                
            # Check RSI calculation
            rsi = analysis.get("rsi", {})
            rsi_value = rsi.get("value", 50)
            if rsi_value < 0 or rsi_value > 100:
                health["issues"].append(f"RSI value out of range: {rsi_value}")
                health["overall_status"] = "ERROR"
                
            # Check data freshness
            timestamp = analysis.get("timestamp")
            if timestamp:
                from datetime import datetime, timedelta
                analysis_time = pd.Timestamp(timestamp)
                if pd.Timestamp.now() - analysis_time > timedelta(minutes=10):
                    health["warnings"].append("Analysis data is more than 10 minutes old")
                    
            return health
            
        except Exception as e:
            logger.error(f"Error checking indicator health: {e}")
            return {"overall_status": "ERROR", "issues": [str(e)], "warnings": []}"""LINE Bot notification service for trading signals - REFACTORED for v2.0"""
import logging
from datetime import datetime
from typing import Dict, Optional

from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError, LineBotApiError
from linebot.models import TextSendMessage

logger = logging.getLogger(__name__)


class LineNotifier:
    """
    REFACTORED LINE Bot service for v2.0
    
    Main responsibilities:
    - Send trading signal notifications
    - Send position update alerts
    - Send daily summaries and error alerts
    - Handle LINE webhook verification
    
    Uses ConfigManager for:
    - LINE channel access token
    - LINE channel secret
    - LINE user ID
    """

    def __init__(self, config: Dict):
        """
        Initialize LINE notifier with ConfigManager config
        
        Args:
            config: Configuration from ConfigManager.get_line_config()
                   Expected keys: 'access_token', 'secret', optionally 'user_id'
        """
        # Configuration from ConfigManager
        self.channel_access_token = config.get("access_token")
        self.channel_secret = config.get("secret")
        self.user_id = config.get("user_id")  # Optional, can be set later

        if not self.channel_access_token or not self.channel_secret:
            logger.warning(
                "LINE credentials not fully configured - notifications disabled"
            )
            self.line_bot_api = None
            self.handler = None
            return

        try:
            self.line_bot_api = LineBotApi(self.channel_access_token)
            self.handler = WebhookHandler(self.channel_secret)
            logger.info("LineNotifier v2.0 initialized successfully")
        except Exception as e:
            logger.error(f"LINE Bot initialization failed: {e}")
            self.line_bot_api = None
            self.handler = None

    def send_signal_alert(self, analysis: Dict) -> bool:
        """
        Send trading signal alert to LINE
        
        Args:
            analysis: Complete signal analysis from SignalDetector
            
        Returns:
            bool: True if message sent successfully
        """
        try:
            if not self.line_bot_api or not self.user_id:
                logger.warning("LINE not properly configured, cannot send signal alert")
                return False

            # Check for valid signals
            signals = analysis.get("signals", {})
            recommendation = analysis.get("recommendation", "")

            if signals.get("buy") or signals.get("short"):
                # Create entry signal message
                message = self._create_entry_signal_message(analysis)
            else:
                # No relevant signal to send
                logger.debug(f"No tradeable signal found for {analysis.get('symbol', 'UNKNOWN')}")
                return False

            # Send message
            self.line_bot_api.push_message(self.user_id, TextSendMessage(text=message))
            logger.info(
                f"Signal alert sent for {analysis.get('symbol')} - {recommendation}"
            )
            return True

        except LineBotApiError as e:
            logger.error(f"LINE API error: {e}")
            return False
        except Exception as e:
            logger.error(f"Error sending signal alert: {e}")
            return False

    def send_position_update(self, update_data: Dict) -> bool:
        """
        Send position update notification to LINE
        
        Args:
            update_data: Position update data with events and position info
            
        Returns:
            bool: True if message sent successfully
        """
        try:
            if not self.line_bot_api or not self.user_id:
                logger.warning("LINE not properly configured, cannot send position update")
                return False

            # Check if there are significant events to report
            events = update_data.get("events", [])
            if not events:
                return False  # No significant update

            message = self._create_position_update_message(update_data)
            self.line_bot_api.push_message(self.user_id, TextSendMessage(text=message))
            logger.info(f"Position update sent: {', '.join(events)}")
            return True

        except Exception as e:
            logger.error(f"Error sending position update: {e}")
            return False

    def send_daily_summary(self, summary: Dict) -> bool:
        """
        Send daily trading summary
        
        Args:
            summary: Daily summary data
            
        Returns:
            bool: True if message sent successfully
        """
        try:
            if not self.line_bot_api or not self.user_id:
                logger.warning("LINE not properly configured, cannot send daily summary")
                return False

            message = self._create_daily_summary_message(summary)
            self.line_bot_api.push_message(self.user_id, TextSendMessage(text=message))
            logger.info("Daily summary sent")
            return True

        except Exception as e:
            logger.error(f"Error sending daily summary: {e}")
            return False

    def _create_entry_signal_message(self, analysis: Dict) -> str:
        """Create formatted message for entry signals"""
        symbol = analysis.get("symbol", "UNKNOWN")
        timeframe = analysis.get("timeframe", "4h")
        current_price = analysis.get("current_price", 0)
        signals = analysis.get("signals", {})
        risk_levels = analysis.get("risk_levels", {})
        signal_strength = analysis.get("signal_strength", 0)

        # Determine signal type and colors
        if signals.get("buy"):
            signal_type = "üü¢ LONG"
            direction = "LONG"
            signal_emoji = "üìà"
        elif signals.get("short"):
            signal_type = "üî¥ SHORT"
            direction = "SHORT"
            signal_emoji = "üìâ"
        else:
            signal_type = "‚ö´ UNKNOWN"
            direction = "UNKNOWN"
            signal_emoji = "‚ùì"

        # Get indicator values
        indicators = analysis.get("indicators", {})
        squeeze = indicators.get("squeeze", {})
        macd = indicators.get("macd", {})
        rsi = indicators.get("rsi", {})

        # Create formatted message
        message = f"""ü§ñ SQUEEZE BOT SIGNAL v2.0

{signal_emoji} {signal_type}
Symbol: {symbol}
Timeframe: {timeframe.upper()}
Price: ${current_price:.4f}
Strength: {signal_strength}%

üìä INDICATORS:
- Squeeze: {"OFF ‚úÖ" if squeeze.get('squeeze_off') else "ON ‚ùå"}
- Momentum: {squeeze.get('momentum_direction', 'NEUTRAL')}
- MACD: {macd.get('cross_direction', 'NONE')} Cross
- RSI: {rsi.get('value', 50):.1f}

üéØ TRADE SETUP:
- Entry: ${risk_levels.get('entry_price', current_price):.4f}
- SL: ${risk_levels.get('stop_loss', 0):.4f}
- TP1: ${risk_levels.get('take_profit_1', 0):.4f}
- TP2: ${risk_levels.get('take_profit_2', 0):.4f}
- TP3: ${risk_levels.get('take_profit_3', 0):.4f}

‚öñÔ∏è R:R = {risk_levels.get('risk_reward_ratio', 0):.2f}
üïê {datetime.now().strftime('%H:%M:%S')}

#{symbol} #{timeframe.upper()} #{direction} #v2"""

        return message

    def _create_position_update_message(self, update_data: Dict) -> str:
        """Create formatted message for position updates"""
        # Extract position and update information
        position = update_data.get("position", {})
        updates = update_data.get("updates", {})
        events = update_data.get("events", [])

        symbol = position.get("symbol", "UNKNOWN")
        direction = position.get("direction", "UNKNOWN")
        current_price = position.get("current_price", 0)
        pnl_pct = position.get("pnl_pct", 0)

        # Direction emoji
        direction_emoji = "üü¢" if direction == "LONG" else "üî¥" if direction == "SHORT" else "‚ö´"

        message = f"üìä POSITION UPDATE v2.0\n\n"
        message += f"{direction_emoji} {direction} Position\n"
        message += f"Symbol: {symbol}\n"
        message += f"Current Price: ${current_price:.4f}\n"

        # P&L with color
        pnl_emoji = "üü¢" if pnl_pct > 0 else "üî¥" if pnl_pct < 0 else "‚ö´"
        message += f"P&L: {pnl_emoji} {pnl_pct:+.2f}%\n\n"

        # Report events
        for event in events:
            if "SL hit" in event:
                message += f"üõë {event}\n"
            elif "TP" in event and "hit" in event:
                message += f"üéØ {event}\n"
            elif "Position closed" in event:
                message += f"üèÅ {event}\n"

        message += f"\nüïê {datetime.now().strftime('%H:%M:%S')}"
        message += f"\n#{symbol} #{direction} #Update #v2"

        return message

    def _create_daily_summary_message(self, summary: Dict) -> str:
        """Create formatted daily summary message"""
        total_signals = summary.get("total_signals", 0)
        active_positions = summary.get("active_positions", 0)
        closed_positions = summary.get("closed_positions", 0)
        total_pnl_pct = summary.get("total_pnl_pct", 0)
        win_rate_pct = summary.get("win_rate_pct", 0)
        wins = summary.get("wins", 0)
        losses = summary.get("losses", 0)
        version = summary.get("version", "2.0")

        # P&L with color
        pnl_emoji = "üü¢" if total_pnl_pct > 0 else "üî¥" if total_pnl_pct < 0 else "‚ö´"

        message = f"üìà DAILY SUMMARY {version}\n\n"
        message += f"üö® Signals Today: {total_signals}\n"
        message += f"üìä Active Positions: {active_positions}\n"
        message += f"‚úÖ Closed Positions: {closed_positions}\n"
        message += f"üí∞ Total P&L: {pnl_emoji} {total_pnl_pct:+.2f}%\n"
        message += f"üéØ Win Rate: {win_rate_pct:.1f}% ({wins}W/{losses}L)\n\n"

        # Best/worst performers if available
        best_performer = summary.get("best_performer", "")
        worst_performer = summary.get("worst_performer", "")

        if best_performer:
            message += f"üèÜ Best: {best_performer}\n"
        if worst_performer:
            message += f"üìâ Worst: {worst_performer}\n"

        message += f"\nüìÖ {datetime.now().strftime('%Y-%m-%d')}"
        message += f"\n#DailySummary #SqueezeBot #{version.replace('.', '')}"

        return message

    def send_test_message(self) -> bool:
        """Send test message to verify LINE integration"""
        try:
            if not self.line_bot_api or not self.user_id:
                logger.warning("LINE not properly configured for test")
                return False

            test_message = f"ü§ñ Squeeze Bot Test Message v2.0\n\n"
            test_message += f"‚úÖ LINE integration is working!\n"
            test_message += f"üïê Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            test_message += f"üöÄ Status: Ready for LONG/SHORT signals\n"
            test_message += f"üîß Version: 2.0-refactored"

            self.line_bot_api.push_message(
                self.user_id, TextSendMessage(text=test_message)
            )
            logger.info("Test message sent successfully")
            return True

        except Exception as e:
            logger.error(f"Error sending test message: {e}")
            return False

    def send_error_alert(self, error_message: str, context: str = "") -> bool:
        """Send error alert to LINE"""
        try:
            if not self.line_bot_api or not self.user_id:
                logger.warning("LINE not properly configured, cannot send error alert")
                return False

            message = f"‚ö†Ô∏è SQUEEZE BOT ERROR v2.0\n\n"
            message += f"üö® Error: {error_message}\n"
            if context:
                message += f"üìç Context: {context}\n"
            message += f"\nüïê Time: {datetime.now().strftime('%H:%M:%S')}"

            self.line_bot_api.push_message(self.user_id, TextSendMessage(text=message))
            logger.info("Error alert sent to LINE")
            return True

        except Exception as e:
            logger.error(f"Failed to send error alert: {e}")
            return False

    def verify_webhook_signature(self, body: str, signature: str) -> bool:
        """Verify LINE webhook signature"""
        try:
            if not self.handler:
                return False
            self.handler.handle(body, signature)
            return True
        except InvalidSignatureError:
            logger.error("Invalid LINE webhook signature")
            return False
        except Exception as e:
            logger.error(f"Webhook signature verification error: {e}")
            return False

    def set_user_id(self, user_id: str):
        """Set LINE user ID for notifications"""
        self.user_id = user_id
        logger.info(f"LINE user ID set: {user_id}")

    def is_configured(self) -> bool:
        """Check if LINE notifier is properly configured"""
        return (
            self.line_bot_api is not None
            and self.channel_access_token is not None
            and self.channel_secret is not None
        )

    def is_ready(self) -> bool:
        """Check if LINE notifier is ready to send messages"""
        return self.is_configured() and self.user_id is not None

    def get_status(self) -> Dict:
        """Get LINE notifier status"""
        return {
            "configured": self.is_configured(),
            "ready": self.is_ready(),
            "has_user_id": self.user_id is not None,
            "version": "2.0-refactored",
        }

    def shutdown(self):
        """Shutdown LINE notifier"""
        try:
            logger.info("Shutting down LineNotifier v2.0...")
            # Clean up any resources if needed
            logger.info("LineNotifier shutdown complete")
        except Exception as e:
            logger.error(f"Error during LineNotifier shutdown: {e}")

    # Legacy compatibility methods
    def send_position_alert(self, position_data: Dict) -> bool:
        """Legacy method - redirects to send_position_update"""
        # Convert legacy format to new format
        update_data = {
            "position": position_data,
            "events": position_data.get("events", []),
            "updates": position_data.get("updates", {}),
        }
        return self.send_position_update(update_data)"""
=============================================================================
üìä PERFORMANCE ANALYZER ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö TRADING ANALYTICS
=============================================================================
‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£:
1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î‡∏à‡∏≤‡∏Å Google Sheets
2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡πà‡∏≤‡∏á‡πÜ (Win Rate, PnL, Drawdown)
3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
4. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå pattern ‡πÅ‡∏•‡∏∞ performance ‡∏ï‡∏≤‡∏° timeframe/symbol
=============================================================================
"""

import logging
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import statistics

logger = logging.getLogger(__name__)


class PerformanceAnalyzer:
    """
    =======================================================================
    üìà PERFORMANCE ANALYZER CLASS - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î
    =======================================================================
    
    ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà:
    - ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î‡∏à‡∏≤‡∏Å Google Sheets
    - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞ metrics ‡∏ï‡πà‡∏≤‡∏á‡πÜ
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
    - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå patterns ‡πÅ‡∏•‡∏∞ trends
    """

    def __init__(self, config: Dict, sheets_logger=None):
        """
        ===================================================================
        üöÄ INITIALIZATION LAYER
        ===================================================================
        """
        self.config = config
        self.sheets_logger = sheets_logger
        
        # üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å sheets
        self.trading_data = []
        self.signal_data = []
        
        # üèÜ Performance metrics
        self.performance_cache = {}
        self.last_analysis_time = None
        
        logger.info("PerformanceAnalyzer initialized")

    def load_trading_data(self, days: int = 30) -> bool:
        """
        ===================================================================
        üìã DATA LOADING LAYER - ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Sheets
        ===================================================================
        
        Args:
            days: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            
        Returns:
            True ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        """
        if not self.sheets_logger or not self.sheets_logger._initialized:
            logger.error("SheetsLogger not available")
            return False
            
        try:
            # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Trading_Journal
            worksheet = self.sheets_logger.spreadsheet.worksheet("Trading_Journal")
            records = worksheet.get_all_records()
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
            cutoff_date = datetime.now() - timedelta(days=days)
            filtered_data = []
            
            for record in records:
                try:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                    date_str = record.get("Date", "")
                    if date_str:
                        trade_date = datetime.strptime(date_str, "%Y-%m-%d")
                        if trade_date >= cutoff_date:
                            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                            clean_record = self._clean_trading_record(record)
                            if clean_record:
                                filtered_data.append(clean_record)
                                
                except (ValueError, TypeError) as e:
                    logger.warning(f"Invalid date in record: {record}, error: {e}")
                    continue
            
            self.trading_data = filtered_data
            logger.info(f"Loaded {len(self.trading_data)} trading records")
            
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• signals ‡∏î‡πâ‡∏ß‡∏¢
            self._load_signal_data(days)
            
            return True
            
        except Exception as e:
            logger.error(f"Error loading trading data: {e}")
            return False

    def _clean_trading_record(self, record: Dict) -> Optional[Dict]:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• trading record"""
        try:
            clean_record = {
                "date": record.get("Date", ""),
                "symbol": record.get("Symbol", ""),
                "direction": record.get("Signal", ""),
                "entry_price": float(record.get("Entry", 0)),
                "sl": float(record.get("SL", 0)),
                "tp1": float(record.get("TP1", 0)),
                "tp2": float(record.get("TP2", 0)),
                "tp3": float(record.get("TP3", 0)),
                "win_loss": record.get("Win/Loss", ""),
                "win_rate": record.get("Win Rate", "")
            }
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            if (clean_record["symbol"] and 
                clean_record["direction"] and 
                clean_record["entry_price"] > 0):
                return clean_record
                
            return None
            
        except (ValueError, TypeError) as e:
            logger.warning(f"Error cleaning record: {e}")
            return None

    def _load_signal_data(self, days: int):
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• signals ‡∏à‡∏≤‡∏Å Signals worksheet"""
        try:
            worksheet = self.sheets_logger.spreadsheet.worksheet("Signals")
            records = worksheet.get_all_records()
            
            cutoff_date = datetime.now() - timedelta(days=days)
            self.signal_data = []
            
            for record in records:
                try:
                    timestamp_str = record.get("Timestamp", "")
                    if timestamp_str:
                        # ‡πÅ‡∏õ‡∏•‡∏á ISO timestamp
                        signal_date = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                        if signal_date.replace(tzinfo=None) >= cutoff_date:
                            self.signal_data.append(record)
                            
                except (ValueError, TypeError):
                    continue
                    
            logger.info(f"Loaded {len(self.signal_data)} signal records")
            
        except Exception as e:
            logger.warning(f"Could not load signal data: {e}")
            self.signal_data = []

    def calculate_basic_metrics(self) -> Dict:
        """
        ===================================================================
        üìä BASIC METRICS LAYER - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        ===================================================================
        """
        if not self.trading_data:
            return {"error": "No trading data available"}
        
        try:
            total_trades = len(self.trading_data)
            closed_trades = [t for t in self.trading_data if t["win_loss"] in ["WIN", "LOSS"]]
            wins = [t for t in closed_trades if t["win_loss"] == "WIN"]
            losses = [t for t in closed_trades if t["win_loss"] == "LOSS"]
            
            metrics = {
                "total_trades": total_trades,
                "closed_trades": len(closed_trades),
                "open_trades": total_trades - len(closed_trades),
                "wins": len(wins),
                "losses": len(losses),
                "win_rate": round((len(wins) / max(len(closed_trades), 1)) * 100, 1),
                "loss_rate": round((len(losses) / max(len(closed_trades), 1)) * 100, 1)
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"Error calculating basic metrics: {e}")
            return {"error": str(e)}

    def calculate_pnl_metrics(self) -> Dict:
        """
        ===================================================================
        üí∞ PNL METRICS LAYER - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≥‡πÑ‡∏£‡∏Ç‡∏≤‡∏î‡∏ó‡∏∏‡∏ô
        ===================================================================
        """
        if not self.trading_data:
            return {"error": "No trading data available"}
            
        try:
            closed_trades = [t for t in self.trading_data if t["win_loss"] in ["WIN", "LOSS"]]
            
            if not closed_trades:
                return {"message": "No closed trades for PnL calculation"}
            
            pnl_data = []
            
            for trade in closed_trades:
                try:
                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PnL ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á TP/SL
                    entry = trade["entry_price"]
                    sl = trade["sl"]
                    tp1 = trade["tp1"]
                    
                    if trade["win_loss"] == "WIN":
                        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏ñ‡∏∂‡∏á TP1
                        if trade["direction"] == "LONG":
                            pnl_percent = ((tp1 - entry) / entry) * 100
                        else:  # SHORT
                            pnl_percent = ((entry - tp1) / entry) * 100
                    else:  # LOSS
                        # ‡∏ñ‡∏∂‡∏á SL
                        if trade["direction"] == "LONG":
                            pnl_percent = ((sl - entry) / entry) * 100
                        else:  # SHORT
                            pnl_percent = ((entry - sl) / entry) * 100
                    
                    pnl_data.append(pnl_percent)
                    
                except (ValueError, ZeroDivisionError):
                    continue
            
            if not pnl_data:
                return {"message": "Could not calculate PnL data"}
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
            total_pnl = sum(pnl_data)
            avg_win = statistics.mean([p for p in pnl_data if p > 0]) if any(p > 0 for p in pnl_data) else 0
            avg_loss = statistics.mean([p for p in pnl_data if p < 0]) if any(p < 0 for p in pnl_data) else 0
            
            metrics = {
                "total_pnl_percent": round(total_pnl, 2),
                "average_pnl_percent": round(statistics.mean(pnl_data), 2),
                "median_pnl_percent": round(statistics.median(pnl_data), 2),
                "best_trade_percent": round(max(pnl_data), 2),
                "worst_trade_percent": round(min(pnl_data), 2),
                "average_win_percent": round(avg_win, 2),
                "average_loss_percent": round(avg_loss, 2),
                "profit_factor": round(abs(avg_win / avg_loss), 2) if avg_loss != 0 else 0,
                "total_trades_analyzed": len(pnl_data)
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"Error calculating PnL metrics: {e}")
            return {"error": str(e)}

    def analyze_by_direction(self) -> Dict:
        """
        ===================================================================
        üß≠ DIRECTION ANALYSIS LAYER - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° LONG/SHORT
        ===================================================================
        """
        if not self.trading_data:
            return {"error": "No trading data available"}
            
        try:
            directions = {}
            
            for direction in ["LONG", "SHORT"]:
                trades = [t for t in self.trading_data if t["direction"] == direction]
                closed_trades = [t for t in trades if t["win_loss"] in ["WIN", "LOSS"]]
                wins = len([t for t in closed_trades if t["win_loss"] == "WIN"])
                
                directions[direction.lower()] = {
                    "total_trades": len(trades),
                    "closed_trades": len(closed_trades),
                    "wins": wins,
                    "losses": len(closed_trades) - wins,
                    "win_rate": round((wins / max(len(closed_trades), 1)) * 100, 1)
                }
            
            return directions
            
        except Exception as e:
            logger.error(f"Error analyzing by direction: {e}")
            return {"error": str(e)}

    def analyze_by_symbol(self, top_n: int = 10) -> Dict:
        """
        ===================================================================
        üè∑Ô∏è SYMBOL ANALYSIS LAYER - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° Symbol
        ===================================================================
        """
        if not self.trading_data:
            return {"error": "No trading data available"}
            
        try:
            symbols = {}
            
            # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° symbol
            for trade in self.trading_data:
                symbol = trade["symbol"]
                if symbol not in symbols:
                    symbols[symbol] = {
                        "total_trades": 0,
                        "closed_trades": 0,
                        "wins": 0,
                        "losses": 0,
                        "win_rate": 0
                    }
                
                symbols[symbol]["total_trades"] += 1
                
                if trade["win_loss"] in ["WIN", "LOSS"]:
                    symbols[symbol]["closed_trades"] += 1
                    if trade["win_loss"] == "WIN":
                        symbols[symbol]["wins"] += 1
                    else:
                        symbols[symbol]["losses"] += 1
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì win rate
            for symbol in symbols:
                closed = symbols[symbol]["closed_trades"]
                if closed > 0:
                    symbols[symbol]["win_rate"] = round(
                        (symbols[symbol]["wins"] / closed) * 100, 1
                    )
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° total trades ‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà top N
            sorted_symbols = sorted(
                symbols.items(), 
                key=lambda x: x[1]["total_trades"], 
                reverse=True
            )[:top_n]
            
            return dict(sorted_symbols)
            
        except Exception as e:
            logger.error(f"Error analyzing by symbol: {e}")
            return {"error": str(e)}

    def analyze_signal_quality(self) -> Dict:
        """
        ===================================================================
        üéØ SIGNAL QUALITY LAYER - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
        ===================================================================
        """
        if not self.signal_data:
            return {"message": "No signal data available"}
            
        try:
            total_signals = len(self.signal_data)
            
            # ‡∏ô‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
            signal_types = {}
            timeframes = {}
            
            for signal in self.signal_data:
                # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
                signal_type = signal.get("Signal", "UNKNOWN")
                signal_types[signal_type] = signal_types.get(signal_type, 0) + 1
                
                # Timeframe
                timeframe = signal.get("Timeframe", "UNKNOWN")
                timeframes[timeframe] = timeframes.get(timeframe, 0) + 1
            
            return {
                "total_signals": total_signals,
                "signal_types": signal_types,
                "timeframes": timeframes,
                "signals_per_day": round(total_signals / max(30, 1), 1)  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ 30 ‡∏ß‡∏±‡∏ô
            }
            
        except Exception as e:
            logger.error(f"Error analyzing signal quality: {e}")
            return {"error": str(e)}

    def generate_performance_report(self, days: int = 30) -> Dict:
        """
        ===================================================================
        üìã REPORT GENERATION LAYER - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
        ===================================================================
        """
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        if not self.load_trading_data(days):
            return {"error": "Could not load trading data"}
        
        try:
            report = {
                "report_date": datetime.now().isoformat(),
                "analysis_period_days": days,
                "basic_metrics": self.calculate_basic_metrics(),
                "pnl_metrics": self.calculate_pnl_metrics(),
                "direction_analysis": self.analyze_by_direction(),
                "symbol_analysis": self.analyze_by_symbol(top_n=10),
                "signal_analysis": self.analyze_signal_quality()
            }
            
            # Cache ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            self.performance_cache = report
            self.last_analysis_time = datetime.now()
            
            logger.info(f"Generated performance report for {days} days")
            return report
            
        except Exception as e:
            logger.error(f"Error generating performance report: {e}")
            return {"error": str(e)}

    def get_summary_stats(self) -> Dict:
        """
        ===================================================================
        üìä SUMMARY STATS LAYER - ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠
        ===================================================================
        """
        if not self.trading_data:
            if not self.load_trading_data(7):  # ‡πÇ‡∏´‡∏•‡∏î 7 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                return {"error": "No data available"}
        
        try:
            basic = self.calculate_basic_metrics()
            
            return {
                "total_trades": basic.get("total_trades", 0),
                "win_rate": basic.get("win_rate", 0),
                "active_positions": basic.get("open_trades", 0),
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            logger.error(f"Error getting summary stats: {e}")
            return {"error": str(e)}

    def compare_timeframes(self) -> Dict:
        """
        ===================================================================
        ‚è∞ TIMEFRAME COMPARISON LAYER - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance ‡∏ï‡∏≤‡∏° timeframe
        ===================================================================
        """
        if not self.signal_data:
            return {"message": "No signal data for timeframe analysis"}
            
        try:
            timeframe_performance = {}
            
            # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà signals ‡∏Å‡∏±‡∏ö trades (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ)
            for signal in self.signal_data:
                timeframe = signal.get("Timeframe", "UNKNOWN")
                symbol = signal.get("Symbol", "")
                
                if timeframe not in timeframe_performance:
                    timeframe_performance[timeframe] = {
                        "total_signals": 0,
                        "signal_strength": []
                    }
                
                timeframe_performance[timeframe]["total_signals"] += 1
                
                # ‡πÄ‡∏Å‡πá‡∏ö signal strength ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                strength = signal.get("Signal_Strength", 0)
                if strength:
                    timeframe_performance[timeframe]["signal_strength"].append(float(strength))
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ signal strength
            for tf in timeframe_performance:
                strengths = timeframe_performance[tf]["signal_strength"]
                if strengths:
                    timeframe_performance[tf]["avg_signal_strength"] = round(
                        statistics.mean(strengths), 1
                    )
                else:
                    timeframe_performance[tf]["avg_signal_strength"] = 0
            
            return timeframe_performance
            
        except Exception as e:
            logger.error(f"Error comparing timeframes: {e}")
            return {"error": str(e)}

    def get_recent_performance(self, days: int = 7) -> Dict:
        """‡∏î‡∏π‡∏ú‡∏•‡∏á‡∏≤‡∏ô N ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        return self.generate_performance_report(days)

    def export_data_for_analysis(self) -> Dict:
        """
        ===================================================================
        üì§ DATA EXPORT LAYER - Export ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        ===================================================================
        """
        try:
            export_data = {
                "trading_data": self.trading_data,
                "signal_data": self.signal_data,
                "export_timestamp": datetime.now().isoformat(),
                "data_summary": {
                    "total_trades": len(self.trading_data),
                    "total_signals": len(self.signal_data),
                    "date_range": {
                        "oldest_trade": min([t["date"] for t in self.trading_data]) if self.trading_data else None,
                        "newest_trade": max([t["date"] for t in self.trading_data]) if self.trading_data else None
                    }
                }
            }
            
            return export_data
            
        except Exception as e:
            logger.error(f"Error exporting data: {e}")
            return {"error": str(e)}import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import pandas as pd

from ..utils.core_utils import JSONManager, ErrorHandler
from ..utils.data_types import DataConverter
try:
    from config.settings import RISK_MANAGEMENT
except ImportError:
    # Fallback if settings not available
    RISK_MANAGEMENT = {
        "4h": {"tp_levels": [3.0, 5.0, 7.0], "sl_level": 3.0},
        "1h": {"tp_levels": [2.0, 3.5, 5.0], "sl_level": 2.0},
        "1d": {"tp_levels": [5.0, 8.0, 12.0], "sl_level": 4.0}
    }

class PositionManager:
    """Centralized position management - ‡∏£‡∏ß‡∏° PositionTracker + PriceMonitor logic"""
    
    PRICE_TOLERANCE = 0.005  # 0.1% tolerance for TP/SL detection
    
    def __init__(self, data_manager):
        self.logger = logging.getLogger(__name__)
        self.json_manager = JSONManager()
        self.data_converter = DataConverter()
        self.data_manager = data_manager
        self.positions_file = "data/positions.json"
        self.positions = self._load_positions()
        
        self.logger.info("‚úÖ PositionManager initialized")
    
    @ErrorHandler.service_error_handler("PositionManager")
    def create_position(self, signal_data: Dict) -> Optional[str]:
        """Create new position from signal"""
        try:
            symbol = signal_data['symbol']
            timeframe = signal_data['timeframe']
            direction = signal_data['direction']
            entry_price = signal_data['current_price']
            
            # Validate entry price
            if not self.data_converter.validate_price_data(entry_price):
                self.logger.error(f"Invalid entry price: {entry_price}")
                return None
            
            # Check price sanity (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö cached price ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            cached_price = self.data_manager.price_cache.get(symbol)
            if not self.validate_price_sanity(symbol, entry_price, cached_price):
                self.logger.error(f"Invalid entry price: {symbol} = {entry_price}")
                return None
            
            # Check for existing position
            position_id = f"{symbol}_{timeframe}_{direction}"
            if position_id in self.positions:
                existing = self.positions[position_id]
                if existing['status'] == 'ACTIVE':
                    self.logger.info(f"Position {position_id} already exists")
                    return None
            
            # Calculate TP/SL levels
            tp_levels, sl_level = self._calculate_levels(entry_price, direction, timeframe)
            
            position = {
                'id': position_id,
                'symbol': symbol,
                'timeframe': timeframe,
                'direction': direction,
                'entry_price': entry_price,
                'entry_time': datetime.now().isoformat(),
                'status': 'ACTIVE',
                'tp_levels': tp_levels,
                'sl_level': sl_level,
                'tp_hit': {'TP1': False, 'TP2': False, 'TP3': False},
                'sl_hit': False,
                'current_price': entry_price,
                'pnl_pct': 0.0,
                'signal_strength': signal_data.get('signal_strength', 0),
                'created_by': 'signal_detector',
                'last_update': datetime.now().isoformat()
            }
            
            # Sanitize data before saving
            position = self.data_converter.sanitize_signal_data(position)
            
            self.positions[position_id] = position
            self._save_positions()
            
            self.logger.info(f"‚úÖ Created position: {position_id} at {entry_price}")
            return position_id
            
        except Exception as e:
            self.logger.error(f"Error creating position: {e}")
            return None
    
    @ErrorHandler.service_error_handler("PositionManager")
    def update_positions(self) -> Dict[str, Dict]:
        """Update all active positions with current prices"""
        updates = {}
        
        try:
            active_positions = {k: v for k, v in self.positions.items() 
                              if v['status'] == 'ACTIVE'}
            
            if not active_positions:
                return updates
            
            # Get current prices for all active symbols
            symbols = list(set([pos['symbol'] for pos in active_positions.values()]))
            current_prices = self.data_manager.get_current_prices_cached(symbols)
            
            for position_id, position in active_positions.items():
                symbol = position['symbol']
                current_price = current_prices.get(symbol)
                
                if current_price is None:
                    continue
                
                # Update position with current price
                old_price = position['current_price']
                position['current_price'] = current_price
                position['last_update'] = datetime.now().isoformat()
                
                # Calculate P&L
                entry_price = position['entry_price']
                direction = position['direction']
                
                if direction == 'LONG':
                    pnl_pct = ((current_price - entry_price) / entry_price) * 100
                else:  # SHORT
                    pnl_pct = ((entry_price - current_price) / entry_price) * 100
                
                position['pnl_pct'] = round(pnl_pct, 2)
                
                # Check TP/SL hits
                tp_sl_update = self._check_tp_sl_hits(position, old_price, current_price)
                if tp_sl_update:
                    updates[position_id] = tp_sl_update
            
            self._save_positions()
            
            if updates:
                self.logger.info(f"üìä Updated {len(updates)} positions with TP/SL hits")
            
            return updates
            
        except Exception as e:
            self.logger.error(f"Error updating positions: {e}")
            return {}
    
    def _check_tp_sl_hits(self, position: Dict, old_price: float, current_price: float) -> Optional[Dict]:
        """Check if TP/SL levels are hit"""
        direction = position['direction']
        tp_levels = position['tp_levels']
        sl_level = position['sl_level']
        updates = {}
        
        try:
            # Check TP hits
            for tp_name, tp_price in tp_levels.items():
                if not position['tp_hit'][tp_name]:
                    hit = False
                    
                    if direction == 'LONG':
                        tp_threshold = tp_price * (1 - self.PRICE_TOLERANCE)
                        hit = current_price >= tp_threshold
                    else:  # SHORT
                        tp_threshold = tp_price * (1 + self.PRICE_TOLERANCE)
                        hit = current_price <= tp_threshold
                    
                    if hit:
                        position['tp_hit'][tp_name] = True
                        updates[f'{tp_name}_hit'] = {
                            'hit': True,
                            'price': current_price,
                            'target_price': tp_price,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        self.logger.info(f"üéØ {tp_name} hit for {position['symbol']}: {current_price}")
                        
                        # Check if all TPs hit
                        if all(position['tp_hit'].values()):
                            position['status'] = 'CLOSED'
                            position['close_reason'] = 'ALL_TP_HIT'
                            position['close_time'] = datetime.now().isoformat()
                            updates['position_closed'] = True
            
            # Check SL hit
            if not position['sl_hit']:
                sl_hit = False
                
                if direction == 'LONG':
                    sl_threshold = sl_level * (1 + self.PRICE_TOLERANCE)
                    sl_hit = current_price <= sl_threshold
                else:  # SHORT
                    sl_threshold = sl_level * (1 - self.PRICE_TOLERANCE)
                    sl_hit = current_price >= sl_threshold
                
                if sl_hit:
                    position['sl_hit'] = True
                    position['status'] = 'CLOSED'
                    position['close_reason'] = 'SL_HIT'
                    position['close_time'] = datetime.now().isoformat()
                    updates['sl_hit'] = {
                        'hit': True,
                        'price': current_price,
                        'target_price': sl_level,
                        'timestamp': datetime.now().isoformat()
                    }
                    updates['position_closed'] = True
                    
                    self.logger.info(f"üõë SL hit for {position['symbol']}: {current_price}")
            
            return updates if updates else None
            
        except Exception as e:
            self.logger.error(f"Error checking TP/SL hits: {e}")
            return None
    
    def _calculate_levels(self, entry_price: float, direction: str, timeframe: str) -> Tuple[Dict, float]:
        """Calculate TP and SL levels"""
        risk_config = RISK_MANAGEMENT.get(timeframe, RISK_MANAGEMENT['4h'])
        tp_percentages = risk_config['tp_levels']  # [3.0, 5.0, 7.0]
        sl_percentage = risk_config['sl_level']    # 3.0
        
        tp_levels = {}
        
        if direction == 'LONG':
            tp_levels['TP1'] = round(entry_price * (1 + tp_percentages[0] / 100), 8)
            tp_levels['TP2'] = round(entry_price * (1 + tp_percentages[1] / 100), 8)
            tp_levels['TP3'] = round(entry_price * (1 + tp_percentages[2] / 100), 8)
            sl_level = round(entry_price * (1 - sl_percentage / 100), 8)
        else:  # SHORT
            tp_levels['TP1'] = round(entry_price * (1 - tp_percentages[0] / 100), 8)
            tp_levels['TP2'] = round(entry_price * (1 - tp_percentages[1] / 100), 8)
            tp_levels['TP3'] = round(entry_price * (1 - tp_percentages[2] / 100), 8)
            sl_level = round(entry_price * (1 + sl_percentage / 100), 8)
        
        return tp_levels, sl_level
    
    def get_active_positions(self) -> Dict:
        """Get all active positions"""
        return {k: v for k, v in self.positions.items() if v['status'] == 'ACTIVE'}
    
    def get_position_status(self, symbol: str, timeframe: str) -> Optional[Dict]:
        """Get position status for symbol/timeframe"""
        for position in self.positions.values():
            if (position['symbol'] == symbol and 
                position['timeframe'] == timeframe and 
                position['status'] == 'ACTIVE'):
                return position
        return None
    
    @ErrorHandler.service_error_handler("PositionManager")
    def close_position(self, position_id: str, reason: str = 'MANUAL') -> bool:
        """Manually close a position"""
        try:
            if position_id in self.positions:
                self.positions[position_id]['status'] = 'CLOSED'
                self.positions[position_id]['close_reason'] = reason
                self.positions[position_id]['close_time'] = datetime.now().isoformat()
                self._save_positions()
                self.logger.info(f"üîí Closed position {position_id}: {reason}")
                return True
            return False
        except Exception as e:
            self.logger.error(f"Error closing position: {e}")
            return False
    
    def get_positions_summary(self) -> Dict:
        """Get positions summary statistics"""
        all_positions = self.positions
        active_positions = self.get_active_positions()
        closed_positions = [pos for pos in all_positions.values() if pos['status'] == 'CLOSED']
        
        # Calculate P&L stats
        total_pnl = sum(pos.get('pnl_pct', 0) for pos in active_positions.values())
        wins = len([pos for pos in closed_positions if pos.get('pnl_pct', 0) > 0])
        losses = len([pos for pos in closed_positions if pos.get('pnl_pct', 0) < 0])
        win_rate = (wins / len(closed_positions) * 100) if closed_positions else 0
        
        return {
            'total_positions': len(all_positions),
            'active_positions': len(active_positions),
            'closed_positions': len(closed_positions),
            'total_pnl_pct': round(total_pnl, 2),
            'win_rate_pct': round(win_rate, 2),
            'wins': wins,
            'losses': losses
        }
    
    def _load_positions(self) -> Dict:
        """Load positions from JSON file"""
        positions = self.json_manager.load_json(self.positions_file, {})
        self.logger.info(f"üìÇ Loaded {len(positions)} positions")
        return positions
    
    def _save_positions(self):
        """Save positions to JSON file"""
        success = self.json_manager.save_json(self.positions, self.positions_file)
        if not success:
            self.logger.error("‚ùå Failed to save positions")
    
    def cleanup_old_positions(self, days_old: int = 30):
        """Clean up old closed positions"""
        try:
            cutoff_date = datetime.now().timestamp() - (days_old * 24 * 60 * 60)
            
            positions_to_remove = []
            for pos_id, position in self.positions.items():
                if position['status'] == 'CLOSED':
                    close_time = position.get('close_time')
                    if close_time:
                        close_timestamp = pd.to_datetime(close_time).timestamp()
                        if close_timestamp < cutoff_date:
                            positions_to_remove.append(pos_id)
            
            for pos_id in positions_to_remove:
                del self.positions[pos_id]
            
            if positions_to_remove:
                self._save_positions()
                self.logger.info(f"üßπ Cleaned up {len(positions_to_remove)} old positions")
            
        except Exception as e:
            self.logger.error(f"Error cleaning up positions: {e}")
    
    def validate_price_sanity(self, symbol: str, price: float, previous_price: float = None) -> bool:
        """Check if price is within reasonable range"""

        # Check zero or negative
        if price <= 0:
            self.logger.error(f"INVALID PRICE: {symbol} = {price} (zero or negative)")
            return False

        # Check percentage change if we have previous price
        if previous_price and previous_price > 0:
            pct_change = abs((price - previous_price) / previous_price * 100)
            if pct_change > 30:
                self.logger.error(
                    f"SUSPICIOUS PRICE CHANGE: {symbol} {previous_price} -> {price} ({pct_change:.1f}%)"
                )
                return False

        return True
"""Position tracking service to manage active trades and prevent duplicate signals."""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

logger = logging.getLogger(__name__)


class Position:
    """Individual position data structure."""

    def __init__(
        self,
        symbol: str,
        timeframe: str,
        direction: str,
        entry_price: float,
        stop_loss: float,
        take_profits: List[float],
        timestamp: str = None,
    ):
        """
        Initialize a new position.

        Args:
            symbol: Trading symbol (e.g., 'BTCUSDT')
            timeframe: Position timeframe ('1h', '1d')
            direction: 'LONG' or 'SHORT'
            entry_price: Entry price level
            stop_loss: Stop loss price level
            take_profits: List of take profit levels
            timestamp: Position creation timestamp
        """
        self.symbol = symbol
        self.timeframe = timeframe
        self.direction = direction
        self.entry_price = entry_price
        self.stop_loss = stop_loss
        self.take_profits = take_profits
        self.timestamp = timestamp or datetime.now().isoformat()
        self.status = "ACTIVE"
        self.hit_tps = []  # Track which TPs have been hit
        self.current_pnl = 0.0
        self.max_pnl = 0.0
        self.min_pnl = 0.0

    def to_dict(self) -> Dict:
        """Convert position to dictionary."""
        return {
            "symbol": self.symbol,
            "timeframe": self.timeframe,
            "direction": self.direction,
            "entry_price": self.entry_price,
            "stop_loss": self.stop_loss,
            "take_profits": self.take_profits,
            "timestamp": self.timestamp,
            "status": self.status,
            "hit_tps": self.hit_tps,
            "current_pnl": self.current_pnl,
            "max_pnl": self.max_pnl,
            "min_pnl": self.min_pnl,
        }

    @classmethod
    def from_dict(cls, data: Dict):
        """Create position from dictionary."""
        position = cls(
            symbol=data["symbol"],
            timeframe=data["timeframe"],
            direction=data["direction"],
            entry_price=data["entry_price"],
            stop_loss=data["stop_loss"],
            take_profits=data["take_profits"],
            timestamp=data["timestamp"],
        )
        position.status = data.get("status", "ACTIVE")
        position.hit_tps = data.get("hit_tps", [])
        position.current_pnl = data.get("current_pnl", 0.0)
        position.max_pnl = data.get("max_pnl", 0.0)
        position.min_pnl = data.get("min_pnl", 0.0)
        return position

    def update_pnl(self, current_price: float) -> float:
        """Update P&L based on current price."""
        if self.direction == "LONG":
            pnl_percent = ((current_price - self.entry_price) / self.entry_price) * 100
        else:  # SHORT
            pnl_percent = ((self.entry_price - current_price) / self.entry_price) * 100

        self.current_pnl = pnl_percent
        self.max_pnl = max(self.max_pnl, pnl_percent)
        self.min_pnl = min(self.min_pnl, pnl_percent)

        return pnl_percent


class PositionTracker:
    """Track active positions and manage entry/exit logic."""

    def __init__(self, positions_file: str = "data/positions.json"):
        """
        Initialize position tracker.

        Args:
            positions_file: Path to JSON file for storing positions
        """
        self.positions_file = Path(positions_file)
        self.positions: Dict[str, Position] = {}

        # Ensure data directory exists
        self.positions_file.parent.mkdir(parents=True, exist_ok=True)

        # Load existing positions
        self.load_positions()

    def load_positions(self) -> None:
        """Load positions from file."""
        try:
            if self.positions_file.exists():
                with open(self.positions_file, "r") as f:
                    data = json.load(f)

                for key, pos_data in data.items():
                    self.positions[key] = Position.from_dict(pos_data)

                logger.info(f"Loaded {len(self.positions)} positions from file")
            else:
                logger.info("No existing positions file found, starting fresh")

        except Exception as e:
            logger.error(f"Error loading positions: {e}")
            self.positions = {}

    def save_positions(self) -> None:
        """Save positions to file."""
        try:
            data = {}
            for key, position in self.positions.items():
                data[key] = position.to_dict()

            with open(self.positions_file, "w") as f:
                json.dump(data, f, indent=2, default=str)

            logger.debug(f"Saved {len(self.positions)} positions to file")

        except Exception as e:
            logger.error(f"Error saving positions: {e}")

    def get_position_key(self, symbol: str, timeframe: str) -> str:
        """Generate unique key for position."""
        return f"{symbol}_{timeframe}"

    def has_active_position(self, symbol: str, timeframe: str) -> bool:
        """Check if there's an active position for symbol/timeframe."""
        key = self.get_position_key(symbol, timeframe)
        return key in self.positions and self.positions[key].status == "ACTIVE"

    def get_position(self, symbol: str, timeframe: str) -> Optional[Position]:
        """Get position for symbol/timeframe."""
        key = self.get_position_key(symbol, timeframe)
        return self.positions.get(key)

    def create_position(
        self,
        symbol: str,
        timeframe: str,
        direction: str,
        entry_price: float,
        risk_levels: Dict,
    ) -> Position:
        """
        Create new position.

        Args:
            symbol: Trading symbol
            timeframe: Position timeframe
            direction: 'LONG' or 'SHORT'
            entry_price: Entry price
            risk_levels: Risk management levels from signal analysis

        Returns:
            Created Position object
        """
        try:
            key = self.get_position_key(symbol, timeframe)

            # Close any existing position first
            if key in self.positions:
                self.close_position(symbol, timeframe, "REPLACED")

            # Extract TP/SL levels
            stop_loss = risk_levels.get("stop_loss", entry_price)
            take_profits = [
                risk_levels.get("take_profit_1", entry_price),
                risk_levels.get("take_profit_2", entry_price),
                risk_levels.get("take_profit_3", entry_price),
            ]

            # Create new position
            position = Position(
                symbol=symbol,
                timeframe=timeframe,
                direction=direction,
                entry_price=entry_price,
                stop_loss=stop_loss,
                take_profits=take_profits,
            )

            self.positions[key] = position
            self.save_positions()

            logger.info(
                f"Created {direction} position for {symbol} {timeframe} at {entry_price}"
            )
            return position

        except Exception as e:
            logger.error(f"Error creating position: {e}")
            raise

    def close_position(
        self, symbol: str, timeframe: str, reason: str = "MANUAL"
    ) -> Optional[Position]:
        """
        Close position.

        Args:
            symbol: Trading symbol
            timeframe: Position timeframe
            reason: Closure reason (SL, TP1, TP2, TP3, MANUAL, etc.)

        Returns:
            Closed Position object or None if not found
        """
        try:
            key = self.get_position_key(symbol, timeframe)

            if key in self.positions:
                position = self.positions[key]
                position.status = f"CLOSED_{reason}"

                # Remove from active positions
                del self.positions[key]
                self.save_positions()

                logger.info(
                    f"Closed {position.direction} position for {symbol} {timeframe} - {reason}"
                )
                return position

            return None

        except Exception as e:
            logger.error(f"Error closing position: {e}")
            return None

    def update_position_tracking(
        self, symbol: str, timeframe: str, current_price: float
    ) -> Dict:
        """
        üîß FIXED: Update position tracking and check for TP/SL hits.
        ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ TP2 ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡πä‡∏Å‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏∏‡∏Å TP levels ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

        Args:
            symbol: Trading symbol
            timeframe: Position timeframe
            current_price: Current market price

        Returns:
            Dict with tracking results and any triggered levels
        """
        try:
            key = self.get_position_key(symbol, timeframe)

            if key not in self.positions:
                return {"status": "NO_POSITION"}

            position = self.positions[key]

            if position.status != "ACTIVE":
                return {"status": "INACTIVE_POSITION"}

            # Update P&L
            pnl_percent = position.update_pnl(current_price)

            result = {
                "status": "TRACKING",
                "symbol": symbol,
                "timeframe": timeframe,
                "direction": position.direction,
                "entry_price": position.entry_price,
                "current_price": current_price,
                "pnl_percent": pnl_percent,
                "triggered_levels": [],
            }

            # ‚úÖ Check for Stop Loss hit FIRST
            sl_hit = False
            if position.direction == "LONG":
                sl_hit = current_price <= position.stop_loss
            else:  # SHORT
                sl_hit = current_price >= position.stop_loss
            
            if sl_hit:
                self.close_position(symbol, timeframe, "SL")
                result["triggered_levels"].append(
                    {"type": "SL", "price": position.stop_loss}
                )
                result["final_pnl"] = pnl_percent
                logger.info(f"{symbol} STOP LOSS hit at {current_price}")
                return result

            # üéØ FIXED: Check ALL TP levels simultaneously
            new_tp_hits = []  # ‡πÄ‡∏Å‡πá‡∏ö TP ‡∏ó‡∏µ‡πà hit ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ
            
            for i, tp_price in enumerate(position.take_profits, 1):
                tp_key = f"TP{i}"
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ TP ‡∏ô‡∏µ‡πâ hit ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
                if tp_key not in position.hit_tps:
                    tp_hit = False
                    
                    if position.direction == "LONG":
                        tp_hit = current_price >= tp_price
                    else:  # SHORT
                        tp_hit = current_price <= tp_price
                    
                    if tp_hit:
                        new_tp_hits.append({
                            "key": tp_key,
                            "price": tp_price,
                            "level": i
                        })

            # üìù Process new TP hits in order
            if new_tp_hits:
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö (TP1, TP2, TP3)
                new_tp_hits.sort(key=lambda x: x["level"])
                
                for tp_hit in new_tp_hits:
                    tp_key = tp_hit["key"]
                    tp_price = tp_hit["price"]
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô hit_tps
                    position.hit_tps.append(tp_key)
                    
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô result
                    result["triggered_levels"].append({
                        "type": tp_key,
                        "price": tp_price,
                        "pnl_percent": pnl_percent,
                    })
                    
                    logger.info(f"üéØ {symbol} {tp_key} HIT at {current_price} (target: {tp_price})")

            # üîÑ ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö hit_tps ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            if position.hit_tps:
                position.hit_tps.sort(key=lambda x: int(x[2:]))

            # ‚úÖ Close position on TP3 hit
            if "TP3" in position.hit_tps:
                self.close_position(symbol, timeframe, "TP3")
                result["final_pnl"] = pnl_percent
                logger.info(f"üèÅ {symbol} position CLOSED on TP3")

            # üíæ Save updated position
            if position.status == "ACTIVE":
                self.save_positions()

            return result

        except Exception as e:
            logger.error(f"‚ùå Error updating position tracking: {e}")
            return {"status": "ERROR", "error": str(e)}

    def get_position_status(self, symbol: str, timeframe: str) -> Dict:
        """
        üîç NEW: Get detailed status of specific position
        ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debug ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö TP levels
        """
        try:
            key = self.get_position_key(symbol, timeframe)
            
            if key not in self.positions:
                return {"status": "NO_POSITION", "symbol": symbol, "timeframe": timeframe}
            
            position = self.positions[key]
            
            return {
                "status": position.status,
                "symbol": symbol,
                "timeframe": timeframe,
                "direction": position.direction,
                "entry_price": position.entry_price,
                "current_pnl": position.current_pnl,
                "max_pnl": position.max_pnl,
                "min_pnl": position.min_pnl,
                "stop_loss": position.stop_loss,
                "take_profits": {
                    "TP1": position.take_profits[0] if len(position.take_profits) > 0 else None,
                    "TP2": position.take_profits[1] if len(position.take_profits) > 1 else None,
                    "TP3": position.take_profits[2] if len(position.take_profits) > 2 else None,
                },
                "hit_tps": position.hit_tps,
                "timestamp": position.timestamp,
            }
            
        except Exception as e:
            logger.error(f"Error getting position status: {e}")
            return {"status": "ERROR", "error": str(e)}

    def force_check_tp_levels(self, symbol: str, timeframe: str, current_price: float) -> Dict:
        """
        üö® NEW: Force check TP levels for debugging
        ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏° TP ‡πÑ‡∏°‡πà‡∏ï‡∏¥‡πä‡∏Å
        """
        try:
            position = self.get_position(symbol, timeframe)
            
            if not position:
                return {"error": "No position found"}
            
            result = {
                "symbol": symbol,
                "current_price": current_price,
                "entry_price": position.entry_price,
                "direction": position.direction,
                "hit_tps": position.hit_tps,
                "tp_analysis": {}
            }
            
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞ TP level
            for i, tp_price in enumerate(position.take_profits, 1):
                tp_key = f"TP{i}"
                already_hit = tp_key in position.hit_tps
                
                if position.direction == "LONG":
                    should_hit = current_price >= tp_price
                    distance = current_price - tp_price
                else:  # SHORT
                    should_hit = current_price <= tp_price
                    distance = tp_price - current_price
                
                result["tp_analysis"][tp_key] = {
                    "target_price": tp_price,
                    "already_hit": already_hit,
                    "should_hit_now": should_hit,
                    "price_distance": distance,
                    "hit_condition": f"current_price {'‚â•' if position.direction == 'LONG' else '‚â§'} {tp_price}"
                }
            
            return result
            
        except Exception as e:
            logger.error(f"Error in force TP check: {e}")
            return {"error": str(e)}

    def get_all_active_positions(self) -> List[Position]:
        """Get all active positions."""
        return [pos for pos in self.positions.values() if pos.status == "ACTIVE"]

    def get_position_summary(self) -> Dict:
        """Get summary of all positions."""
        active_positions = self.get_all_active_positions()

        summary = {
            "total_active": len(active_positions),
            "by_timeframe": {},
            "by_direction": {"LONG": 0, "SHORT": 0},
            "positions": [],
        }

        for position in active_positions:
            # Count by timeframe
            tf = position.timeframe
            summary["by_timeframe"][tf] = summary["by_timeframe"].get(tf, 0) + 1

            # Count by direction
            summary["by_direction"][position.direction] += 1

            # Add position info
            summary["positions"].append(
                {
                    "symbol": position.symbol,
                    "timeframe": position.timeframe,
                    "direction": position.direction,
                    "entry_price": position.entry_price,
                    "current_pnl": position.current_pnl,
                    "max_pnl": position.max_pnl,
                    "min_pnl": position.min_pnl,
                    "hit_tps": position.hit_tps,
                    "timestamp": position.timestamp,
                }
            )

        return summary

    def cleanup_old_positions(self, days: int = 7) -> int:
        """
        Clean up old inactive positions.

        Args:
            days: Remove positions older than this many days

        Returns:
            Number of positions cleaned up
        """
        try:
            from datetime import datetime, timedelta

            cutoff_date = datetime.now() - timedelta(days=days)
            positions_to_remove = []

            for key, position in self.positions.items():
                if position.status != "ACTIVE":
                    pos_date = datetime.fromisoformat(
                        position.timestamp.replace("Z", "+00:00")
                    )
                    if pos_date < cutoff_date:
                        positions_to_remove.append(key)

            for key in positions_to_remove:
                del self.positions[key]

            if positions_to_remove:
                self.save_positions()
                logger.info(f"Cleaned up {len(positions_to_remove)} old positions")

            return len(positions_to_remove)

        except Exception as e:
            logger.error(f"Error cleaning up positions: {e}")
            return 0"""Price data fetching service from Binance API."""

import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional

import pandas as pd
import requests

logger = logging.getLogger(__name__)


class PriceFetcher:
    """Price data fetching from Binance API."""

    def __init__(self, base_url: str = "https://api.binance.com/api/v3"):
        """Initialize price fetcher."""
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "SqueezeBot/1.0"})

    def get_klines(
        self, symbol: str, interval: str = "1h", limit: int = 100
    ) -> Optional[pd.DataFrame]:
        """
        Get kline/candlestick data from Binance.

        Args:
            symbol: Trading pair (e.g., 'BTCUSDT')
            interval: Timeframe ('1h', '4h', '1d')
            limit: Number of records (max 1000)

        Returns:
            DataFrame with OHLCV data or None if error
        """
        try:
            url = f"{self.base_url}/klines"
            params = {
                "symbol": symbol.upper(),
                "interval": interval,
                "limit": min(limit, 1000),  # Binance max limit
            }

            logger.debug(f"Fetching {symbol} data for {interval}")
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()

            if not data:
                logger.warning(f"No data received for {symbol}")
                return None

            # Convert to DataFrame
            df = pd.DataFrame(
                data,
                columns=[
                    "timestamp",
                    "open",
                    "high",
                    "low",
                    "close",
                    "volume",
                    "close_time",
                    "quote_asset_volume",
                    "number_of_trades",
                    "taker_buy_base_asset_volume",
                    "taker_buy_quote_asset_volume",
                    "ignore",
                ],
            )

            # Convert data types
            df["datetime"] = pd.to_datetime(df["timestamp"], unit="ms")
            numeric_columns = ["open", "high", "low", "close", "volume"]

            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors="coerce")

            # Select and reorder relevant columns
            result_df = df[
                ["datetime", "open", "high", "low", "close", "volume"]
            ].copy()
            result_df = result_df.sort_values("datetime").reset_index(drop=True)

            logger.info(
                f"Successfully fetched {len(result_df)} records for {symbol} {interval}"
            )
            return result_df

        except requests.exceptions.Timeout:
            logger.error(f"Timeout fetching data for {symbol}")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"Network error fetching {symbol}: {e}")
            return None
        except ValueError as e:
            logger.error(f"Data parsing error for {symbol}: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error fetching {symbol}: {e}")
            return None

    def get_current_price(self, symbol: str) -> Optional[float]:
        """
        Get current price for a symbol.

        Args:
            symbol: Trading pair (e.g., 'BTCUSDT')

        Returns:
            Current price as float or None if error
        """
        try:
            url = f"{self.base_url}/ticker/price"
            params = {"symbol": symbol.upper()}

            response = self.session.get(url, params=params, timeout=5)
            response.raise_for_status()

            data = response.json()
            price = float(data["price"])

            logger.debug(f"Current price for {symbol}: {price}")
            return price

        except Exception as e:
            logger.error(f"Error fetching current price for {symbol}: {e}")
            return None

    def get_multiple_symbols(
        self, symbols: List[str], interval: str = "1h", limit: int = 100
    ) -> Dict[str, pd.DataFrame]:
        """
        Get price data for multiple symbols.

        Args:
            symbols: List of trading pairs
            interval: Timeframe
            limit: Number of records per symbol

        Returns:
            Dict mapping symbol to DataFrame
        """
        results = {}
        failed_symbols = []

        for symbol in symbols:
            logger.info(f"Fetching data for {symbol}")

            df = self.get_klines(symbol, interval, limit)
            if df is not None:
                results[symbol] = df
            else:
                failed_symbols.append(symbol)

            # Rate limiting - avoid hitting Binance limits
            time.sleep(0.1)

        if failed_symbols:
            logger.warning(f"Failed to fetch data for symbols: {failed_symbols}")

        logger.info(
            f"Successfully fetched data for {len(results)}/{len(symbols)} symbols"
        )
        return results

    def validate_data(self, df: pd.DataFrame, min_records: int = 50) -> bool:
        """
        Validate that DataFrame has sufficient data for analysis.

        Args:
            df: DataFrame to validate
            min_records: Minimum number of records required

        Returns:
            bool: True if valid, False otherwise
        """
        if df is None or df.empty:
            logger.error("DataFrame is None or empty")
            return False

        # Check required columns
        required_columns = ["datetime", "open", "high", "low", "close", "volume"]
        if not all(col in df.columns for col in required_columns):
            logger.error("Missing required columns in price data")
            return False

        # Check for sufficient data
        if len(df) < min_records:
            logger.warning(f"Insufficient data: {len(df)} < {min_records} records")
            return False

        # Check for NaN values in price columns
        price_columns = ["open", "high", "low", "close"]
        if df[price_columns].isnull().any().any():
            logger.error("Found NaN values in price data")
            return False

        # Check for logical price relationships
        invalid_candles = (
            (df["high"] < df["low"])
            | (df["high"] < df["open"])
            | (df["high"] < df["close"])
            | (df["low"] > df["open"])
            | (df["low"] > df["close"])
        )

        if invalid_candles.any():
            logger.error("Found invalid price relationships in data")
            return False

        logger.debug(f"Data validation passed: {len(df)} records")
        return True

    def get_market_info(self, symbol: str) -> Optional[Dict]:
        """
        Get market information for a symbol.

        Args:
            symbol: Trading pair

        Returns:
            Dict with market info or None if error
        """
        try:
            url = f"{self.base_url}/ticker/24hr"
            params = {"symbol": symbol.upper()}

            response = self.session.get(url, params=params, timeout=5)
            response.raise_for_status()

            data = response.json()

            # Extract relevant information
            market_info = {
                "symbol": data["symbol"],
                "price_change": float(data["priceChange"]),
                "price_change_percent": float(data["priceChangePercent"]),
                "high_price": float(data["highPrice"]),
                "low_price": float(data["lowPrice"]),
                "volume": float(data["volume"]),
                "quote_volume": float(data["quoteVolume"]),
                "last_price": float(data["lastPrice"]),
            }

            return market_info

        except Exception as e:
            logger.error(f"Error fetching market info for {symbol}: {e}")
            return None
"""
Price Monitor - REFACTORED for v2.0
Simplified coordinator that delegates position tracking to PositionManager
"""

import logging
import time
from datetime import datetime
from typing import Dict, List, Optional
from threading import Thread, Lock

logger = logging.getLogger(__name__)


class PriceMonitor:
    """
    REFACTORED Price Monitor - Now acts as coordinator only
    
    Main responsibilities:
    - Coordinate with PositionManager for position updates
    - Handle Google Sheets logging
    - Provide monitoring status and controls
    
    Removed responsibilities (now handled by PositionManager):
    - TP/SL calculation and detection
    - Position tracking logic
    - Price fetching (delegated to DataManager)
    """

    def __init__(self, config: Dict, sheets_logger=None):
        """
        Initialize simplified Price Monitor coordinator
        
        Args:
            config: Configuration dictionary
            sheets_logger: SheetsLogger instance for Google Sheets integration
        """
        self.config = config
        self.sheets_logger = sheets_logger
        
        # Monitoring control
        self.monitoring = False
        self.monitor_thread = None
        self.monitor_lock = Lock()
        
        # Configuration
        self.update_interval = config.get("PRICE_MONITOR_INTERVAL", 30)
        
        # Statistics
        self.stats = {
            "monitoring_cycles": 0,
            "positions_updated": 0,
            "tp_hits_logged": 0,
            "sl_hits_logged": 0,
            "last_check": None,
            "errors": 0
        }
        
        # Services (will be injected)
        self.position_manager = None
        self.data_manager = None
        
        logger.info(f"PriceMonitor v2.0 initialized as coordinator (interval: {self.update_interval}s)")

    def set_services(self, position_manager=None, data_manager=None):
        """
        Inject refactored services
        
        Args:
            position_manager: PositionManager instance
            data_manager: DataManager instance
        """
        self.position_manager = position_manager
        self.data_manager = data_manager
        logger.info("Services injected into PriceMonitor")

    def start_monitoring(self):
        """Start the monitoring coordinator"""
        if self.monitoring:
            logger.warning("Monitoring already running")
            return
        
        if not self.position_manager:
            logger.error("Cannot start monitoring: No PositionManager available")
            return
            
        self.monitoring = True
        self.monitor_thread = Thread(
            target=self._monitoring_loop,
            daemon=True,
            name="PriceMonitorCoordinator"
        )
        self.monitor_thread.start()
        
        logger.info("PriceMonitor coordinator started")

    def stop_monitoring(self):
        """Stop the monitoring coordinator"""
        if not self.monitoring:
            return
            
        self.monitoring = False
        
        if self.monitor_thread and self.monitor_thread.is_alive():
            logger.info("Stopping price monitor coordinator...")
            self.monitor_thread.join(timeout=15)
            
        logger.info("Price monitor coordinator stopped")

    def _monitoring_loop(self):
        """
        Main monitoring loop - simplified coordinator
        
        This loop now focuses on:
        1. Triggering PositionManager updates
        2. Logging results to Google Sheets
        3. Collecting statistics
        """
        logger.info("PriceMonitor coordinator loop started")
        
        while self.monitoring:
            try:
                self.stats["last_check"] = datetime.now().isoformat()
                self.stats["monitoring_cycles"] += 1
                
                # Check if we have services available
                if not self.position_manager:
                    logger.warning("No PositionManager available, skipping cycle")
                    time.sleep(self.update_interval)
                    continue
                
                # Get active positions count
                summary = self.position_manager.get_positions_summary()
                active_count = summary.get("active_positions", 0)
                
                if active_count == 0:
                    logger.debug("No active positions to monitor")
                    time.sleep(self.update_interval)
                    continue
                
                logger.info(f"Monitoring {active_count} active positions")
                
                # Trigger PositionManager to update all positions
                updates = self.position_manager.update_positions()
                self.stats["positions_updated"] += len(updates)
                
                # Process updates and log to sheets if available
                if updates and self.sheets_logger:
                    self._process_updates_for_sheets(updates)
                
                # Log summary
                if updates:
                    logger.info(f"Processed {len(updates)} position updates")
                    for position_id, update_info in updates.items():
                        if update_info.get('position_closed'):
                            logger.info(f"Position closed: {position_id}")
                        elif any(key.endswith('_hit') for key in update_info.keys()):
                            tp_hits = [k for k in update_info.keys() if k.endswith('_hit') and k.startswith('TP')]
                            if tp_hits:
                                logger.info(f"TP hit: {position_id} - {tp_hits}")
                
                # Sleep until next cycle
                time.sleep(self.update_interval)
                
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}")
                self.stats["errors"] += 1
                time.sleep(30)  # Wait 30 seconds on error

    def _process_updates_for_sheets(self, updates: Dict):
        """
        Process position updates for Google Sheets logging
        
        Args:
            updates: Dictionary of position updates from PositionManager
        """
        try:
            if not self.sheets_logger:
                return
                
            for position_id, update_info in updates.items():
                # Get the position data
                position_data = self.position_manager.positions.get(position_id)
                if not position_data:
                    continue
                
                # Log TP hits
                for tp_level in ['TP1', 'TP2', 'TP3']:
                    tp_key = f'{tp_level}_hit'
                    if update_info.get(tp_key, {}).get('hit', False):
                        try:
                            self.sheets_logger.log_tp_hit(position_data, update_info[tp_key])
                            self.stats["tp_hits_logged"] += 1
                            logger.info(f"Logged {tp_level} hit for {position_id}")
                        except Exception as e:
                            logger.error(f"Error logging {tp_level} hit: {e}")
                
                # Log SL hits
                if update_info.get('sl_hit', {}).get('hit', False):
                    try:
                        self.sheets_logger.log_sl_hit(position_data, update_info['sl_hit'])
                        self.stats["sl_hits_logged"] += 1
                        logger.info(f"Logged SL hit for {position_id}")
                    except Exception as e:
                        logger.error(f"Error logging SL hit: {e}")
                
                # Log position closure
                if update_info.get('position_closed', False):
                    try:
                        self.sheets_logger.log_position_close(position_data)
                        logger.info(f"Logged position closure for {position_id}")
                    except Exception as e:
                        logger.error(f"Error logging position closure: {e}")
                        
        except Exception as e:
            logger.error(f"Error processing updates for sheets: {e}")

    def get_monitoring_status(self) -> Dict:
        """Get current monitoring status"""
        status = {
            "monitoring": self.monitoring,
            "thread_alive": self.monitor_thread.is_alive() if self.monitor_thread else False,
            "update_interval": self.update_interval,
            "sheets_connected": self.sheets_logger is not None,
            "services": {
                "position_manager": self.position_manager is not None,
                "data_manager": self.data_manager is not None
            },
            "stats": self.stats.copy(),
            "version": "2.0-refactored"
        }
        
        # Add position summary if available
        if self.position_manager:
            try:
                summary = self.position_manager.get_positions_summary()
                status["positions_count"] = summary.get("active_positions", 0)
                status["total_positions"] = summary.get("total_positions", 0)
            except Exception as e:
                logger.error(f"Error getting position summary: {e}")
                status["positions_count"] = 0
                status["total_positions"] = 0
        
        return status

    def force_check_all_positions(self) -> Dict:
        """
        Force check all positions immediately via PositionManager
        
        Returns:
            Dict with check results
        """
        try:
            if not self.position_manager:
                return {"error": "No PositionManager available"}
            
            logger.info("Force checking all positions via PositionManager")
            
            # Get current active positions
            active_positions = self.position_manager.get_active_positions()
            
            # Trigger immediate update
            updates = self.position_manager.update_positions()
            
            # Process any updates for sheets
            if updates and self.sheets_logger:
                self._process_updates_for_sheets(updates)
            
            return {
                "status": "success",
                "message": "Force check completed",
                "positions_checked": len(active_positions),
                "updates_found": len(updates),
                "updates": updates,
                "timestamp": datetime.now().isoformat(),
                "version": "2.0-refactored"
            }
            
        except Exception as e:
            logger.error(f"Error in force check: {e}")
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def get_price_for_symbol(self, symbol: str) -> Dict:
        """
        Get current price for symbol via DataManager
        
        Args:
            symbol: Trading symbol
            
        Returns:
            Dict with price information
        """
        try:
            if not self.data_manager:
                return {"error": "No DataManager available"}
            
            price = self.data_manager.get_single_price(symbol.upper())
            
            if price is not None:
                return {
                    "status": "success",
                    "symbol": symbol.upper(),
                    "current_price": price,
                    "timestamp": datetime.now().isoformat(),
                    "version": "2.0-refactored"
                }
            else:
                return {
                    "status": "error",
                    "error": f"Could not get price for {symbol}",
                    "symbol": symbol.upper()
                }
                
        except Exception as e:
            logger.error(f"Error getting price for {symbol}: {e}")
            return {
                "status": "error",
                "error": str(e),
                "symbol": symbol.upper()
            }

    # Legacy compatibility methods
    def get_current_price(self, symbol: str) -> Optional[float]:
        """
        Legacy method for getting current price
        Delegates to DataManager if available
        """
        try:
            if self.data_manager:
                return self.data_manager.get_single_price(symbol)
            else:
                logger.warning("No DataManager available for price fetch")
                return None
        except Exception as e:
            logger.error(f"Error getting current price for {symbol}: {e}")
            return None

    def get_stats(self) -> Dict:
        """Get monitoring statistics"""
        stats = self.stats.copy()
        stats["version"] = "2.0-refactored"
        stats["monitoring_active"] = self.monitoring
        
        if self.position_manager:
            try:
                summary = self.position_manager.get_positions_summary()
                stats["current_active_positions"] = summary.get("active_positions", 0)
                stats["total_positions"] = summary.get("total_positions", 0)
            except Exception as e:
                logger.error(f"Error getting position stats: {e}")
        
        return stats

    def reset_stats(self):
        """Reset monitoring statistics"""
        self.stats = {
            "monitoring_cycles": 0,
            "positions_updated": 0,
            "tp_hits_logged": 0,
            "sl_hits_logged": 0,
            "last_check": None,
            "errors": 0
        }
        logger.info("Monitoring statistics reset")

    def shutdown(self):
        """Shutdown the price monitor"""
        try:
            logger.info("Shutting down PriceMonitor v2.0...")
            self.stop_monitoring()
            logger.info("PriceMonitor shutdown complete")
        except Exception as e:
            logger.error(f"Error during PriceMonitor shutdown: {e}")

    # Removed methods that are now handled by PositionManager:
    # - get_active_positions_from_sheets() -> PositionManager handles positions
    # - update_sheet_cell() -> SheetsLogger handles sheet updates  
    # - check_tp_sl_levels() -> PositionManager._check_tp_sl_hits() handles this
    # - monitor_positions() -> replaced with _monitoring_loop() coordinator"""
Auto Scheduler for Signal Detection and Notification - REFACTORED for v2.0
Simplified to use refactored services architecture
"""
import json
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, List
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from config.settings import Config

logger = logging.getLogger(__name__)


class SignalScheduler:
    """
    REFACTORED Signal Scheduler for v2.0
    
    Main responsibilities:
    - Schedule automated signal scanning
    - Coordinate between refactored services
    - Send notifications and log data
    - Prevent duplicate signals with cooldown system
    
    Uses refactored services:
    - SignalDetector (with integrated DataManager + PositionManager)
    - LineNotifier (via ConfigManager)
    - SheetsLogger (via ConfigManager)
    """

    def __init__(self, config: Dict):
        """
        Initialize scheduler with refactored architecture
        
        Args:
            config: Configuration dictionary from ConfigManager
        """
        # Basic configuration
        self.config = config
        self.scheduler = BackgroundScheduler()
        self.running = False
        
        # Services (will be injected)
        self.signal_detector = None
        self.position_manager = None
        self.line_notifier = None
        self.sheets_logger = None
        
        # Signal deduplication system
        self.last_signals = {}  # Store signal history
        self.cooldown_minutes = Config.SIGNAL_COOLDOWN_MINUTES
        self.signal_history_file = "data/signal_history.json"
        
        # Load signal history from file
        self._load_signal_history()
        
        logger.info(f"SignalScheduler v2.0 initialized with {self.cooldown_minutes}min cooldown")

    def _load_signal_history(self):
        """Load signal history from file"""
        try:
            if os.path.exists(self.signal_history_file):
                with open(self.signal_history_file, 'r') as f:
                    data = json.load(f)
                
                # Convert string timestamps back to datetime
                for key, timestamp_str in data.items():
                    self.last_signals[key] = datetime.fromisoformat(timestamp_str)
                
                logger.info(f"Loaded {len(self.last_signals)} signal history records")
            else:
                logger.info("No signal history file found, starting fresh")
                
        except Exception as e:
            logger.error(f"Error loading signal history: {e}")
            self.last_signals = {}

    def _save_signal_history(self):
        """Save signal history to file"""
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(self.signal_history_file), exist_ok=True)
            
            # Convert datetime to string for JSON storage
            data = {}
            for key, timestamp in self.last_signals.items():
                data[key] = timestamp.isoformat()
            
            with open(self.signal_history_file, 'w') as f:
                json.dump(data, f, indent=2)
                
            logger.debug(f"Saved {len(data)} signal history records")
            
        except Exception as e:
            logger.error(f"Error saving signal history: {e}")

    def _is_duplicate_signal(self, symbol: str, timeframe: str, direction: str) -> bool:
        """
        Check if signal is duplicate within cooldown period
        
        Args:
            symbol: Trading symbol
            timeframe: Timeframe
            direction: Signal direction (LONG/SHORT)
            
        Returns:
            bool: True if signal is duplicate
        """
        signal_key = f"{symbol}_{timeframe}_{direction}"
        current_time = datetime.now()
        
        # Check if we've sent this signal recently
        if signal_key in self.last_signals:
            last_time = self.last_signals[signal_key]
            time_diff = current_time - last_time
            
            # If still within cooldown period
            if time_diff.total_seconds() < (self.cooldown_minutes * 60):
                remaining_minutes = self.cooldown_minutes - (time_diff.total_seconds() / 60)
                logger.debug(f"Signal cooldown active for {signal_key}: {remaining_minutes:.1f} minutes remaining")
                return True
        
        # Clean up old data (keep only last 24 hours)
        cutoff_time = current_time - timedelta(hours=24)
        keys_to_remove = [
            key for key, timestamp in self.last_signals.items()
            if timestamp < cutoff_time
        ]
        for key in keys_to_remove:
            del self.last_signals[key]
        
        return False

    def _record_signal(self, symbol: str, timeframe: str, direction: str):
        """Record signal that was sent"""
        signal_key = f"{symbol}_{timeframe}_{direction}"
        self.last_signals[signal_key] = datetime.now()
        
        # Save to file immediately
        self._save_signal_history()
        logger.debug(f"Recorded signal: {signal_key}")

    def set_services(self, signal_detector, position_manager, line_notifier, sheets_logger):
        """
        Inject refactored services
        
        Args:
            signal_detector: SignalDetector instance
            position_manager: PositionManager instance
            line_notifier: LineNotifier instance
            sheets_logger: SheetsLogger instance
        """
        self.signal_detector = signal_detector
        self.position_manager = position_manager
        self.line_notifier = line_notifier
        self.sheets_logger = sheets_logger
        
        logger.info("Refactored services injected into scheduler")

    def start_scheduler(self):
        """Start the automated scheduler"""
        if self.running:
            logger.warning("Scheduler already running")
            return
        
        if not all([self.signal_detector, self.position_manager]):
            logger.error("Required services not set")
            return
        
        # Job 1: Scan 4H signals - ‡∏ó‡∏∏‡∏Å 15 ‡∏ô‡∏≤‡∏ó‡∏µ
        self.scheduler.add_job(
            func=self._scan_4h_signals,
            trigger="cron",
            hour="*",
            minute="*/15",      # ‚Üê ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ! ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏õ‡πá‡∏ô "0"
            id="scan_4h_signals",
            name="4H Signal Scanner v2.0",
            replace_existing=True,
        )

        # Job 2: ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‚úÖ
        self.scheduler.add_job(
            func=self._scan_1d_signals,
            trigger="cron",
            hour="0,4,8,12,16,20",
            minute=0,
            id="scan_1d_signals",
            name="1D Signal Scanner v2.0 (Every 4H)",
            replace_existing=True,
        )
        
        # Job 2: Update positions every 2 minutes (via PositionManager)
        self.scheduler.add_job(
            func=self._update_positions_refactored,
            trigger=IntervalTrigger(minutes=2),
            id="update_positions",
            name="Position Tracker v2.0",
            replace_existing=True,
        )
        
        # Job 3: Daily summary at midnight
        self.scheduler.add_job(
            func=self._send_daily_summary,
            trigger="cron",
            hour=0,
            minute=0,
            id="daily_summary",
            name="Daily Summary v2.0",
            replace_existing=True,
        )
        
        # Start scheduler
        self.scheduler.start()
        self.running = True
        
        logger.info("SignalScheduler v2.0 started successfully")
        logger.info("Scheduled jobs:")
        logger.info(" - 4H signals: every 15 minutes")
        logger.info(" - 1D signals: every 4 hours (00:00, 04:00, 08:00, 12:00, 16:00, 20:00 UTC)")
        logger.info(" - Position updates: every 2 minutes (via PositionManager)")
        logger.info(" - Daily summary: daily at 00:00 UTC (07:00 ICT)")
        logger.info(f" - Signal cooldown: {self.cooldown_minutes} minutes")

    def stop_scheduler(self):
        """Stop the scheduler"""
        if not self.running:
            logger.warning("Scheduler not running")
            return
        
        # Save history before stopping
        self._save_signal_history()
        
        self.scheduler.shutdown(wait=False)
        self.running = False
        
        logger.info("SignalScheduler v2.0 stopped")

    def get_scheduler_status(self) -> Dict:
        """Get current scheduler status"""
        if not self.running:
            return {
                "status": "stopped",
                "jobs": [],
                "next_run_times": {},
                "signal_history_count": len(self.last_signals),
                "version": "2.0-refactored"
            }
        
        jobs = []
        next_run_times = {}
        
        for job in self.scheduler.get_jobs():
            job_info = {
                "id": job.id,
                "name": job.name,
                "next_run": (
                    job.next_run_time.isoformat() if job.next_run_time else None
                ),
                "trigger": str(job.trigger),
            }
            jobs.append(job_info)
            next_run_times[job.id] = job_info["next_run"]
        
        return {
            "status": "running",
            "jobs": jobs,
            "next_run_times": next_run_times,
            "scheduler_state": str(self.scheduler.state),
            "signal_history_count": len(self.last_signals),
            "cooldown_minutes": self.cooldown_minutes,
            "version": "2.0-refactored",
            "services_connected": {
                "signal_detector": self.signal_detector is not None,
                "position_manager": self.position_manager is not None,
                "line_notifier": self.line_notifier is not None,
                "sheets_logger": self.sheets_logger is not None
            }
        }

    def _scan_4h_signals(self):
        """Scan 4H signals"""
        try:
            logger.info("Starting 4H signal scan v2.0...")
            
            symbols = Config.DEFAULT_SYMBOLS if hasattr(Config, 'DEFAULT_SYMBOLS') else ["BTCUSDT", "ETHUSDT"]
        
            if not symbols:
                logger.warning("No symbols configured for scanning")
                return
            
            active_signals = self.signal_detector.get_active_signals(symbols, ["4h"])
            logger.info(f"Found {len(active_signals)} active signals on 4H")
            
            processed_count = 0
            for signal in active_signals:
                if self._process_signal_refactored(signal, "4h"):
                    processed_count += 1
            
            logger.info(f"Processed {processed_count}/{len(active_signals)} signals on 4H")
            
        except Exception as e:
            logger.error(f"Error in 4H signal scan: {e}")

    def _scan_1d_signals(self):
        """Scan 1D signals using refactored SignalDetector"""
        try:
            logger.info("Starting 1D signal scan v2.0...")
            
            symbols = Config.DEFAULT_SYMBOLS if hasattr(Config, 'DEFAULT_SYMBOLS') else ["BTCUSDT", "ETHUSDT"]
        
            if not symbols:
                logger.warning("No symbols configured for scanning")
                return
            
            # Use refactored SignalDetector
            active_signals = self.signal_detector.get_active_signals(symbols, ["1d"])
            logger.info(f"Found {len(active_signals)} active signals on 1D")
            
            processed_count = 0
            for signal in active_signals:
                if self._process_signal_refactored(signal, "1d"):
                    processed_count += 1
            
            logger.info(f"Processed {processed_count}/{len(active_signals)} signals on 1D")
            
        except Exception as e:
            logger.error(f"Error in 1D signal scan: {e}")
            if self.line_notifier:
                try:
                    self.line_notifier.send_error_alert(
                        f"1D signal scan failed: {str(e)}", "Scheduler v2.0"
                    )
                except:
                    pass

    def _process_signal_refactored(self, signal: Dict, timeframe: str) -> bool:
        """
        Process signal using refactored architecture
        
        Args:
            signal: Signal data from SignalDetector
            timeframe: Timeframe being processed
            
        Returns:
            bool: True if signal was processed successfully
        """
        try:
            # Extract basic signal information
            symbol = signal.get("symbol")
            signals = signal.get("signals", {})
            signal_strength = signal.get("signal_strength", 0)
            position_created = signal.get("position_created", False)
            
            # Validate basic data
            if not symbol:
                return False
            
            # Check signal strength threshold (75%)
            if signal_strength < 75:
                logger.debug(f"Skipping {symbol} {timeframe} - signal strength {signal_strength} < 75")
                return False
            
            # Determine trading direction
            direction = None
            if signals.get("buy"):
                direction = "LONG"
            elif signals.get("short"):
                direction = "SHORT"
            
            if not direction:
                logger.debug(f"No valid signal direction for {symbol} {timeframe}")
                return False
            
            # Check for duplicate signals
            if self._is_duplicate_signal(symbol, timeframe, direction):
                logger.debug(f"Skipping duplicate signal: {symbol} {timeframe} {direction}")
                return False
            
            # SignalDetector should have already created position if valid
            if not position_created:
                logger.debug(f"Position not auto-created for {symbol} {timeframe}")
                # Still record to prevent duplicate attempts
                self._record_signal(symbol, timeframe, direction)
                return False
            
            # Send LINE notification for new signals with positions
            if self.line_notifier:
                try:
                    self.line_notifier.send_signal_alert(signal)
                    logger.info(f"Sent LINE notification for {symbol} {timeframe} {direction}")
                except Exception as e:
                    logger.warning(f"Failed to send LINE notification: {e}")
            
            # Log to Google Sheets for new signals with positions
            if self.sheets_logger:
                try:
                    self.sheets_logger.log_trading_journal(signal)
                    logger.info(f"Logged to Google Sheets: {symbol} {timeframe} {direction}")
                except Exception as e:
                    logger.warning(f"Failed to log to Google Sheets: {e}")
            
            # Record signal in history
            self._record_signal(symbol, timeframe, direction)
            
            logger.info(f"Processed new signal: {symbol} {timeframe} {direction} (Strength: {signal_strength})")
            return True
            
        except Exception as e:
            logger.error(f"Error processing signal {signal.get('symbol', 'UNKNOWN')}: {e}")
            return False

    def _update_positions_refactored(self):
        """
        Update positions using refactored PositionManager
        
        This method now simply triggers PositionManager to update all positions
        and handles any notifications/logging for the results
        """
        try:
            if not self.position_manager:
                logger.warning("No PositionManager available for position updates")
                return
            
            # Get current active positions count
            summary = self.position_manager.get_positions_summary()
            active_count = summary.get("active_positions", 0)
            
            if active_count == 0:
                logger.debug("No active positions to update")
                return
            
            logger.debug(f"Updating {active_count} active positions...")
            
            # Trigger PositionManager to update all positions
            updates = self.position_manager.update_positions()
            
            # Process any position updates for notifications
            notifications_sent = 0
            sheets_logged = 0
            
            for position_id, update_info in updates.items():
                try:
                    # Check if any important events occurred
                    events = []
                    if update_info.get('position_closed'):
                        events.append("Position closed")
                    
                    for tp_level in ['TP1', 'TP2', 'TP3']:
                        if update_info.get(f'{tp_level}_hit', {}).get('hit', False):
                            events.append(f"{tp_level} hit")
                    
                    if update_info.get('sl_hit', {}).get('hit', False):
                        events.append("SL hit")
                    
                    if events:
                        logger.info(f"Position {position_id}: {', '.join(events)}")
                        
                        # Send LINE notification if available
                        if self.line_notifier:
                            try:
                                # Format update for LINE notification
                                position_data = self.position_manager.positions.get(position_id)
                                if position_data:
                                    notification_data = {
                                        "position": position_data,
                                        "updates": update_info,
                                        "events": events
                                    }
                                    self.line_notifier.send_position_update(notification_data)
                                    notifications_sent += 1
                            except Exception as e:
                                logger.warning(f"Failed to send position update notification: {e}")
                        
                        # Log to Google Sheets if available
                        if self.sheets_logger:
                            try:
                                position_data = self.position_manager.positions.get(position_id)
                                if position_data:
                                    self.sheets_logger.log_position_update({
                                        "position": position_data,
                                        "updates": update_info
                                    })
                                    sheets_logged += 1
                            except Exception as e:
                                logger.warning(f"Failed to log position update: {e}")
                                
                except Exception as e:
                    logger.error(f"Error processing update for {position_id}: {e}")
            
            if notifications_sent > 0 or sheets_logged > 0:
                logger.info(f"Position updates: {notifications_sent} LINE notifications, {sheets_logged} sheets logs")
                
        except Exception as e:
            logger.error(f"Error in refactored position update: {e}")

    def _send_daily_summary(self):
        """Send daily summary using refactored services"""
        try:
            logger.info("Generating daily summary v2.0...")
            
            # Get statistics from sheets logger if available
            if self.sheets_logger:
                try:
                    stats = self.sheets_logger.get_trading_statistics(days=1)
                except:
                    stats = {}
            else:
                stats = {}
            
            # Get position summary from PositionManager
            if self.position_manager:
                try:
                    position_summary = self.position_manager.get_positions_summary()
                except:
                    position_summary = {}
            else:
                position_summary = {}
            
            daily_summary = {
                "date": datetime.now().strftime("%Y-%m-%d"),
                "version": "2.0-refactored",
                "total_signals": stats.get("total_trades", 0),
                "active_positions": position_summary.get("active_positions", 0),
                "closed_positions": position_summary.get("closed_positions", 0),
                "total_positions": position_summary.get("total_positions", 0),
                "win_rate_pct": position_summary.get("win_rate_pct", 0),
                "total_pnl_pct": position_summary.get("total_pnl_pct", 0),
                "wins": position_summary.get("wins", 0),
                "losses": position_summary.get("losses", 0),
                "signal_history_count": len(self.last_signals),
                "best_performer": stats.get("best_performer", ""),
                "worst_performer": stats.get("worst_performer", "")
            }
            
            # Send LINE notification
            if self.line_notifier:
                try:
                    self.line_notifier.send_daily_summary(daily_summary)
                    logger.info("Daily summary sent via LINE")
                except Exception as e:
                    logger.warning(f"Failed to send daily summary via LINE: {e}")
            
            # Log to Google Sheets
            if self.sheets_logger:
                try:
                    self.sheets_logger.log_daily_summary(daily_summary)
                    logger.info("Daily summary logged to sheets")
                except Exception as e:
                    logger.warning(f"Failed to log daily summary: {e}")
            
        except Exception as e:
            logger.error(f"Error sending daily summary: {e}")

    def get_enhanced_status(self) -> Dict:
        """Get detailed status including position summary"""
        try:
            scheduler_status = self.get_scheduler_status()
            
            # Add position summary if available
            if self.position_manager:
                try:
                    position_summary = self.position_manager.get_positions_summary()
                    scheduler_status["position_summary"] = position_summary
                except Exception as e:
                    logger.error(f"Error getting position summary: {e}")
            
            # Add signal history details
            scheduler_status["signal_history"] = self.get_signal_history()
            
            return scheduler_status
            
        except Exception as e:
            logger.error(f"Error getting enhanced status: {e}")
            return self.get_scheduler_status()

    def force_scan_now(self, timeframe: str = "1d") -> Dict:
        """Force immediate signal scan"""
        try:
            if timeframe == "1d":
                self._scan_1d_signals()
                return {"status": "1D scan completed", "version": "2.0-refactored"}
            else:
                return {"error": "Invalid timeframe. Use '1d' only"}
        except Exception as e:
            logger.error(f"Error in force scan: {e}")
            return {"error": str(e)}

    def force_update_positions(self) -> Dict:
        """Force immediate position update"""
        try:
            self._update_positions_refactored()
            return {"status": "Position update completed", "version": "2.0-refactored"}
        except Exception as e:
            logger.error(f"Error in force position update: {e}")
            return {"error": str(e)}

    def clear_signal_history(self):
        """Clear all signal history (for testing)"""
        self.last_signals = {}
        self._save_signal_history()
        logger.info("Signal history cleared")

    def get_signal_history(self) -> Dict:
        """Get signal history with timestamps"""
        history = {}
        for key, timestamp in self.last_signals.items():
            history[key] = {
                "timestamp": timestamp.isoformat(),
                "minutes_ago": (datetime.now() - timestamp).total_seconds() / 60
            }
        return history

    def _process_signal(self, signal: Dict, timeframe: str):
        """Legacy method - redirects to refactored version"""
        return self._process_signal_refactored(signal, timeframe)

    def _update_positions(self):
        """Legacy method - redirects to refactored version"""
        self._update_positions_refactored()"""
Google Sheets Integration for Trading Signal Logging - REFACTORED for v2.0
Simplified to use ConfigManager for configuration
FIXED: worksheet attribute error + Base64 credentials support
"""

import base64
import json
import logging
import os
from datetime import datetime
from typing import Any, Dict, List, Optional

try:
    import gspread
    from google.oauth2.service_account import Credentials
    SHEETS_AVAILABLE = True
except ImportError:
    SHEETS_AVAILABLE = False
    gspread = None
    Credentials = None

logger = logging.getLogger(__name__)


class SheetsLogger:
    """
    REFACTORED Google Sheets Logger for v2.0
    
    Main responsibilities:
    - Connect to Google Sheets using ConfigManager settings
    - Log trading signals and results
    - Track Trading Journal with Win/Loss tracking
    - Calculate Win Rate automatically
    - Generate trading statistics
    
    Uses ConfigManager for:
    - Google Sheets ID
    - Credentials path/content
    """

    def __init__(self, config: Dict):
        """
        Initialize SheetsLogger with ConfigManager config
        
        Args:
            config: Configuration from ConfigManager.get_google_config()
                   Expected keys: 'sheets_id', 'credentials_path'
        """
        # Configuration from ConfigManager
        self.credentials_path = config.get("credentials_path")
        self.spreadsheet_id = config.get("sheets_id")
        
        # Connection state
        self.gc = None                    # Google Sheets client
        self.spreadsheet = None           # Spreadsheet object
        self._cached_worksheet = None     # Current worksheet cache (FIXED)
        self._initialized = False         # Initialization status

        # Show configuration status
        logger.info("Initializing SheetsLogger v2.0...")
        logger.info(f"Credentials configured: {bool(self.credentials_path)}")
        logger.info(f"Spreadsheet ID configured: {bool(self.spreadsheet_id)}")

        # Check dependencies
        if not SHEETS_AVAILABLE:
            logger.warning("Google Sheets dependencies not installed")
            return

        # Check configuration
        if not self.credentials_path or not self.spreadsheet_id:
            logger.warning("Google Sheets credentials or spreadsheet ID not configured")
            logger.warning(f"   Credentials: {'available' if self.credentials_path else 'missing'}")
            logger.warning(f"   Spreadsheet ID: {'available' if self.spreadsheet_id else 'missing'}")
            return

        # Attempt connection
        try:
            self._initialize_connection()
            self._initialized = True
            logger.info("SheetsLogger v2.0 initialization completed successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Google Sheets connection: {e}")
            self._initialized = False

    @property
    def worksheet(self):
        """
        Property to safely access Trading_Journal worksheet
        Returns None if not initialized or worksheet not available
        """
        if not self._initialized or not self.spreadsheet:
            return None
        
        try:
            # Try to get cached worksheet first
            if self._cached_worksheet:
                return self._cached_worksheet
            
            # Get worksheet from spreadsheet
            worksheet = self.spreadsheet.worksheet("Trading_Journal")
            self._cached_worksheet = worksheet
            return worksheet
            
        except Exception as e:
            logger.error(f"Error accessing Trading_Journal worksheet: {e}")
            return None

    def _initialize_connection(self):
        """Initialize connection to Google Sheets"""
        if not SHEETS_AVAILABLE:
            return

        try:
            # Define OAuth permissions
            scope = [
                "https://spreadsheets.google.com/feeds",
                "https://www.googleapis.com/auth/drive",
            ]

            logger.info(f"Loading credentials from: {type(self.credentials_path)}")

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Base64 credentials ‡πÑ‡∏´‡∏°
            credentials_str = str(self.credentials_path)

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ { ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Base64
            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ { ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô Base64
            if credentials_str and not credentials_str.strip().startswith('{') and not os.path.isfile(credentials_str):
                try:
                    logger.info("Attempting to decode Base64 credentials")
                    # ‡πÄ‡∏û‡∏¥‡πà‡∏° padding ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
                    missing_padding = len(credentials_str) % 4
                    if missing_padding:
                        credentials_str += '=' * (4 - missing_padding)
                    
                    decoded = base64.b64decode(credentials_str)
                    creds_info = json.loads(decoded)
                    creds = Credentials.from_service_account_info(creds_info, scopes=scope)
                    logger.info("Successfully loaded Base64 credentials")
                except Exception as e:
                    logger.error(f"Failed to decode Base64 credentials: {e}")
                    raise

            # Load credentials from file
            elif os.path.isfile(credentials_str):
                logger.info(f"Loading credentials from file: {self.credentials_path}")
                creds = Credentials.from_service_account_file(
                    self.credentials_path, scopes=scope
                )

            # Load credentials from JSON string/dict
            else:
                logger.info("Loading credentials from JSON content")
                if isinstance(self.credentials_path, str):
                    try:
                        creds_info = json.loads(self.credentials_path)
                    except json.JSONDecodeError as e:
                        logger.error(f"Invalid JSON in credentials: {e}")
                        raise ValueError(f"Invalid JSON in credentials: {e}")
                else:
                    creds_info = self.credentials_path

                creds = Credentials.from_service_account_info(creds_info, scopes=scope)

            # Create authorized client
            self.gc = gspread.authorize(creds)
            
            # Connect to spreadsheet
            self.spreadsheet = self.gc.open_by_key(self.spreadsheet_id)

            # Try to access or create the main worksheet
            try:
                self._cached_worksheet = self.spreadsheet.worksheet("Trading_Journal")
                logger.info("Connected to existing Trading_Journal worksheet")
            except gspread.WorksheetNotFound:
                # Create Trading_Journal worksheet if it doesn't exist
                headers = ["Date", "Symbol", "Signal", "Entry", "SL", "TP1", "TP2", "TP3", "Win/Loss", "Win Rate"]
                self._cached_worksheet = self.spreadsheet.add_worksheet(
                    title="Trading_Journal", rows=1000, cols=len(headers)
                )
                self._cached_worksheet.append_row(headers)
                logger.info("Created new Trading_Journal worksheet")

            logger.info("Google Sheets connection established")
            logger.info(f"Spreadsheet: {self.spreadsheet.title}")
            logger.info(f"Spreadsheet ID: {self.spreadsheet_id}")

        except Exception as e:
            logger.error(f"Google Sheets initialization error: {e}")
            logger.error(f"Credentials path type: {type(self.credentials_path)}")
            logger.error(f"Spreadsheet ID: {self.spreadsheet_id}")
            raise

    def _ensure_worksheet_exists(self, worksheet_name: str, headers: List[str]) -> Optional[Any]:
        """Ensure worksheet exists with proper headers"""
        if not self.spreadsheet:
            logger.error("Spreadsheet not initialized")
            return None

        try:
            # Try to find existing worksheet
            try:
                worksheet = self.spreadsheet.worksheet(worksheet_name)
                logger.info(f"Found existing worksheet: {worksheet_name}")

                # Check headers
                existing_headers = worksheet.row_values(1)
                if not existing_headers or existing_headers != headers:
                    worksheet.clear()
                    worksheet.append_row(headers)
                    logger.info(f"Updated headers for worksheet: {worksheet_name}")

                return worksheet

            # Create new worksheet if not found
            except gspread.WorksheetNotFound:
                logger.info(f"Creating new worksheet: {worksheet_name}")
                worksheet = self.spreadsheet.add_worksheet(
                    title=worksheet_name, 
                    rows=1000,
                    cols=len(headers)
                )
                worksheet.append_row(headers)
                logger.info(f"Created new worksheet: {worksheet_name}")
                return worksheet

        except Exception as e:
            logger.error(f"Error ensuring worksheet exists: {e}")
            return None

    def _determine_signal_type(self, signals: Dict) -> str:
        """Determine signal type from signals dictionary"""
        # Strong signals (highest priority)
        if signals.get("strong_buy"):
            return "STRONG_BUY"
        elif signals.get("strong_short"):
            return "STRONG_SHORT"
        
        # Medium signals
        elif signals.get("medium_buy"):
            return "MEDIUM_BUY"
        elif signals.get("medium_short"):
            return "MEDIUM_SHORT"
        
        # Weak signals
        elif signals.get("weak_buy"):
            return "WEAK_BUY"
        elif signals.get("weak_short"):
            return "WEAK_SHORT"
        
        # Experimental signals
        elif signals.get("experimental_buy"):
            return "EXPERIMENTAL_BUY"
        elif signals.get("experimental_short"):
            return "EXPERIMENTAL_SHORT"
        
        # Basic signals (legacy support)
        elif signals.get("buy"):
            return "BUY"
        elif signals.get("short"):
            return "SHORT"
        elif signals.get("sell"):
            return "SELL"
        elif signals.get("cover"):
            return "COVER"
        
        # No signal
        else:
            return "NONE"

    def _has_tradeable_signal(self, signals: Dict) -> bool:
        """Check if signals contain tradeable signals"""
        return any([
            # Strong signals
            signals.get("strong_buy", False),
            signals.get("strong_short", False),
            
            # Medium signals  
            signals.get("medium_buy", False),
            signals.get("medium_short", False),
            
            # Weak signals
            signals.get("weak_buy", False),
            signals.get("weak_short", False),
            
            # Experimental signals
            signals.get("experimental_buy", False),
            signals.get("experimental_short", False),
            
            # Basic signals (legacy)
            signals.get("buy", False),
            signals.get("short", False)
        ])

    def _get_trade_direction(self, signals: Dict) -> Optional[str]:
        """Get trade direction from signals"""
        # LONG/BUY signals
        if any([
            signals.get("strong_buy", False),
            signals.get("medium_buy", False), 
            signals.get("weak_buy", False),
            signals.get("experimental_buy", False),
            signals.get("buy", False)
        ]):
            return "LONG"
        
        # SHORT signals
        elif any([
            signals.get("strong_short", False),
            signals.get("medium_short", False),
            signals.get("weak_short", False), 
            signals.get("experimental_short", False),
            signals.get("short", False)
        ]):
            return "SHORT"
        
        # No clear direction
        else:
            return None

    def log_signal(self, analysis: Dict) -> bool:
        """
        Log detailed signal information to Signals worksheet
        
        Args:
            analysis: Complete analysis from SignalDetector
            
        Returns:
            bool: True if logged successfully
        """
        # Check connection status
        if not self._initialized:
            logger.warning("SheetsLogger not properly initialized, skipping detailed signal log")
            return False
            
        if not self.spreadsheet:
            logger.warning("Google Sheets not initialized, skipping detailed signal log")
            return False

        try:
            symbol = analysis.get("symbol", "UNKNOWN")
            timeframe = analysis.get("timeframe", "UNKNOWN")
            logger.info(f"Attempting to log detailed signal: {symbol} ({timeframe})")
            
            # Define headers for detailed logging
            headers = [
                "Timestamp", "Symbol", "Timeframe", "Price", "Signal", "Recommendation",
                "Squeeze_Off", "Momentum", "MACD_Cross", "RSI_Value", "RSI_Level", 
                "Signal_Strength", "Entry_Price", "Stop_Loss", "TP1", "TP2", "TP3", "Risk_Reward"
            ]

            # Prepare worksheet
            logger.info("Ensuring Signals worksheet exists...")
            worksheet = self._ensure_worksheet_exists("Signals", headers)
            if not worksheet:
                logger.error("Failed to create/access Signals worksheet")
                return False

            # Extract data from analysis
            signals = analysis.get("signals", {})
            indicators = analysis.get("indicators", {})
            risk_levels = analysis.get("risk_levels", {})

            # Determine signal type
            signal_type = self._determine_signal_type(signals)
            logger.info(f"Signal type determined: {signal_type}")

            # Prepare row data
            try:
                row_data = [
                    analysis.get("timestamp", datetime.now().isoformat()),
                    symbol,
                    timeframe,
                    float(analysis.get("current_price", 0)),
                    signal_type,
                    analysis.get("recommendation", ""),
                    bool(indicators.get("squeeze", {}).get("squeeze_off", False)),
                    str(indicators.get("squeeze", {}).get("momentum_direction", "")),
                    str(indicators.get("macd", {}).get("cross_direction", "")),
                    float(indicators.get("rsi", {}).get("value", 0)),
                    str(indicators.get("rsi", {}).get("extreme_level", "")),
                    float(analysis.get("signal_strength", 0)),
                    float(risk_levels.get("entry_price", 0)),
                    float(risk_levels.get("stop_loss", 0)),
                    float(risk_levels.get("take_profit_1", 0)),
                    float(risk_levels.get("take_profit_2", 0)),
                    float(risk_levels.get("take_profit_3", 0)),
                    float(risk_levels.get("risk_reward_ratio", 0)),
                ]
                
                logger.info(f"Row data prepared: {len(row_data)} columns")
                
            except Exception as e:
                logger.error(f"Error preparing row data: {e}")
                return False

            # Append to worksheet
            logger.info("Appending row to worksheet...")
            worksheet.append_row(row_data)

            logger.info(f"Detailed signal logged successfully: {symbol} - {signal_type}")
            return True

        except Exception as e:
            logger.error(f"Error logging detailed signal to sheets: {e}")
            return False

    def log_trading_journal(self, analysis: Dict) -> bool:
        """
        Log tradeable signals to Trading_Journal worksheet
        
        Args:
            analysis: Analysis from SignalDetector
            
        Returns:
            bool: True if logged successfully
        """
        # Check connection status
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping trading journal log")
            return False

        try:
            signals = analysis.get("signals", {})
            
            # Check for tradeable signals
            if not self._has_tradeable_signal(signals):
                logger.debug(f"No tradeable signals found for {analysis.get('symbol', 'UNKNOWN')}")
                return False
            
            # Headers for Trading Journal
            headers = [
                "Date", "Symbol", "Signal", "Entry", "SL", 
                "TP1", "TP2", "TP3", "Win/Loss", "Win Rate"
            ]

            # Prepare worksheet (use main worksheet if available)
            worksheet = self.worksheet
            if not worksheet:
                worksheet = self._ensure_worksheet_exists("Trading_Journal", headers)
                if not worksheet:
                    logger.error("Failed to create/access Trading_Journal worksheet")
                    return False

            risk_levels = analysis.get("risk_levels", {})
            
            # Get trade direction
            trade_direction = self._get_trade_direction(signals)
            if not trade_direction:
                logger.debug(f"No clear trade direction for {analysis.get('symbol', 'UNKNOWN')}")
                return False
            
            # Check for duplicates before logging
            symbol = analysis.get("symbol", "")
            records = worksheet.get_all_records()
            today = datetime.now().strftime("%Y-%m-%d")

            for rec in records:
                if (rec.get("Date") == today and 
                    rec.get("Symbol") == symbol and 
                    rec.get("Signal") == trade_direction and
                    not rec.get("Win/Loss")):
                    logger.warning(f"Duplicate signal blocked: {symbol} {trade_direction}")
                    return False

            # Prepare row data
            row_data = [
                datetime.now().strftime("%Y-%m-%d"),
                analysis.get("symbol", ""),
                trade_direction,
                float(risk_levels.get("entry_price", 0)),
                float(risk_levels.get("stop_loss", 0)),
                float(risk_levels.get("take_profit_1", 0)),
                float(risk_levels.get("take_profit_2", 0)),
                float(risk_levels.get("take_profit_3", 0)),
                "",
                ""
            ]

            # Append to worksheet
            worksheet.append_row(row_data)
            logger.info(f"Trading journal logged: {analysis.get('symbol')} - {trade_direction} (Signal: {self._determine_signal_type(signals)})")
            return True

        except Exception as e:
            logger.error(f"Error logging trading journal: {e}")
            return False

    def log_tp_hit(self, position_data: Dict, tp_info: Dict) -> bool:
        """
        Log Take Profit hit to Google Sheets
        
        Args:
            position_data: Position information
            tp_info: TP hit information
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping TP hit log")
            return False

        try:
            symbol = position_data.get("symbol", "")
            entry_price = position_data.get("entry_price", 0)
            tp_price = tp_info.get("target_price", 0)
            current_price = tp_info.get("price", 0)
            
            # Determine which TP was hit from the price
            tp_levels = position_data.get("tp_levels", {})
            tp_level = "TP1"  # Default
            
            for tp_name, tp_target in tp_levels.items():
                if abs(tp_target - tp_price) < 0.001:  # Account for floating point precision
                    tp_level = tp_name
                    break
            
            return self.update_trading_result(symbol, entry_price, f"take_profit_{tp_level[-1]}", current_price)

        except Exception as e:
            logger.error(f"Error logging TP hit: {e}")
            return False

    def log_sl_hit(self, position_data: Dict, sl_info: Dict) -> bool:
        """
        Log Stop Loss hit to Google Sheets
        
        Args:
            position_data: Position information
            sl_info: SL hit information
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping SL hit log")
            return False

        try:
            symbol = position_data.get("symbol", "")
            entry_price = position_data.get("entry_price", 0)
            current_price = sl_info.get("price", 0)
            
            return self.update_trading_result(symbol, entry_price, "stop_loss", current_price)

        except Exception as e:
            logger.error(f"Error logging SL hit: {e}")
            return False

    def log_position_close(self, position_data: Dict) -> bool:
        """
        Log position closure to Google Sheets
        
        Args:
            position_data: Position information
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping position close log")
            return False

        try:
            symbol = position_data.get("symbol", "")
            entry_price = position_data.get("entry_price", 0)
            close_reason = position_data.get("close_reason", "MANUAL")
            
            # Determine if it's a win or loss based on close reason
            if close_reason in ["ALL_TP_HIT", "TP3_HIT"]:
                result_type = "WIN"
            elif close_reason == "SL_HIT":
                result_type = "LOSS"
            else:
                result_type = "MANUAL_CLOSE"
            
            # Update the trading journal
            return self._update_position_status(symbol, entry_price, result_type)

        except Exception as e:
            logger.error(f"Error logging position close: {e}")
            return False

    def update_trading_result(self, symbol: str, entry_price: float, triggered_level: str, triggered_price: float) -> bool:
        """
        Update trading result with TP/SL marks
        
        Args:
            symbol: Trading symbol
            entry_price: Entry price to match
            triggered_level: Level that was triggered (e.g., "take_profit_1", "stop_loss")
            triggered_price: Price at which level was triggered
            
        Returns:
            bool: True if updated successfully
        """
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping result update")
            return False

        try:
            # Access Trading Journal worksheet - FIXED: use safe access
            worksheet = self.worksheet
            if not worksheet:
                logger.error("Cannot access Trading_Journal worksheet")
                return False

            records = worksheet.get_all_records()

            # Find matching row
            for i, record in enumerate(records, start=2):  # start=2 because row 1 is header
                if (record.get("Symbol") == symbol and 
                    abs(float(record.get("Entry", 0)) - entry_price) < 0.001 and  # Account for floating point precision
                    not record.get("Win/Loss")):  # Not yet updated
                    
                    # Update based on triggered level
                    if triggered_level == "stop_loss":
                        # Mark SL column (column E = 5)
                        current_sl = worksheet.cell(i, 5).value
                        worksheet.update_cell(i, 5, f"‚ùå {current_sl}")
                        worksheet.update_cell(i, 9, "LOSS")  # Win/Loss column
                        logger.info(f"Updated LOSS: {symbol} hit SL at {triggered_price}")
                        
                    elif triggered_level.startswith("take_profit"):
                        # Mark appropriate TP column
                        if triggered_level == "take_profit_1":
                            col = 6  # TP1 column
                        elif triggered_level == "take_profit_2":
                            col = 7  # TP2 column
                        elif triggered_level == "take_profit_3":
                            col = 8  # TP3 column
                        else:
                            continue
                        
                        current_tp = worksheet.cell(i, col).value
                        worksheet.update_cell(i, col, f"‚úÖ {current_tp}")
                        worksheet.update_cell(i, 9, "WIN")  # Win/Loss column
                        logger.info(f"Updated WIN: {symbol} hit {triggered_level} at {triggered_price}")
                    
                    # Update Win Rate
                    self._update_win_rate(worksheet)
                    
                    logger.info(f"Trading result updated successfully: {symbol} - {triggered_level}")
                    return True
            
            logger.warning(f"No matching trade found for {symbol} at {entry_price}")
            return False

        except Exception as e:
            logger.error(f"Error updating trading result: {e}")
            return False

    def _update_position_status(self, symbol: str, entry_price: float, status: str) -> bool:
        """Update position status in trading journal"""
        try:
            worksheet = self.worksheet
            if not worksheet:
                logger.error("Cannot access Trading_Journal worksheet")
                return False

            records = worksheet.get_all_records()

            # Find matching row
            for i, record in enumerate(records, start=2):
                if (record.get("Symbol") == symbol and 
                    abs(float(record.get("Entry", 0)) - entry_price) < 0.001 and
                    not record.get("Win/Loss")):
                    
                    worksheet.update_cell(i, 9, status)  # Win/Loss column
                    self._update_win_rate(worksheet)
                    
                    logger.info(f"Updated position status: {symbol} - {status}")
                    return True
            
            return False

        except Exception as e:
            logger.error(f"Error updating position status: {e}")
            return False

    def _update_win_rate(self, worksheet):
        """Calculate and update Win Rate"""
        try:
            # Get all records
            records = worksheet.get_all_records()
            
            # Filter completed trades
            completed_trades = [r for r in records if r.get("Win/Loss") in ["WIN", "LOSS"]]
            
            if not completed_trades:
                logger.debug("No completed trades found for win rate calculation")
                return
            
            # Calculate Win Rate
            total = len(completed_trades)
            wins = len([r for r in completed_trades if r.get("Win/Loss") == "WIN"])
            win_rate = f"{round(wins / total * 100, 1)}%"
            
            logger.info(f"Win Rate calculated: {wins}/{total} = {win_rate}")
            
            # Update Win Rate in the last row with data
            for i in range(len(records), 1, -1):  # From bottom to top
                if records[i-2].get("Win/Loss"):  # i-2 because records start from 0
                    worksheet.update_cell(i, 10, win_rate)  # Win Rate column (column J = 10)
                    logger.info(f"Win Rate updated in row {i}: {win_rate}")
                    break
                    
        except Exception as e:
            logger.error(f"Error updating win rate: {e}")

    def test_connection(self) -> bool:
        """Test Google Sheets connection"""
        try:
            if not self.spreadsheet:
                logger.error("Spreadsheet object is None")
                return False

            title = self.spreadsheet.title
            worksheet_count = len(self.spreadsheet.worksheets())
            
            logger.info(f"Google Sheets connection test successful!")
            logger.info(f"  Spreadsheet: {title}")
            logger.info(f"  Worksheets: {worksheet_count}")
            logger.info(f"  URL: https://docs.google.com/spreadsheets/d/{self.spreadsheet_id}")
            
            return True

        except Exception as e:
            logger.error(f"Google Sheets connection test failed: {e}")
            return False

    def get_trading_statistics(self, days: int = 30) -> Dict:
        """
        Generate trading statistics
        
        Args:
            days: Number of days to analyze (simplified - uses all data for now)
            
        Returns:
            Dict with trading statistics
        """
        if not self._initialized or not self.spreadsheet:
            return {}

        try:
            worksheet = self.worksheet
            if not worksheet:
                return {}

            records = worksheet.get_all_records()
            
            # Filter completed trades (simplified - use all for now)
            completed_trades = [r for r in records if r.get("Win/Loss") in ["WIN", "LOSS"]]
            
            if not completed_trades:
                return {"total_trades": 0, "win_rate": 0, "total_pnl": 0}
            
            # Calculate statistics
            total_trades = len(completed_trades)
            wins = len([r for r in completed_trades if r.get("Win/Loss") == "WIN"])
            win_rate = round(wins / total_trades * 100, 1) if total_trades > 0 else 0
            
            return {
                "total_trades": total_trades,
                "wins": wins,
                "losses": total_trades - wins,
                "win_rate": win_rate,
                "total_pnl": 0,  # TODO: Calculate from actual prices
                "best_performer": "",
                "worst_performer": "",
                "version": "2.0-refactored"
            }
            
        except Exception as e:
            logger.error(f"Error getting trading statistics: {e}")
            return {}

    def log_daily_summary(self, summary_data: Dict) -> bool:
        """
        Log daily summary to Google Sheets
        
        Args:
            summary_data: Summary data dictionary
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            return False

        try:
            headers = [
                "Date", "Total_Signals", "Active_Positions", "Closed_Positions", 
                "Total_PnL", "Win_Rate", "Best_Performer", "Worst_Performer", "Version"
            ]

            worksheet = self._ensure_worksheet_exists("Daily_Summary", headers)
            if not worksheet:
                return False

            row_data = [
                summary_data.get("date", ""),
                summary_data.get("total_signals", 0),
                summary_data.get("active_positions", 0),
                summary_data.get("closed_positions", 0),
                summary_data.get("total_pnl", 0),
                summary_data.get("win_rate", 0),
                summary_data.get("best_performer", ""),
                summary_data.get("worst_performer", ""),
                summary_data.get("version", "2.0-refactored"),
            ]

            worksheet.append_row(row_data)
            logger.info(f"Daily summary logged for {summary_data.get('date', 'today')}")
            return True

        except Exception as e:
            logger.error(f"Error logging daily summary: {e}")
            return False

    def log_position_update(self, update_data: Dict) -> bool:
        """
        Log position update information
        
        Args:
            update_data: Position update data
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            return False

        try:
            # Extract position and update information
            position = update_data.get("position", {})
            updates = update_data.get("updates", {})
            
            if not position or not updates:
                return False
            
            symbol = position.get("symbol", "")
            entry_price = position.get("entry_price", 0)
            
            # Process TP hits
            for tp_level in ['TP1', 'TP2', 'TP3']:
                tp_key = f'{tp_level}_hit'
                if updates.get(tp_key, {}).get('hit', False):
                    tp_info = updates[tp_key]
                    self.log_tp_hit(position, tp_info)
            
            # Process SL hits
            if updates.get('sl_hit', {}).get('hit', False):
                sl_info = updates['sl_hit']
                self.log_sl_hit(position, sl_info)
            
            # Process position closure
            if updates.get('position_closed', False):
                self.log_position_close(position)
            
            return True

        except Exception as e:
            logger.error(f"Error logging position update: {e}")

            return False

    def shutdown(self):
        """Shutdown SheetsLogger"""
        try:
            logger.info("Shutting down SheetsLogger v2.0...")
            # Clean up any resources if needed
            self._cached_worksheet = None
            logger.info("SheetsLogger shutdown complete")
        except Exception as e:
            logger.error(f"Error during SheetsLogger shutdown: {e}")"""Signal detection logic combining all indicators - CONSERVATIVE MODE v2.0"""

import logging
from datetime import datetime
import time
from typing import Dict, List, Optional, Tuple

from .indicators import TechnicalIndicators
from ..utils.core_utils import ErrorHandler
from ..utils.data_types import DataConverter

logger = logging.getLogger(__name__)


class SignalDetector:
    """Detect trading signals using Squeeze + MACD Uncle Cholok + RSI strategy - CONSERVATIVE"""

    def __init__(self, config: Dict):
        """Initialize signal detector with refactored services"""
        # Extract refactored services
        self.data_manager = config["data_manager"]
        self.position_manager = config["position_manager"]
        self.config_manager = config["config_manager"]
        
        # Initialize utilities
        self.indicators = TechnicalIndicators()
        self.data_converter = DataConverter()
        self.active_positions = set()
        
        # Get configuration from ConfigManager
        self.risk_management = self._load_risk_config()
        self.indicator_settings = self._load_indicator_config()
        
        logger.info("‚úÖ SignalDetector initialized - CONSERVATIVE MODE")
    
    def _load_risk_config(self) -> Dict:
        """Load risk management configuration from Config"""
        try:
            # ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Config class
            from config.settings import Config
            return Config.RISK_MANAGEMENT
        except Exception as e:
            logger.warning(f"Error loading risk config, using defaults: {e}")
            return {
                "4h": {"tp_levels": [1.2, 1.5, 2.0], "sl_level": 1.5},
                "1d": {"tp_levels": [3.0, 5.0, 7.0], "sl_level": 3.0}
            }
    
    def _load_indicator_config(self) -> Dict:
        """Load indicator configuration"""
        try:
            return {
                "squeeze": {"length": 20, "bb_mult": 2.0, "kc_mult": 1.5},
                "macd": {"fast": 8, "slow": 17, "signal": 9}, 
                "rsi": {
                    "period": 14, 
                    "oversold": 40,
                    "overbought": 60
                }
            }
        except Exception as e:
            logger.warning(f"Error loading indicator config, using defaults: {e}")
            return {
                "squeeze": {"length": 20, "bb_mult": 2.0, "kc_mult": 1.5},
                "macd": {"fast": 8, "slow": 17, "signal": 9}, 
                "rsi": {"period": 14, "oversold": 40, "overbought": 60}
            }

    @ErrorHandler.service_error_handler("SignalDetector")
    def analyze_symbol(self, symbol: str, timeframe: str = "4h") -> Optional[Dict]:
        """Analyze symbol using refactored data flow"""
        try:
            logger.info(f"üîç Analyzing {symbol} on {timeframe} (CONSERVATIVE)")

            # Get data from DataManager
            df = self.data_manager.get_klines(symbol, timeframe, limit=100)

            if df is None:
                logger.warning(f"No data available for {symbol} {timeframe}")
                return {
                    "error": f"Failed to fetch data for {symbol}",
                    "symbol": symbol,
                    "timeframe": timeframe,
                    "timestamp": datetime.now().isoformat(),
                    "version": "2.0-conservative"
                }

            # Validate data quality
            if not self.data_converter.validate_dataframe(df):
                logger.warning(f"Invalid DataFrame for {symbol} {timeframe}")
                return None

            # Calculate all indicators
            analysis = self.indicators.analyze_all_indicators(df, self.indicator_settings)

            # Detect trading signals with CONSERVATIVE logic
            signals = self._detect_signals_improved_fixed(analysis, timeframe, df)

            # Calculate risk management levels  
            risk_levels = self._calculate_risk_levels(
                analysis["current_price"], timeframe, signals
            )

            # Handle position creation with duplicate prevention
            position_created = self._handle_signal_position_fixed(
                symbol, timeframe, signals, analysis["current_price"], risk_levels
            )

            # Create comprehensive result
            result = {
                "symbol": symbol,
                "timeframe": timeframe,
                "timestamp": datetime.now().isoformat(),
                "current_price": analysis["current_price"],
                "version": "2.0-conservative",
                
                # Indicator values
                "indicators": {
                    "squeeze": analysis["squeeze"],
                    "macd": analysis["macd"],
                    "rsi": analysis["rsi"],
                },
                
                # Trading signals
                "signals": signals,
                
                # Risk management
                "risk_levels": risk_levels,
                
                # Overall assessment
                "signal_strength": self._calculate_signal_strength_improved(signals),
                "recommendation": self._get_recommendation_improved(signals),
                
                # Position info
                "position_created": position_created,
                "has_active_position": self._has_active_position_strict(symbol, timeframe),
            }

            # Convert NumPy types for JSON serialization
            result = self.data_converter.sanitize_signal_data(result)

            # Log significant results
            if result.get('recommendation'):
                logger.info(f"Analysis complete for {symbol}: {result['recommendation']}")
                if position_created:
                    logger.info(f"üÜï Created position for {symbol} {timeframe}")
            
            return result

        except Exception as e:
            logger.error(f"Analysis error for {symbol}: {str(e)}")
            return {
                "error": f"Analysis error for {symbol}: {str(e)}",
                "symbol": symbol,
                "timeframe": timeframe,
                "timestamp": datetime.now().isoformat(),
                "version": "2.0-conservative"
            }

    def _has_active_position_strict(self, symbol: str, timeframe: str) -> bool:
        """‚úÖ STRICT check if position exists - prevent duplicates"""
        try:
            # Check 1: Via PositionManager
            position = self.position_manager.get_position_status(symbol, timeframe)
            if position is not None:
                logger.debug(f"Found active position via PositionManager: {symbol} {timeframe}")
                return True
            
            # Check 2: Check all direction combinations
            for direction in ["LONG", "SHORT"]:
                position_id = f"{symbol}_{timeframe}_{direction}"
                if position_id in self.position_manager.positions:
                    pos_data = self.position_manager.positions[position_id]
                    if pos_data.get('status') == 'ACTIVE':
                        logger.debug(f"Found active position by ID: {position_id}")
                        return True
            
            # Check 3: In active_positions set
            if symbol in self.active_positions:
                logger.debug(f"Found in active_positions set: {symbol}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking active position: {e}")
            return True  # Return True on error to prevent duplicate

    @ErrorHandler.service_error_handler("SignalDetector") 
    def _handle_signal_position_fixed(
        self, symbol: str, timeframe: str, signals: Dict, current_price: float, risk_levels: Dict
    ) -> bool:
        """Handle position creation - only blocks if active position exists"""
        try:
            # Check if we have a valid signal
            has_signal = signals.get("buy") or signals.get("short")
        
            if not has_signal:
                return False
        
            # Check if active position exists (no cooldown check)
            if symbol in self.active_positions:
                logger.warning(f"‚ö†Ô∏è {symbol} already in active positions set")
                return False

            # Check via PositionManager strictly
            existing_position = self.position_manager.get_position_status(symbol, timeframe)
            if existing_position:
                logger.warning(f"‚ö†Ô∏è Position already exists: {symbol} {timeframe}")
                return False
        
            # Check all possible position IDs
            direction = "LONG" if signals.get("buy") else "SHORT"
            for dir_check in ["LONG", "SHORT"]:
                check_id = f"{symbol}_{timeframe}_{dir_check}"
                if check_id in self.position_manager.positions:
                    if self.position_manager.positions[check_id].get('status') == 'ACTIVE':
                        logger.warning(f"‚ö†Ô∏è Found active {dir_check} position: {symbol} {timeframe}")
                        return False
        
            # Create signal data for position creation
            signal_data = {
                "symbol": symbol,
                "timeframe": timeframe,
                "direction": direction,
                "current_price": current_price,
                "signal_strength": self._calculate_signal_strength_improved(signals)
            }
        
            # Create position using PositionManager
            position_id = self.position_manager.create_position(signal_data)
        
            if position_id:
                logger.info(f"‚úÖ Created {direction} position: {symbol} {timeframe} @ {current_price}")
            
                # Update tracking (no cooldown timestamp)
                self.active_positions.add(symbol)
            
                return True
            else:
                logger.warning(f"Failed to create position for {symbol} {timeframe}")
                return False
            
        except Exception as e:
            logger.error(f"Error handling signal position: {e}")
            return False

    def _detect_signals_improved_fixed(self, analysis: Dict, timeframe: str = "1d", df=None) -> Dict[str, bool]:
        """
        1D: CDC ActionZone (EMA 12/26 Crossover)
        4H: RSI + MACD + Enhanced filters (7 conditions)
        """
        try:
            import pandas as pd
            
            # Validate
            if df is None or 'close' not in df.columns:
                logger.warning("Invalid dataframe")
                return {"buy": False, "short": False, "sell": False, "cover": False}
            
            # ========================================
            # 1D: CDC ACTIONZONE (EMA CROSSOVER)
            # ========================================
            if timeframe == "1d":
                if len(df) < 30:
                    logger.warning(f"Insufficient data: {len(df)} candles")
                    return {"buy": False, "short": False, "sell": False, "cover": False}
                
                # Calculate EMA 12 and 26
                df['ema12'] = df['close'].ewm(span=12, adjust=False).mean()
                df['ema26'] = df['close'].ewm(span=26, adjust=False).mean()
                
                # Current values
                ema12_curr = df['ema12'].iloc[-1]
                ema26_curr = df['ema26'].iloc[-1]
                ema12_prev = df['ema12'].iloc[-2]
                ema26_prev = df['ema26'].iloc[-2]
                price_curr = df['close'].iloc[-1]
                
                # Check crossover
                ema_cross_up = (ema12_prev <= ema26_prev) and (ema12_curr > ema26_curr)
                ema_cross_down = (ema12_prev >= ema26_prev) and (ema12_curr < ema26_curr)
                
                # CDC ActionZone conditions
                # GREEN: EMA12 > EMA26 AND Price > EMA12
                buy_signal = ema_cross_up and (price_curr > ema12_curr)
                
                # RED: EMA12 < EMA26 AND Price < EMA12
                short_signal = ema_cross_down and (price_curr < ema12_curr)
                
                # Log
                if buy_signal:
                    logger.info(
                        f"üü¢ 1D CDC BUY | "
                        f"EMA12: {ema12_curr:.2f} > EMA26: {ema26_curr:.2f} | "
                        f"Price: {price_curr:.2f} > EMA12"
                    )
                elif short_signal:
                    logger.info(
                        f"üî¥ 1D CDC SELL | "
                        f"EMA12: {ema12_curr:.2f} < EMA26: {ema26_curr:.2f} | "
                        f"Price: {price_curr:.2f} < EMA12"
                    )
                else:
                    logger.debug(
                        f"1D No signal | EMA12: {ema12_curr:.2f}, EMA26: {ema26_curr:.2f}"
                    )
                
                return {
                    "buy": buy_signal,
                    "short": short_signal,
                    "sell": False,
                    "cover": False
                }
            
            # ========================================
            # 4H: 5 CONDITIONS (‡∏•‡∏ö MA50 + Volume)
            # ========================================
            else:  # timeframe == "4h"
                if len(df) < 30:
                    logger.warning(f"Insufficient data: {len(df)} candles")
                    return {"buy": False, "short": False, "sell": False, "cover": False}
                
                # Calculate RSI(14)
                from ta.momentum import RSIIndicator
                rsi_indicator = RSIIndicator(df['close'], window=14)
                df['rsi'] = rsi_indicator.rsi()
                df['rsi_ma'] = df['rsi'].rolling(window=14).mean()
                
                # RSI values
                rsi_current = df['rsi'].iloc[-1]
                rsi_ma_current = df['rsi_ma'].iloc[-1]
                rsi_prev = df['rsi'].iloc[-2]
                rsi_ma_prev = df['rsi_ma'].iloc[-2]
                
                # MACD
                macd_data = analysis.get("macd", {})
                macd_cross = macd_data.get("cross_direction", "NONE")
                macd_line = macd_data.get("macd_line", 0)
                
                # Squeeze
                squeeze_data = analysis.get("squeeze", {})
                squeeze_off = squeeze_data.get("squeeze_off", False)
                
                # Check NaN
                if any(pd.isna([rsi_current, rsi_ma_current, rsi_prev, rsi_ma_prev])):
                    logger.warning("NaN values in RSI")
                    return {"buy": False, "short": False, "sell": False, "cover": False}
                
                # RSI Crossovers
                rsi_cross_up = (rsi_prev <= rsi_ma_prev) and (rsi_current > rsi_ma_current)
                rsi_cross_down = (rsi_prev >= rsi_ma_prev) and (rsi_current < rsi_ma_current)
                
                # 4H Signals (5 conditions)
                buy_signal = (
                    rsi_cross_up and 
                    macd_cross == "UP" and 
                    macd_line > 0 and
                    squeeze_off
                )
                
                short_signal = (
                    rsi_cross_down and 
                    macd_cross == "DOWN" and 
                    macd_line < 0 and
                    squeeze_off
                )
                
                # Log
                if buy_signal:
                    logger.info(
                        f"üü¢ 4H LONG | "
                        f"RSI: {rsi_prev:.2f}‚Üí{rsi_current:.2f} | "
                        f"MACD: {macd_cross} ({macd_line:.6f}) | "
                        f"Squeeze: OFF"
                    )
                elif short_signal:
                    logger.info(
                        f"üî¥ 4H SHORT | "
                        f"RSI: {rsi_prev:.2f}‚Üí{rsi_current:.2f} | "
                        f"MACD: {macd_cross} ({macd_line:.6f}) | "
                        f"Squeeze: OFF"
                    )
                else:
                    logger.debug(
                        f"4H No signal | RSI: {rsi_current:.2f}, "
                        f"MACD: {macd_cross}, Squeeze: {squeeze_off}"
                    )
                
                return {
                    "buy": buy_signal,
                    "short": short_signal,
                    "sell": False,
                    "cover": False
                }
        
        except Exception as e:
            logger.error(f"Error detecting signals: {e}", exc_info=True)
            return {"buy": False, "short": False, "sell": False, "cover": False}

    def _check_market_trend_enhanced(self, df) -> str:
        """Conservative trend detection using MA20 and MA50"""
        try:
            close = df['close']
            
            # Calculate MAs
            ma_20 = close.rolling(20).mean()
            ma_50 = close.rolling(50).mean() if len(close) >= 50 else None
            
            current_price = close.iloc[-1]
            ma_20_current = ma_20.iloc[-1]
            
            # Case 1: Have MA50 - strict check
            if ma_50 is not None:
                ma_50_current = ma_50.iloc[-1]
                
                # Uptrend: Price > MA20 AND MA20 > MA50
                if current_price > ma_20_current and ma_20_current > ma_50_current:
                    return "UP"
                
                # Downtrend: Price < MA20 AND MA20 < MA50
                elif current_price < ma_20_current and ma_20_current < ma_50_current:
                    return "DOWN"
                
                else:
                    return "NEUTRAL"
            
            # Case 2: No MA50 - use only MA20
            else:
                if current_price > ma_20_current:
                    return "UP"
                elif current_price < ma_20_current:
                    return "DOWN"
                else:
                    return "NEUTRAL"
                    
        except Exception as e:
            logger.error(f"Error checking market trend: {e}")
            return "NEUTRAL"

    def _get_recommendation_improved(self, signals: Dict[str, bool]) -> str:
        """Generate recommendation based on signals"""
        if signals.get("buy"):
            return "LONG"
        elif signals.get("short"):
            return "SHORT"
        else:
            return ""

    def _calculate_signal_strength_improved(self, signals: Dict[str, bool]) -> int:
        """Calculate signal strength (0-100)"""
        if signals.get("buy") or signals.get("short"):
            return 100  # Signals that pass conservative conditions = 100%
        else:
            return 0

    def _calculate_risk_levels(self, current_price: float, timeframe: str, signals: Dict) -> Dict:
        """Calculate Stop Loss and Take Profit levels"""
        try:
            risk_config = self.risk_management.get(
                timeframe, self.risk_management.get("4h", {})
            )

            tp_percentages = risk_config.get("tp_levels", [3.0, 5.0, 7.0])
            sl_percentage = risk_config.get("sl_level", 3.0)

            risk_levels = {"timeframe": timeframe, "entry_price": current_price}

            # Determine signal direction
            is_long_signal = signals.get("buy", False)
            is_short_signal = signals.get("short", False)

            # Calculate levels based on signal direction
            if is_long_signal:
                risk_levels.update({
                    "direction": "LONG",
                    "stop_loss": current_price * (1 - sl_percentage / 100),
                    "take_profit_1": current_price * (1 + tp_percentages[0] / 100),
                    "take_profit_2": current_price * (1 + tp_percentages[1] / 100),
                    "take_profit_3": current_price * (1 + tp_percentages[2] / 100),
                    "risk_reward_ratio": tp_percentages[0] / sl_percentage,
                })

            elif is_short_signal:
                risk_levels.update({
                    "direction": "SHORT",
                    "stop_loss": current_price * (1 + sl_percentage / 100),
                    "take_profit_1": current_price * (1 - tp_percentages[0] / 100),
                    "take_profit_2": current_price * (1 - tp_percentages[1] / 100),
                    "take_profit_3": current_price * (1 - tp_percentages[2] / 100),
                    "risk_reward_ratio": tp_percentages[0] / sl_percentage,
                })

            return risk_levels

        except Exception as e:
            logger.error(f"Error calculating risk levels: {e}")
            return {"error": "Failed to calculate risk levels"}

    def scan_multiple_symbols(self, symbols: List[str], timeframes: List[str] = None) -> List[Dict]:
        """Scan multiple symbols for signals across different timeframes"""
        if timeframes is None:
            timeframes = ["4h", "1d"]

        results = []

        for symbol in symbols:
            for timeframe in timeframes:
                logger.info(f"üîç Scanning {symbol} on {timeframe}")
                result = self.analyze_symbol(symbol, timeframe)
                if result:
                    results.append(result)
                time.sleep(0.2)

        return results

    def get_active_signals(self, symbols: List[str], timeframes: List[str] = None) -> List[Dict]:
        """Get only signals with active recommendations"""
        if timeframes is None:
            timeframes = ["4h", "1d"]

        all_results = self.scan_multiple_symbols(symbols, timeframes)

        # Filter only results with actual recommendations
        active_signals = []
        for result in all_results:
            if "signals" in result and result.get("recommendation"):
                signals = result["signals"]
                if signals.get("buy") or signals.get("short"):
                    active_signals.append(result)

        logger.info(f"Found {len(active_signals)} active signals out of {len(all_results)} scans")
        return active_signals

    def scan_all_symbols(self, symbols: List[str] = None, timeframes: List[str] = None) -> List[Dict]:
        """Scan all symbols and return all results"""
        if symbols is None:
            symbols = ["BTCUSDT", "ETHUSDT", "BNBUSDT"]
        if timeframes is None:
            timeframes = ["4h", "1d"]
            
        return self.scan_multiple_symbols(symbols, timeframes)

    def validate_signal_quality(self, analysis: Dict) -> Dict:
        """Validate signal quality and reliability"""
        try:
            quality_score = 0
            quality_factors = []

            indicators = analysis.get("indicators", {})
            signals = analysis.get("signals", {})

            # Check squeeze momentum quality
            squeeze = indicators.get("squeeze", {})
            if squeeze.get("squeeze_off"):
                quality_score += 30
                quality_factors.append("Squeeze breakout confirmed")

                # Check momentum strength
                details = squeeze.get("details", {})
                momentum_value = abs(details.get("momentum_value", 0))
                if momentum_value > 0.001:
                    quality_score += 10
                    quality_factors.append("Strong momentum")

            # Check MACD quality
            macd = indicators.get("macd", {})
            if macd.get("cross_direction") != "NONE":
                quality_score += 25
                quality_factors.append("MACD cross confirmed")

                # Check if MACD is above/below zero line
                macd_details = macd.get("details", {})
                if macd_details.get("macd_above_zero") and signals.get("buy"):
                    quality_score += 10
                    quality_factors.append("MACD above zero line")
                elif not macd_details.get("macd_above_zero") and signals.get("short"):
                    quality_score += 10
                    quality_factors.append("MACD below zero line")

            # Check RSI quality
            rsi = indicators.get("rsi", {})
            rsi_value = rsi.get("value", 50)
            
            if rsi_value < 40 or rsi_value > 60:
                quality_score += 20
                level = "oversold" if rsi_value < 40 else "overbought"
                quality_factors.append(f"RSI {level} level")

                # Check RSI trend alignment
                rsi_details = rsi.get("details", {})
                rsi_trend = rsi_details.get("rsi_trend", "NEUTRAL")
                if (rsi_value < 40 and rsi_trend == "RISING") or (rsi_value > 60 and rsi_trend == "FALLING"):
                    quality_score += 5
                    quality_factors.append("RSI trend alignment")

            # Signal grade bonus
            if signals.get("buy") or signals.get("short"):
                quality_score += 15
                quality_factors.append("Strong signal grade")

            # Risk-reward assessment
            risk_levels = analysis.get("risk_levels", {})
            risk_reward = risk_levels.get("risk_reward_ratio", 0)
            if risk_reward >= 1.0:
                quality_score += 5
                quality_factors.append("Favorable risk-reward ratio")

            # Cap quality score at 100
            quality_score = min(quality_score, 100)

            return {
                "quality_score": quality_score,
                "quality_factors": quality_factors,
                "risk_reward_ratio": risk_reward,
                "signal_reliability": (
                    "HIGH" if quality_score >= 80
                    else "MEDIUM" if quality_score >= 60 
                    else "LOW"
                ),
            }

        except Exception as e:
            logger.error(f"Error validating signal quality: {e}")
            return {
                "quality_score": 0,
                "quality_factors": [],
                "risk_reward_ratio": 0,
                "signal_reliability": "UNKNOWN",
            }

    # Position Management Integration Methods
    def get_position_summary(self) -> Dict:
        """Get positions summary from PositionManager"""
        try:
            return self.position_manager.get_positions_summary()
        except Exception as e:
            logger.error(f"Error getting position summary: {e}")
            return {"error": str(e)}
    
    def get_position_status(self, symbol: str, timeframe: str) -> Dict:
        """Get specific position status from PositionManager"""
        try:
            position = self.position_manager.get_position_status(symbol, timeframe)
            return {
                "position_found": position is not None,
                "position": position,
                "symbol": symbol,
                "timeframe": timeframe
            }
        except Exception as e:
            logger.error(f"Error getting position status: {e}")
            return {"error": str(e), "position_found": False}
    
    def force_close_position(self, symbol: str, timeframe: str, reason: str = "MANUAL") -> Dict:
        """Force close a position via PositionManager"""
        try:
            # Create position_id in the format expected by PositionManager
            position_id = f"{symbol}_{timeframe}_LONG"  # Try LONG first
            success = self.position_manager.close_position(position_id, reason)
            
            if not success:
                # Try SHORT if LONG doesn't exist
                position_id = f"{symbol}_{timeframe}_SHORT"
                success = self.position_manager.close_position(position_id, reason)
            
            if success:
                # Remove from tracking
                if symbol in self.active_positions:
                    self.active_positions.remove(symbol)
                
                return {
                    "success": True, 
                    "message": f"Closed position for {symbol} {timeframe}",
                    "reason": reason
                }
            else:
                return {
                    "success": False, 
                    "message": f"No active position found for {symbol} {timeframe}"
                }
                
        except Exception as e:
            logger.error(f"Error force closing position: {e}")
            return {"success": False, "error": str(e)}
    
    def update_all_positions(self, current_prices: Dict[str, float]) -> List[Dict]:
        """Update all positions with current prices via PositionManager"""
        try:
            updates = self.position_manager.update_positions()
            
            # Format results for compatibility
            results = []
            for position_id, update_info in updates.items():
                # Extract symbol from position_id (format: SYMBOL_TIMEFRAME_DIRECTION)
                parts = position_id.split('_')
                if len(parts) >= 3:
                    symbol = parts[0]
                    timeframe = parts[1]
                    
                    result = {
                        "symbol": symbol,
                        "timeframe": timeframe,
                        "position_id": position_id,
                        "update_info": update_info
                    }
                    results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"Error updating all positions: {e}")
            return []

    def get_data_storage_stats(self) -> Dict:
        """Get data storage statistics from DataManager"""
        try:
            return self.data_manager.get_cache_stats()
        except Exception as e:
            logger.error(f"Error getting data storage stats: {e}")
            return {"error": str(e)}
    
    def force_data_update(self, symbol: str, timeframe: str):
        """Force data update for symbol/timeframe via DataManager"""
        try:
            # Clear cache to force refresh
            self.data_manager.clear_cache()
            logger.info(f"Forced data update for {symbol} {timeframe}")
        except Exception as e:
            logger.error(f"Error forcing data update for {symbol} {timeframe}: {e}")
    
    def clear_position_history(self):
        """Clear position history for testing"""
        self.active_positions.clear()
        logger.info("Cleared position history and tracking")
    
    def shutdown(self):
        """Shutdown SignalDetector and cleanup resources"""
        try:
            logger.info("Shutting down SignalDetector CONSERVATIVE mode...")
            
            # Clear data manager cache
            if hasattr(self.data_manager, 'clear_cache'):
                self.data_manager.clear_cache()
            
            # Cleanup old positions  
            if hasattr(self.position_manager, 'cleanup_old_positions'):
                self.position_manager.cleanup_old_positions()
            
            # Clear tracking
            self.active_positions.clear()
                
            logger.info("SignalDetector shutdown complete")
            
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")