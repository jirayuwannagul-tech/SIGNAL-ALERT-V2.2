========================================
SIGNAL-ALERT V2.2 - COMPLETE CODE
Generated: $(date)
========================================


========================================
FILE: config/settings.py
========================================

"""Configuration settings for Squeeze Bot - COMPLETE v2.0"""

import os
from typing import Dict, List, Optional
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class Config:
    """Base configuration class with layer organization - COMPLETE v2.0"""

    # ================================================================
    # üåê LAYER 1: System Basics (updated for refactored architecture)
    # ================================================================

    # System version and identification
    VERSION = "2.0-refactored"
    DEBUG = os.getenv("DEBUG", "false").lower() == "true"
    
    # Basic settings
    CHECK_INTERVAL = int(os.getenv("CHECK_INTERVAL", "900"))  # 15 minutes
    TIMEFRAMES = ["4h", "1d"]
    MAX_CONCURRENT_REQUESTS = int(os.getenv("MAX_CONCURRENT_REQUESTS", "10"))
    
    # Refactored service settings
    SIGNAL_COOLDOWN_MINUTES = int(os.getenv("SIGNAL_COOLDOWN_MINUTES", "30"))
    PRICE_MONITOR_INTERVAL = int(os.getenv("PRICE_MONITOR_INTERVAL", "30"))  # seconds

    # ================================================================
    # üì° LAYER 2: External API Connections (updated for ConfigManager)
    # ================================================================

    # Binance API
    BINANCE_BASE_URL = os.getenv("BINANCE_BASE_URL", "https://fapi.binance.com/fapi/v1")
    BINANCE_API_KEY = os.getenv("BINANCE_API_KEY", "")
    BINANCE_SECRET_KEY = os.getenv("BINANCE_SECRET_KEY", "")

    # LINE Bot (updated for ConfigManager)
    LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "")
    LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET", "")
    LINE_USER_ID = os.getenv("LINE_USER_ID", "")

    # Google Sheets (updated for ConfigManager)
    GOOGLE_SHEETS_ID = os.getenv("GOOGLE_SHEETS_ID", "")
    GOOGLE_SHEETS_CREDENTIALS = os.getenv("GOOGLE_SHEETS_CREDENTIALS", "")
    
    # Fix: Unescape JSON string
    _creds_json = os.getenv("GOOGLE_CREDENTIALS_JSON", "")
    if _creds_json:
        # Railway may wrap with quotes, remove them
        _creds_json = _creds_json.strip('"')
        # Replace escaped quotes and newlines
        _creds_json = _creds_json.replace('\\"', '"').replace('\\n', '\n')
    
    GOOGLE_APPLICATION_CREDENTIALS = _creds_json or os.getenv("GOOGLE_APPLICATION_CREDENTIALS", "/app/credentials.json")

    # ================================================================
    # üí∞ LAYER 3: Symbol Selection (COMPLETE - 50 symbols)
    # ================================================================

    # Top 50 Crypto by Market Cap - ALL VERIFIED on Binance Futures ‚úÖ
    DEFAULT_SYMBOLS = [
        # Top 10 - Major coins
        "BTCUSDT", "ETHUSDT", "XRPUSDT", "BNBUSDT", "SOLUSDT",
        "ADAUSDT", "DOGEUSDT", "TRXUSDT", "TONUSDT", "LINKUSDT",
        
        # 11-20 - Large caps
        "AVAXUSDT", "DOTUSDT", "LTCUSDT", "NEARUSDT", "UNIUSDT",
        "ICPUSDT", "APTUSDT", "ATOMUSDT", "HBARUSDT", "FILUSDT",
        
        # 21-30 - Mid caps
        "ARBUSDT", "OPUSDT", "SUIUSDT", "INJUSDT", "STXUSDT",
        "IMXUSDT", "AAVEUSDT", "GRTUSDT", "RENDERUSDT", "TIAUSDT",
        
        # 31-40 - DeFi & Layer 1/2
        "POLUSDT", "MKRUSDT", "ALGOUSDT", "LDOUSDT", "VETUSDT",
        "SEIUSDT", "TAOUSDT", "FTMUSDT", "KAVAUSDT", "RUNEUSDT",
        
        # 41-50 - Gaming, Metaverse & Others
        "BEAMXUSDT", "SANDUSDT", "MANAUSDT", "AXSUSDT", "FLOWUSDT",
        "CHZUSDT", "ENSUSDT", "APEUSDT", "QNTUSDT", "EGLDUSDT"
    ]

    # Priority symbols - Updated to 10 for better coverage
    PRIORITY_SYMBOLS = [
        "BTCUSDT", "ETHUSDT", "BNBUSDT", "XRPUSDT", "SOLUSDT",
        "ADAUSDT", "DOGEUSDT", "AVAXUSDT", "LINKUSDT", "DOTUSDT"
    ]

    # ================================================================
    # üìä LAYER 4: Technical Indicators (COMPLETE with ATR)
    # ================================================================

    INDICATORS = {
        "squeeze": {
            "length": int(os.getenv("SQUEEZE_LENGTH", "20")),
            "bb_mult": float(os.getenv("SQUEEZE_BB_MULT", "2.0")),
            "kc_mult": float(os.getenv("SQUEEZE_KC_MULT", "1.5")),
            "use_true_range": True
        },
        "macd": {
            "fast": int(os.getenv("MACD_FAST", "8")),
            "slow": int(os.getenv("MACD_SLOW", "17")),
            "signal": int(os.getenv("MACD_SIGNAL", "9"))
        },
        "rsi": {
            "period": int(os.getenv("RSI_PERIOD", "14")),
            "oversold": int(os.getenv("RSI_OVERSOLD", "40")),
            "overbought": int(os.getenv("RSI_OVERBOUGHT", "60"))
        },
        "atr": {
            "period": int(os.getenv("ATR_PERIOD", "14")),
            "min_atr_multiplier": float(os.getenv("MIN_ATR_MULT", "0.5"))
        }
    }

    # ================================================================
    # üí∏ LAYER 5: Risk Management (updated for PositionManager)
    # ================================================================

    RISK_MANAGEMENT = {
        "15m": {
            "tp_levels": [0.5, 1.0, 1.5],
            "sl_level": 0.5,
            "max_risk_per_trade": 2.0,
            "max_open_trades": 5
        },
        "4h": {
            "tp_levels": [2.0, 4.0, 6.0],
            "sl_level": 1.5,
            "max_risk_per_trade": 3.0,
            "max_open_trades": 3
        },
        "1d": {
            "tp_levels": [5.0, 7.0, 9.0],
            "sl_level": 5.0,
            "max_risk_per_trade": 3.0,
            "max_open_trades": 2
        }
    }

    # ================================================================
    # üîÑ LAYER 6: Signal Filtering (COMPLETE with liquidity checks)
    # ================================================================

    SIGNAL_FILTERING = {
        "min_signal_strength": int(os.getenv("MIN_SIGNAL_STRENGTH", "70")),
        "require_volume_confirmation": True,
        "min_volume_ratio": float(os.getenv("MIN_VOLUME_RATIO", "1.2")),
        "cooldown_minutes": int(os.getenv("SIGNAL_COOLDOWN", "15")),
        "max_signals_per_day": int(os.getenv("MAX_SIGNALS_PER_DAY", "20")),
        "min_24h_volume_usd": int(os.getenv("MIN_24H_VOLUME", "5000000")),
        "min_open_interest_usd": int(os.getenv("MIN_OPEN_INTEREST", "2000000")),
        "max_spread_percentage": float(os.getenv("MAX_SPREAD_PCT", "0.1"))
    }

    # ================================================================
    # üè∑Ô∏è LAYER 7: Signal Classification
    # ================================================================

    SIGNAL_CATEGORIES = {
        "strong": {
            "min_strength": 90,
            "description": "High confidence signals",
            "notification_priority": "high"
        },
        "medium": {
            "min_strength": 70,
            "description": "Medium confidence signals",
            "notification_priority": "medium"
        },
        "weak": {
            "min_strength": 50,
            "description": "Low confidence signals",
            "notification_priority": "low"
        }
    }

    # ================================================================
    # üì± LAYER 8: Notifications
    # ================================================================

    NOTIFICATIONS = {
        "line_enabled": bool(os.getenv("LINE_NOTIFICATIONS", "true").lower() == "true"),
        "sheets_enabled": bool(os.getenv("SHEETS_LOGGING", "true").lower() == "true"),
        "console_enabled": True,
        "signal_strength_threshold": 75,
        "notification_cooldown": 300,
        "version": "2.0.106"
    }

    # ================================================================
    # üóÉÔ∏è LAYER 9: Data Storage (COMPLETE with performance optimization)
    # ================================================================

    DATA_STORAGE = {
        "enabled": True,
        "cache_duration_hours": 24,
        "max_candles_per_symbol": 500,
        "storage_path": "./data/candles",
        "backup_enabled": True,
        "compression": True,
        "price_cache_timeout": 30,
        "connection_pool_size": 20,
        "batch_size": 10,
        "retry_attempts": 3,
        "retry_delay_seconds": 2,
        "use_compression": True,
        "max_memory_mb": 512
    }

    # ================================================================
    # üóÉÔ∏è LAYER 10: Position Management (RESTORED)
    # ================================================================

    POSITION_MANAGEMENT = {
        "positions_file": "data/positions.json",
        "auto_cleanup_days": 30,
        "max_positions_per_symbol": 1,
        "position_timeout_hours": 168,
        "track_partial_fills": True,
        "calculate_pnl": True
    }

    # ================================================================
    # üìä LAYER 11: Symbol Categories & Tiers (FIXED)
    # ================================================================

    SYMBOL_CATEGORIES = {
        "tier1": ["BTCUSDT", "ETHUSDT"],
        "tier2": ["BTCUSDT", "ETHUSDT", "BNBUSDT", "XRPUSDT", "SOLUSDT",
                  "ADAUSDT", "DOGEUSDT", "AVAXUSDT", "LINKUSDT", "DOTUSDT"],
        "tier3": DEFAULT_SYMBOLS[:30],
        "tier4": DEFAULT_SYMBOLS[30:],
        "layer1": ["BTCUSDT", "ETHUSDT", "SOLUSDT", "ADAUSDT", "AVAXUSDT", 
                   "DOTUSDT", "NEARUSDT", "APTUSDT", "ATOMUSDT", "ICPUSDT",
                   "SUIUSDT", "ALGOUSDT", "SEIUSDT", "FTMUSDT", "EGLDUSDT"],
        "layer2": ["ARBUSDT", "OPUSDT", "POLUSDT", "IMXUSDT", "STXUSDT"],
        "defi": ["UNIUSDT", "AAVEUSDT", "MKRUSDT", "LDOUSDT", "INJUSDT",
                 "RUNEUSDT", "KAVAUSDT", "GRTUSDT"],
        "gaming": ["BEAMXUSDT", "SANDUSDT", "MANAUSDT", "AXSUSDT", 
                   "FLOWUSDT", "CHZUSDT", "APEUSDT", "IMXUSDT"]
    }

    # ================================================================
    # üõ°Ô∏è Validation Methods
    # ================================================================

    @classmethod
    def validate_config(cls) -> List[str]:
        """Validate required configuration settings for v2.0"""
        errors = []

        if not cls.BINANCE_BASE_URL:
            errors.append("BINANCE_BASE_URL is required")

        if not cls.TIMEFRAMES:
            errors.append("At least one timeframe must be specified")

        if not cls.DEFAULT_SYMBOLS:
            errors.append("At least one symbol must be specified")

        required_indicators = ["squeeze", "macd", "rsi"]
        for indicator in required_indicators:
            if indicator not in cls.INDICATORS:
                errors.append(f"Missing required indicator: {indicator}")

        for timeframe in cls.TIMEFRAMES:
            if timeframe not in cls.RISK_MANAGEMENT:
                errors.append(f"Missing risk management settings for timeframe: {timeframe}")

        if cls.SIGNAL_COOLDOWN_MINUTES < 1:
            errors.append("SIGNAL_COOLDOWN_MINUTES must be at least 1 minute")

        if cls.PRICE_MONITOR_INTERVAL < 10:
            errors.append("PRICE_MONITOR_INTERVAL must be at least 10 seconds")

        return errors

    @classmethod
    def initialize_config(cls) -> None:
        """Initialize configuration and display status for v2.0"""
        print("=" * 60)
        print(f"üöÄ SIGNAL ALERT SYSTEM v{cls.VERSION} - CONFIGURATION")
        print("=" * 60)
        
        print(f"Check Interval: {cls.CHECK_INTERVAL} seconds")
        print(f"Timeframes: {', '.join(cls.TIMEFRAMES)}")
        print(f"Symbols: {len(cls.DEFAULT_SYMBOLS)} total")
        print(f"Priority Symbols: {len(cls.PRIORITY_SYMBOLS)}")
        print(f"Signal Cooldown: {cls.SIGNAL_COOLDOWN_MINUTES} minutes")
        print(f"Price Monitor Interval: {cls.PRICE_MONITOR_INTERVAL} seconds")
        
        print("\nüì° API Connections:")
        print(f"Binance: {'‚úÖ Configured' if cls.BINANCE_BASE_URL else '‚ùå Not configured'}")
        print(f"Line Bot: {'‚úÖ Configured' if cls.LINE_CHANNEL_ACCESS_TOKEN else '‚ùå Not configured'}")
        print(f"Google Sheets: {'‚úÖ Configured' if cls.GOOGLE_SHEETS_ID else '‚ùå Not configured'}")
        
        print(f"\nüîß Refactored Services:")
        print(f"ConfigManager: ‚úÖ Ready")
        print(f"DataManager: ‚úÖ Ready")
        print(f"PositionManager: ‚úÖ Ready")
        
        print(f"\nüìä Technical Indicators:")
        for name, settings in cls.INDICATORS.items():
            print(f"{name.upper()}: {settings}")

        errors = cls.validate_config()
        if errors:
            print(f"\n‚ùå Configuration Errors:")
            for error in errors:
                print(f"  - {error}")
        else:
            print(f"\n‚úÖ Configuration validated successfully!")

        print("=" * 60)

    # ================================================================
    # üîß Helper Functions (COMPLETE SET)
    # ================================================================

    @classmethod
    def get_timeframe_config(cls, timeframe: str) -> Dict:
        """Get configuration for specific timeframe"""
        return cls.RISK_MANAGEMENT.get(timeframe, cls.RISK_MANAGEMENT.get("4h", {}))

    @classmethod
    def get_indicator_settings(cls, indicator: str) -> Dict:
        """Get settings for specific indicator"""
        return cls.INDICATORS.get(indicator, {})

    @classmethod
    def get_notification_config(cls) -> Dict:
        """Get notification configuration"""
        return cls.NOTIFICATIONS

    @classmethod
    def get_binance_config(cls) -> Dict:
        """Get Binance API configuration for DataManager"""
        return {
            'base_url': cls.BINANCE_BASE_URL,
            'api_key': cls.BINANCE_API_KEY,
            'secret_key': cls.BINANCE_SECRET_KEY,
            'timeout': 30,
            'rate_limit': 1200
        }

    @classmethod
    def get_google_config(cls) -> Dict:
        """Get Google configuration for ConfigManager"""
        return {
            'sheets_id': cls.GOOGLE_SHEETS_ID,
            'credentials_path': cls.GOOGLE_APPLICATION_CREDENTIALS
        }

    @classmethod
    def get_line_config(cls) -> Dict:
        """Get LINE configuration for ConfigManager"""
        return {
            'access_token': cls.LINE_CHANNEL_ACCESS_TOKEN,
            'secret': cls.LINE_CHANNEL_SECRET,
            'user_id': cls.LINE_USER_ID
        }

    @classmethod
    def get_position_config(cls) -> Dict:
        """Get position management configuration for PositionManager"""
        return cls.POSITION_MANAGEMENT

    @classmethod
    def get_symbols_by_tier(cls, tier: int = 4) -> List[str]:
        """Get symbols by tier level"""
        if tier == 1:
            return cls.SYMBOL_CATEGORIES["tier1"]
        elif tier == 2:
            return cls.SYMBOL_CATEGORIES["tier2"]
        elif tier == 3:
            return cls.SYMBOL_CATEGORIES["tier3"]
        else:
            return cls.DEFAULT_SYMBOLS

    @classmethod
    def get_symbols_by_category(cls, category: str) -> List[str]:
        """Get symbols by category"""
        return cls.SYMBOL_CATEGORIES.get(category, [])

    @classmethod
    def is_priority_symbol(cls, symbol: str) -> bool:
        """Check if symbol is in priority list"""
        return symbol in cls.PRIORITY_SYMBOLS

    @classmethod
    def get_update_interval(cls, symbol: str) -> int:
        """Get update interval based on symbol priority (seconds)"""
        if symbol in cls.SYMBOL_CATEGORIES["tier1"]:
            return 30
        elif symbol in cls.SYMBOL_CATEGORIES["tier2"]:
            return 60
        elif symbol in cls.SYMBOL_CATEGORIES["tier3"]:
            return 300
        else:
            return 900

    @classmethod
    def validate_symbol(cls, symbol: str) -> bool:
        """Validate if symbol exists in config"""
        return symbol in cls.DEFAULT_SYMBOLS

    @classmethod
    def get_system_summary(cls) -> Dict:
        """Get configuration summary for debugging v2.0"""
        return {
            "version": cls.VERSION,
            "timeframes": cls.TIMEFRAMES,
            "symbols_count": len(cls.DEFAULT_SYMBOLS),
            "priority_symbols_count": len(cls.PRIORITY_SYMBOLS),
            "risk_management_timeframes": list(cls.RISK_MANAGEMENT.keys()),
            "indicators": list(cls.INDICATORS.keys()),
            "check_interval_seconds": cls.CHECK_INTERVAL,
            "signal_cooldown_minutes": cls.SIGNAL_COOLDOWN_MINUTES,
            "price_monitor_interval": cls.PRICE_MONITOR_INTERVAL,
            "rsi_thresholds": {
                "oversold": cls.INDICATORS["rsi"]["oversold"],
                "overbought": cls.INDICATORS["rsi"]["overbought"]
            },
            "macd_settings": cls.INDICATORS["macd"],
            "line_configured": bool(cls.LINE_CHANNEL_ACCESS_TOKEN),
            "sheets_configured": bool(cls.GOOGLE_SHEETS_ID),
            "binance_configured": bool(cls.BINANCE_BASE_URL),
            "refactored_services": True,
            "position_management": True,
            "symbol_tiers": {
                "tier1": len(cls.SYMBOL_CATEGORIES["tier1"]),
                "tier2": len(cls.SYMBOL_CATEGORIES["tier2"]),
                "tier3": len(cls.SYMBOL_CATEGORIES["tier3"]),
                "tier4": len(cls.SYMBOL_CATEGORIES["tier4"])
            }
        }

    # ================================================================
    # üìã Change Log
    # ================================================================

    @classmethod
    def get_change_log(cls) -> Dict:
        """Get change log for v2.0"""
        return {
            "version": "2.0-refactored",
            "changes": {
                "added": [
                    "50 verified Binance Futures symbols (from 30)",
                    "ATR indicator for volatility filtering",
                    "Liquidity & volume filters (min 24h volume, OI, spread)",
                    "Symbol tier system (tier1-4) for priority management",
                    "Symbol categories by sector (layer1, layer2, defi, gaming)",
                    "Performance optimization (batch processing, retry logic)",
                    "10 priority symbols (from 5)",
                    "Enhanced helper functions (get_update_interval, etc.)",
                    "ConfigManager for centralized configuration",
                    "DataManager replacing PriceFetcher + DataUpdater", 
                    "PositionManager for position logic",
                    "Background position monitoring",
                    "Comprehensive caching system"
                ],
                "modified": [
                    "Signal cooldown: 4 hours ‚Üí 30 minutes",
                    "Max signals per day: 10 ‚Üí 20",
                    "Signal strength threshold: 70% ‚Üí 75%",
                    "MATICUSDT ‚Üí POLUSDT (rebrand)",
                    "Enhanced data storage configuration",
                    "Improved risk management settings"
                ],
                "refactored": [
                    "main.py - new service architecture", 
                    "signal_detector.py - DataManager + PositionManager",
                    "price_monitor.py - coordinator role",
                    "scheduler.py - delegates to services",
                    "sheets_logger.py - uses ConfigManager",
                    "line_notifier.py - uses ConfigManager"
                ]
            }
        }

    # ================================================================
    # üîç Developer Information
    # ================================================================

    @classmethod
    def get_developer_info(cls) -> Dict:
        """Get developer and debugging information for v2.0"""
        return {
            "architecture": {
                "pattern": "Dependency Injection + Single Responsibility",
                "core_services": [
                    "ConfigManager (singleton)",
                    "DataManager (price data + caching)",
                    "PositionManager (position logic + tracking)"
                ],
                "coordinators": [
                    "SignalDetector (analysis + position creation)",
                    "PriceMonitor (monitoring coordination)",
                    "SignalScheduler (job scheduling)"
                ]
            },
            "validation_rules": {
                "required_env_vars": [
                    "BINANCE_BASE_URL",
                    "LINE_CHANNEL_ACCESS_TOKEN", 
                    "GOOGLE_SHEETS_ID"
                ],
                "optional_env_vars": [
                    "CHECK_INTERVAL",
                    "MIN_SIGNAL_STRENGTH",
                    "MAX_SIGNALS_PER_DAY",
                    "SIGNAL_COOLDOWN_MINUTES",
                    "PRICE_MONITOR_INTERVAL",
                    "MIN_24H_VOLUME",
                    "MIN_OPEN_INTEREST",
                    "ATR_PERIOD",
                    "DEBUG"
                ]
            },
            "performance_tips": [
                "Use tier system to prioritize symbol updates",
                "Enable batch processing for 50 symbols",
                "Set appropriate volume filters to reduce noise",
                "Monitor memory usage with max_memory_mb setting",
                "Use compression for candle data storage"
            ]
        }
========================================
FILE: app/main.py
========================================

import logging
import os
import time
from threading import Thread
from flask import Flask, jsonify, request

# New refactored services
from app.services.config_manager import ConfigManager
from app.services.data_manager import DataManager
from app.services.position_manager import PositionManager
from app.services.websocket_manager import WebSocketManager

# Legacy services (will be refactored)
from app.services.signal_detector import SignalDetector
from app.services.scheduler import SignalScheduler
from app.services.sheets_logger import SheetsLogger
from app.services.line_notifier import LineNotifier
from app.services.performance_analyzer import PerformanceAnalyzer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_version():
    """Read and auto-increment version on startup"""
    try:
        # Read current version
        if os.path.exists('version.txt'):
            with open('version.txt', 'r') as f:
                version = int(f.read().strip())
        else:
            version = 106
        
        # Increment version
        new_version = version + 1
        
        # Save new version
        with open('version.txt', 'w') as f:
            f.write(str(new_version))
        
        logger.info(f"üî¢ Version auto-incremented: 2.2.{version} ‚Üí 2.2.{new_version}")
        return f"2.2.{new_version}"
    except Exception as e:
        logger.error(f"Error reading version: {e}")
        return "2.2.0"

VERSION = get_version()

raw_port = os.environ.get("PORT", "8080")
if raw_port == "$PORT" or not raw_port.isdigit():
    port = 8080 
else:
    port = int(raw_port)

# Initialize Flask app
app = Flask(__name__)

# Global services - refactored architecture
services = {
    # Core refactored services
    "config_manager": None,
    "data_manager": None, 
    "position_manager": None,
    "websocket_manager": None,
    
    # Legacy services (to be updated)
    "signal_detector": None,
    "scheduler": None,
    "line_notifier": None,
    "sheets_logger": None,
    "performance_analyzer": None,
    
    "initialized": False,
}


def initialize_services_background():
    """Initialize all services with new refactored architecture"""
    try:
        logger.info(f"üöÄ Starting SIGNAL-ALERT {VERSION} service initialization...")
        
        # Step 1: Initialize ConfigManager (Singleton)
        services["config_manager"] = ConfigManager()
        logger.info("‚úÖ ConfigManager initialized")
        
        # Step 2: Initialize DataManager (replaces PriceFetcher + DataUpdater)
        services["data_manager"] = DataManager()
        logger.info("‚úÖ DataManager initialized (replaces PriceFetcher + DataUpdater)")
        
        # Step 3: Initialize PositionManager (replaces PositionTracker + PriceMonitor logic)
        services["position_manager"] = PositionManager(services["data_manager"])
        logger.info("‚úÖ PositionManager initialized (replaces PositionTracker + PriceMonitor logic)")
        
        # Step 3.5: Initialize WebSocketManager for real-time data (Top 5 coins)
        try:
            # Create callback with SignalDetector
            def kline_callback(kline_data):
                services["data_manager"].process_websocket_kline(
                    kline_data, 
                    signal_detector=services.get("signal_detector")
                )
            
            # Top 3 coins for Rebound strategy
            symbols = ["btcusdt", "ethusdt", "solusdt"]
            services["websocket_managers"] = []
            
            for symbol in symbols:
                ws = WebSocketManager(symbol=symbol, timeframe="15m")
                ws.set_kline_callback(kline_callback)
                ws.connect()
                services["websocket_managers"].append(ws)
                logger.info(f"‚úÖ WebSocket connected: {symbol}")
            
            logger.info(f"‚úÖ All {len(symbols)} WebSockets initialized")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è WebSocketManager failed to initialize: {e}")
            services["websocket_managers"] = []
        
        # Step 4: Initialize notification services with ConfigManager
        try:
            line_config = services["config_manager"].get_line_config()
            services["line_notifier"] = LineNotifier(line_config)
            logger.info("‚úÖ LineNotifier initialized with ConfigManager")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LineNotifier failed to initialize: {e}")
            services["line_notifier"] = None
            
        try:
            google_config = services["config_manager"].get_google_config()
            # üëá ‡πÉ‡∏™‡πà # ‡πÑ‡∏ß‡πâ‡∏´‡∏ô‡πâ‡∏≤ 2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
            # services["sheets_logger"] = SheetsLogger(google_config)
            # logger.info("‚úÖ SheetsLogger initialized with ConfigManager")
            services["sheets_logger"] = None # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è SheetsLogger failed to initialize: {e}")
            services["sheets_logger"] = None
        
        # Step 5: Initialize SignalDetector with new services
        try:
            signal_config = {
                "data_manager": services["data_manager"],
                "position_manager": services["position_manager"],
                "config_manager": services["config_manager"],
                "line_notifier": services["line_notifier"]
            }
            services["signal_detector"] = SignalDetector(signal_config)
            logger.info("‚úÖ SignalDetector initialized with refactored services")
            
            # Register 15m rebound callback
            def on_15m_rebound(kline_data):
                """Callback for 15m candle close - analyze rebound signals"""
                try:
                    result = services["signal_detector"].analyze_rebound(kline_data)
                    if result and result.get('recommendation'):
                        services["line_notifier"].send_signal_alert(result)
                except Exception as e:
                    logger.error(f"Error in 15m rebound callback: {e}")
            
            services["data_manager"].register_rebound_callback(on_15m_rebound)
            logger.info("‚úÖ Registered 15m rebound callback")
        except Exception as e:
            logger.error(f"‚ùå SignalDetector initialization failed: {e}")
            services["signal_detector"] = None
        
        # Step 6: Initialize Scheduler with new architecture
        try:
            scheduler_config = services["config_manager"].get_all()
            services["scheduler"] = SignalScheduler(scheduler_config)
            
            # Inject refactored services into scheduler
            services["scheduler"].set_services(
                signal_detector=services["signal_detector"],
                position_manager=services["position_manager"],
                line_notifier=services["line_notifier"],
                sheets_logger=services["sheets_logger"]
            )
            logger.info("‚úÖ SignalScheduler initialized with refactored services")
            
            # Auto-start scheduler
            services["scheduler"].start_scheduler()
            logger.info("‚úÖ Scheduler auto-started")
            
        except Exception as e:
            logger.error(f"‚ùå SignalScheduler initialization failed: {e}")
            services["scheduler"] = None
        
        # Step 7: Initialize PerformanceAnalyzer
        try:
            services["performance_analyzer"] = PerformanceAnalyzer(
                config={},
                sheets_logger=services["sheets_logger"]
            )
            logger.info("‚úÖ PerformanceAnalyzer initialized")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è PerformanceAnalyzer failed to initialize: {e}")
            services["performance_analyzer"] = None
        
        # Step 8: Start automatic position monitoring
        if services["position_manager"] and services["sheets_logger"]:
            try:
                # Start background position monitoring thread
                monitor_thread = Thread(
                    target=start_position_monitoring,
                    daemon=True
                )
                monitor_thread.start()
                logger.info("‚úÖ Background position monitoring started")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to start background monitoring: {e}")
        
        services["initialized"] = True
        logger.info(f"üéâ All services initialized successfully! SIGNAL-ALERT {VERSION} ready")
        
    except Exception as e:
        logger.error(f"üí• Service initialization failed: {e}")
        services["initialized"] = False


def start_position_monitoring():
    """Background thread for continuous position monitoring"""
    monitor_interval = 30  # 30 seconds
    
    while True:
        try:
            if services["initialized"] and services["position_manager"]:
                updates = services["position_manager"].update_positions()
                
                if updates:
                    logger.info(f"üìä Updated {len(updates)} positions")
                    
                    # Log to sheets if available
                    if services["sheets_logger"]:
                        try:
                            for position_id, update_info in updates.items():
                                if update_info.get('position_closed'):
                                    position = services["position_manager"].positions.get(position_id)
                                    if position:
                                        services["sheets_logger"].log_position_close(position)
                                
                                # ‚úÖ ‡πÅ‡∏Å‡πâ indent ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô for loop ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
                                for tp_level in ['TP1', 'TP2', 'TP3']:
                                    tp_key = f'{tp_level}_hit'
                                    if tp_key in update_info and update_info[tp_key].get('hit'):
                                        position = services["position_manager"].positions.get(position_id)
                                        if position:
                                            services["sheets_logger"].log_tp_hit(position, update_info[tp_key])
                                            logger.info(f"Logged {tp_level} hit for {position_id}")
                                        
                        except Exception as e:
                            logger.error(f"Error logging to sheets: {e}")
                            
            time.sleep(monitor_interval)
            
        except Exception as e:
            logger.error(f"Error in position monitoring thread: {e}")
            time.sleep(monitor_interval)


# Start background initialization
Thread(target=initialize_services_background, daemon=True).start()


@app.route("/")
def root():
    """Home endpoint - system information"""
    config = services["config_manager"]
    cache_stats = services["data_manager"].get_cache_stats() if services["data_manager"] else {}
    
    return jsonify({
        "system": "SIGNAL-ALERT",
        "version": VERSION,
        "status": "running",
        "services_ready": services["initialized"],
        "architecture": "refactored",
        "services": {
            "config_manager": services["config_manager"] is not None,
            "data_manager": services["data_manager"] is not None,
            "position_manager": services["position_manager"] is not None,
            "signal_detector": services["signal_detector"] is not None,
            "scheduler": services["scheduler"] is not None
        },
        "features": [
            "Centralized Data Management",
            "Unified Position Tracking", 
            "Single Source Price Fetching",
            "Automated TP/SL Detection",
            "Google Sheets Integration",
            "Configuration Management",
            "Comprehensive Error Handling"
        ],
        "metrics": {
            "cache_stats": cache_stats,
            "debug_mode": config.is_debug_mode() if config else False
        }
    })


@app.route("/health")
def health_check():
    """System health check"""
    health_data = {
        "status": "healthy" if services["initialized"] else "initializing",
        "timestamp": time.time(),
        "services_initialized": services["initialized"],
        "version": VERSION
    }
    
    status_code = 200 if services["initialized"] else 503
    return jsonify(health_data), status_code


@app.route('/api/test/line', methods=['POST', 'GET'])
def test_line_notification():
    """Test LINE notification"""
    try:
        if not services["line_notifier"]:
            return jsonify({
                "success": False,
                "error": "LineNotifier not initialized"
            }), 500
        
        success = services["line_notifier"].send_test_message()
        
        return jsonify({
            "success": success,
            "message": "Test message sent to LINE" if success else "Failed to send",
            "line_status": services["line_notifier"].get_status()
        })
        
    except Exception as e:
        logger.error(f"Test LINE error: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/api/line/webhook', methods=['POST'])
def line_webhook():
    """‡∏£‡∏±‡∏ö webhook ‡∏à‡∏≤‡∏Å LINE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π Group ID"""
    try:
        # ‡∏î‡∏∂‡∏á signature ‡πÅ‡∏•‡∏∞ body ‡∏à‡∏≤‡∏Å request
        signature = request.headers.get('X-Line-Signature')
        body = request.get_data(as_text=True)
        
        logger.info(f"üì• Received LINE webhook")
        
        # ‡πÅ‡∏õ‡∏•‡∏á JSON body ‡πÄ‡∏õ‡πá‡∏ô dict
        import json
        data = json.loads(body)
        
        # ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏î‡∏π events ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö
        for event in data.get('events', []):
            # ‡πÄ‡∏ä‡πá‡∏Å‡∏ß‡πà‡∏≤‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            source = event.get('source', {})
            
            if source.get('type') == 'group':
                # üéØ ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ Group ID ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£!
                group_id = source.get('groupId')
                
                # ‡πÅ‡∏™‡∏î‡∏á log
                logger.info(f"üéØ GROUP ID FOUND: {group_id}")
                logger.info(f"üìù Message Type: {event.get('type')}")
                logger.info(f"üí¨ Text: {event.get('message', {}).get('text', 'N/A')}")
                
        return jsonify({"status": "ok"}), 200
        
    except Exception as e:
        logger.error(f"‚ùå Webhook error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/receive-signal', methods=['POST'])
def receive_signal_from_outside():
    try:
        data = request.get_json()
        symbol = data.get('symbol', 'UNKNOWN')
        direction = data.get('direction', 'LONG').upper()
        price = data.get('current_price', 0)
        risk = data.get('risk_levels', {})
        
        # --- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì R:R ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö‡∏û‡∏µ‡πà ---
        entry = risk.get('entry_price', price)
        sl = risk.get('stop_loss', 0)
        tp1 = risk.get('take_profit_1', 0)
        
        rr_ratio = 0.0
        if entry and sl and tp1 and (entry != sl):
            risk_amt = abs(entry - sl)
            reward_amt = abs(tp1 - entry)
            rr_ratio = reward_amt / risk_amt if risk_amt > 0 else 0
        # -----------------------------------

        analysis = {
            "symbol": symbol,
            "timeframe": data.get('timeframe', '4H'),
            "current_price": price,
            "signals": {
                "buy": True if direction == "LONG" else False,
                "short": True if direction == "SHORT" else False
            },
            "risk_levels": {
                "entry_price": entry,
                "stop_loss": sl,
                "take_profit_1": tp1,
                "take_profit_2": risk.get('take_profit_2', 0),
                "take_profit_3": risk.get('take_profit_3', 0),
                "risk_reward_ratio": rr_ratio  # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏ó
            },
            "indicators": {
                "squeeze": {"squeeze_off": True, "momentum_direction": "UP" if direction == "LONG" else "DOWN"},
                "macd": {"cross_direction": "BULLISH" if direction == "LONG" else "BEARISH"},
                "rsi": {"value": 55 if direction == "LONG" else 45}
            },
            "signal_strength": data.get('signal_strength', 100)
        }
        
        if services["line_notifier"]:
            services["line_notifier"].send_signal_alert(analysis)
            
        return jsonify({"status": "success", "message": "Signal processed", "rr": rr_ratio}), 200
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        return jsonify({"status": "error", "message": str(e)}), 500
        
@app.route("/startup")
def startup_probe():
    """Startup probe - always return OK for Cloud Run"""
    return jsonify({
        "status": "ok",
        "timestamp": time.time()
    }), 200


@app.route("/keepalive")
def keepalive():
    """Keepalive endpoint for Cloud Run"""
    try:
        scheduler_status = "unknown"
        position_count = 0
        
        if services["initialized"] and services["scheduler"]:
            try:
                status_info = services["scheduler"].get_scheduler_status()
                scheduler_status = status_info.get("status", "unknown")
                
                # Auto-restart scheduler if stopped
                if scheduler_status == "stopped":
                    services["scheduler"].start_scheduler()
                    logger.info("üîÑ Auto-restarted scheduler from keepalive")
                    scheduler_status = "restarted"
            except Exception as e:
                logger.warning(f"Scheduler check failed in keepalive: {e}")
                scheduler_status = "error"
        
        if services["position_manager"]:
            try:
                summary = services["position_manager"].get_positions_summary()
                position_count = summary["active_positions"]
            except Exception as e:
                logger.warning(f"Position count check failed: {e}")
        
        return jsonify({
            "status": "alive",
            "timestamp": time.time(),
            "services_initialized": services["initialized"],
            "scheduler_status": scheduler_status,
            "active_positions": position_count,
            "uptime_check": "ok",
            "version": VERSION
        })
        
    except Exception as e:
        logger.error(f"Keepalive endpoint error: {e}")
        return jsonify({
            "status": "alive",
            "timestamp": time.time(),
            "error": str(e),
            "version": VERSION
        }), 200


def require_services(f):
    """Decorator to check if services are ready"""
    def wrapper(*args, **kwargs):
        if not services["initialized"]:
            return jsonify({
                "error": "Services are still initializing. Please wait...",
                "retry_after": 30,
                "version": VERSION
            }), 503
        return f(*args, **kwargs)
    wrapper.__name__ = f.__name__
    return wrapper


@app.route("/api/signals")
@require_services
def get_signals():
    """Scan for trading signals with new architecture"""
    symbols = request.args.get("symbols", "BTCUSDT,ETHUSDT")
    timeframes = request.args.get("timeframes", "4h")
    
    symbols_list = [s.strip() for s in symbols.split(",")]
    timeframes_list = [t.strip() for t in timeframes.split(",")]
    
    try:
        signals_found = []
        
        for symbol in symbols_list:
            for timeframe in timeframes_list:
                signal = services["signal_detector"].analyze_symbol(symbol, timeframe)
                if signal:
                    signals_found.append(signal)
        
        return jsonify({
            "status": "success",
            "signals": signals_found,
            "signals_found": len(signals_found),
            "timestamp": time.time(),
            "version": VERSION
        })
        
    except Exception as e:
        logger.error(f"Error in get_signals: {e}")
        return jsonify({"error": str(e), "version": VERSION}), 500


@app.route("/api/positions")
@require_services
def get_positions():
    """Get all positions"""
    try:
        active_positions = services["position_manager"].get_active_positions()
        summary = services["position_manager"].get_positions_summary()
        
        return jsonify({
            "status": "success",
            "active_positions": active_positions,
            "summary": summary,
            "total_positions": summary["total_positions"],
            "active_count": summary["active_positions"],
            "version": VERSION
        })
        
    except Exception as e:
        logger.error(f"Error in get_positions: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/positions/summary")
@require_services
def get_positions_summary():
    """Get positions summary"""
    try:
        summary = services["position_manager"].get_positions_summary()
        return jsonify({
            "status": "success",
            "summary": summary,
            "version": VERSION
        })
    except Exception as e:
        logger.error(f"Error in get_positions_summary: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/positions/status/<symbol>/<timeframe>")
@require_services  
def get_position_status(symbol, timeframe):
    """Get specific position status"""
    try:
        position = services["position_manager"].get_position_status(symbol.upper(), timeframe)
        
        return jsonify({
            "status": "success",
            "position_found": position is not None,
            "position": position,
            "version": VERSION
        })
        
    except Exception as e:
        logger.error(f"Error getting position status for {symbol} {timeframe}: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/positions/close", methods=["POST"])
@require_services
def close_position():
    """Manually close a position"""
    try:
        data = request.get_json()
        position_id = data.get("position_id")
        reason = data.get("reason", "MANUAL")
        
        if not position_id:
            return jsonify({"error": "position_id required"}), 400
        
        success = services["position_manager"].close_position(position_id, reason)
        
        if success:
            return jsonify({
                "status": "success",
                "message": f"Position {position_id} closed",
                "reason": reason,
                "version": VERSION
            })
        else:
            return jsonify({
                "error": "Position not found or already closed",
                "position_id": position_id
            }), 404
            
    except Exception as e:
        logger.error(f"Error closing position: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/positions/update", methods=["POST"])
@require_services
def update_positions():
    """Update all positions with current prices"""
    try:
        updates = services["position_manager"].update_positions()
        
        return jsonify({
            "status": "success",
            "positions_updated": len(updates),
            "updates": updates,
            "timestamp": time.time(),
            "version": VERSION
        })
        
    except Exception as e:
        logger.error(f"Error updating positions: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/monitor/status")
@require_services
def get_monitor_status():
    """Get monitoring status"""
    try:
        summary = services["position_manager"].get_positions_summary()
        cache_stats = services["data_manager"].get_cache_stats()
        
        return jsonify({
            "status": "success",
            "monitoring": True,
            "active_positions_count": summary["active_positions"],
            "total_positions": summary["total_positions"],
            "cache_stats": cache_stats,
            "version": VERSION
        })
        
    except Exception as e:
        logger.error(f"Error getting monitor status: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/monitor/force-check", methods=["POST"])
@require_services
def force_check_positions():
    """Force check all positions immediately"""
    try:
        updates = services["position_manager"].update_positions()
        
        return jsonify({
            "status": "success",
            "message": "Force check completed",
            "positions_checked": len(services["position_manager"].get_active_positions()),
            "updates": updates,
            "timestamp": time.time(),
            "version": VERSION
        })
        
    except Exception as e:
        logger.error(f"Error in force check: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/monitor/check/<symbol>")
@require_services
def get_symbol_price(symbol):
    """Get current price for specific symbol"""
    try:
        price = services["data_manager"].get_single_price(symbol.upper())
        
        if price is not None:
            return jsonify({
                "status": "success", 
                "symbol": symbol.upper(),
                "current_price": price,
                "timestamp": time.time(),
                "version": VERSION
            })
        else:
            return jsonify({
                "error": f"Failed to get price for {symbol}",
                "symbol": symbol.upper()
            }), 500
            
    except Exception as e:
        logger.error(f"Error getting price for {symbol}: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/scheduler/start", methods=["POST"])
@require_services
def start_scheduler():
    """Start automatic scheduler"""
    try:
        services["scheduler"].start_scheduler()
        return jsonify({
            "status": "success", 
            "message": "Scheduler started",
            "version": VERSION
        })
    except Exception as e:
        logger.error(f"Error starting scheduler: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/scheduler/stop", methods=["POST"])
@require_services
def stop_scheduler():
    """Stop automatic scheduler"""
    try:
        services["scheduler"].stop_scheduler()
        return jsonify({
            "status": "success",
            "message": "Scheduler stopped", 
            "version": VERSION
        })
    except Exception as e:
        logger.error(f"Error stopping scheduler: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/scheduler/status")
@require_services
def get_scheduler_status():
    """Get scheduler status"""
    try:
        status = services["scheduler"].get_scheduler_status()
        return jsonify({
            "status": "success",
            "scheduler": status,
            "version": VERSION
        })
    except Exception as e:
        logger.error(f"Error getting scheduler status: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/debug/services")
@require_services
def debug_services():
    """Debug endpoint for service status"""
    try:
        debug_info = {
            "version": VERSION,
            "initialized": services["initialized"],
            "services": {}
        }
        
        # Check each service
        for service_name, service in services.items():
            if service_name == "initialized":
                continue
                
            if service is None:
                debug_info["services"][service_name] = "not_available"
            elif service_name == "config_manager":
                debug_info["services"][service_name] = {
                    "available": True,
                    "debug_mode": service.is_debug_mode(),
                    "version": service.get("VERSION", "unknown")
                }
            elif service_name == "data_manager":
                debug_info["services"][service_name] = {
                    "available": True,
                    "cache_stats": service.get_cache_stats()
                }
            elif service_name == "position_manager":
                summary = service.get_positions_summary()
                debug_info["services"][service_name] = {
                    "available": True,
                    "active_positions": summary["active_positions"],
                    "total_positions": summary["total_positions"],
                    "win_rate": summary["win_rate_pct"]
                }
            elif service_name == "scheduler":
                try:
                    status = service.get_scheduler_status()
                    debug_info["services"][service_name] = {
                        "available": True,
                        "status": status.get("status", "unknown")
                    }
                except Exception as e:
                    debug_info["services"][service_name] = {"error": str(e)}
            else:
                debug_info["services"][service_name] = "available"
        
        return jsonify(debug_info)
        
    except Exception as e:
        logger.error(f"Error in debug services: {e}")
        return jsonify({"error": str(e)}), 500


@app.route("/api/debug/positions")
@require_services
def debug_positions():
    """Debug positions in detail"""
    try:
        active_positions = services["position_manager"].get_active_positions()
        summary = services["position_manager"].get_positions_summary()
        
        return jsonify({
            "version": VERSION,
            "total_positions": summary["total_positions"],
            "active_positions": summary["active_positions"],
            "closed_positions": summary["closed_positions"],
            "win_rate_pct": summary["win_rate_pct"],
            "total_pnl_pct": summary["total_pnl_pct"],
            "active_positions_detail": active_positions
        })
        
    except Exception as e:
        logger.error(f"Error in debug positions: {e}")
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    # ‡∏•‡∏ö‡∏û‡∏ß‡∏Å raw_port = ... ‡πÅ‡∏•‡∏∞ if raw_port == ... ‡∏ó‡∏¥‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏´‡∏°‡∏î
    
    logger.info(f"üöÄ Starting SIGNAL-ALERT {VERSION} on port {port}")
    
    try:
        app.run(host="0.0.0.0", port=port, debug=False)
    except Exception as e:
        logger.error(f"üí• Failed to start Flask application: {e}")
        raise

========================================
FILE: app/services/signal_detector.py
========================================

"""Signal detection logic combining all indicators - CONSERVATIVE MODE v2.0"""

import logging
from datetime import datetime
import time
from typing import Dict, List, Optional, Tuple

from .indicators import TechnicalIndicators
from ..utils.core_utils import ErrorHandler
from ..utils.data_types import DataConverter
from .signal_history_manager import SignalHistoryManager

logger = logging.getLogger(__name__)


class SignalDetector:
    """Detect trading signals using Squeeze + MACD Uncle Cholok + RSI strategy - CONSERVATIVE"""

    def __init__(self, config: Dict):
        """Initialize signal detector with refactored services"""
        # Extract refactored services
        self.data_manager = config["data_manager"]
        self.position_manager = config["position_manager"]
        self.config_manager = config["config_manager"]
        self.line_notifier = config.get("line_notifier")
        
        # Initialize utilities
        self.indicators = TechnicalIndicators()
        self.data_converter = DataConverter()
        self.active_positions = set()
        
        # Get configuration from ConfigManager
        self.risk_management = self._load_risk_config()
        self.indicator_settings = self._load_indicator_config()
        self.signal_history = SignalHistoryManager()
        
        logger.info("‚úÖ SignalDetector initialized - CONSERVATIVE MODE")
    
    def _load_risk_config(self) -> Dict:
        """Load risk management configuration from Config"""
        try:
            # ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å Config class
            from config.settings import Config
            return Config.RISK_MANAGEMENT
        except Exception as e:
            logger.warning(f"Error loading risk config, using defaults: {e}")
            return {
                "4h": {"tp_levels": [1.2, 1.5, 2.0], "sl_level": 1.5},
                "1d": {"tp_levels": [3.0, 5.0, 7.0], "sl_level": 3.0}
            }
    
    def _load_indicator_config(self) -> Dict:
        try:
            return {
                "squeeze": {"length": 20, "bb_mult": 2.0, "kc_mult": 1.5},
                "macd": {"fast": 8, "slow": 17, "signal": 9}, 
                "rsi": {
                    "period": 14, 
                    "oversold": 35,  # ‚úÖ ‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å 40 ‡πÄ‡∏õ‡πá‡∏ô 35
                    "overbought": 65 # ‚úÖ ‡πÅ‡∏Å‡πâ‡∏à‡∏≤‡∏Å 60 ‡πÄ‡∏õ‡πá‡∏ô 65
                }
            }
            
        except Exception as e:
            logger.warning(f"Error loading indicator config, using defaults: {e}")
            return {
                "squeeze": {"length": 20, "bb_mult": 2.0, "kc_mult": 1.5},
                "macd": {"fast": 8, "slow": 17, "signal": 9}, 
                "rsi": {"period": 14, "oversold": 40, "overbought": 60}
            }

    def analyze_realtime(self, kline_data: Dict) -> Optional[Dict]:
        """Analyze real-time kline data"""
        try:
            # Only analyze when candle closes
            if not kline_data.get('is_closed'):
                return None
            
            symbol = kline_data['symbol']
            timeframe = kline_data['timeframe']
            
            logger.info(f"üîç Real-time analysis: {symbol} {timeframe}")
            
            # Use existing analyze_symbol
            return self.analyze_symbol(symbol, timeframe)
            
        except Exception as e:
            logger.error(f"Error in real-time analysis: {e}")
            return None
    
    @ErrorHandler.service_error_handler("SignalDetector")
    def analyze_symbol(self, symbol: str, timeframe: str = "4h") -> Optional[Dict]:
        """Analyze symbol using refactored data flow"""
        try:
            logger.info(f"üîç Analyzing {symbol} on {timeframe} (CONSERVATIVE)")

            # Get data from DataManager
            df = self.data_manager.get_klines(symbol, timeframe, limit=100)

            df_1d = self.data_manager.get_klines(symbol, "1d", limit=100)
            trend_1d = self._detect_signals_improved_fixed(None, "1d", df_1d)

            if df is None:
                logger.warning(f"No data available for {symbol} {timeframe}")
                return {
                    "error": f"Failed to fetch data for {symbol}",
                    "symbol": symbol,
                    "timeframe": timeframe,
                    "timestamp": datetime.now().isoformat(),
                    "version": "2.0-conservative"
                }

            # Validate data quality
            if not self.data_converter.validate_dataframe(df):
                logger.warning(f"Invalid DataFrame for {symbol} {timeframe}")
                return None

            # Calculate all indicators
            analysis = self.indicators.analyze_all_indicators(df, self.indicator_settings)

            # Detect trading signals with CONSERVATIVE logic
            signals = self._detect_signals_improved_fixed(analysis, timeframe, df, trend_1d=trend_1d)

            # Calculate risk management levels  
            risk_levels = self._calculate_risk_levels(analysis["current_price"], timeframe, signals, symbol)

            # Handle position creation with duplicate prevention
            position_created = self._handle_signal_position_fixed(
                symbol, timeframe, signals, analysis["current_price"], risk_levels
            )

            # Create comprehensive result
            result = {
                "symbol": symbol,
                "timeframe": timeframe,
                "timestamp": datetime.now().isoformat(),
                "current_price": analysis["current_price"],
                "version": "2.0-conservative",
                
                # Indicator values
                "indicators": {
                    "squeeze": analysis["squeeze"],
                    "macd": analysis["macd"],
                    "rsi": analysis["rsi"],
                },
                
                # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° EMA ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 1D signals
                "ema12": signals.get("ema12", 0),
                "ema26": signals.get("ema26", 0),
                
                # Trading signals
                "signals": signals,
                
                # Risk management
                "risk_levels": risk_levels,
                
                # Overall assessment
                "signal_strength": self._calculate_signal_strength_improved(signals),
                "recommendation": self._get_recommendation_improved(signals),
                
                # Position info
                "position_created": position_created,
                "has_active_position": self._has_active_position_strict(symbol, timeframe),
            }

            # Convert NumPy types for JSON serialization
            result = self.data_converter.sanitize_signal_data(result)

            # Log significant results
            if result.get('recommendation'):
                logger.info(f"Analysis complete for {symbol}: {result['recommendation']}")
                if position_created:
                    logger.info(f"üÜï Created position for {symbol} {timeframe}")
            
            return result

        except Exception as e:
            logger.error(f"Analysis error for {symbol}: {str(e)}")
            return {
                "error": f"Analysis error for {symbol}: {str(e)}",
                "symbol": symbol,
                "timeframe": timeframe,
                "timestamp": datetime.now().isoformat(),
                "version": "2.0-conservative"
            }

    def _has_active_position_strict(self, symbol: str, timeframe: str) -> bool:
        """‚úÖ STRICT check if position exists - prevent duplicates"""
        try:
            # Check 1: Via PositionManager
            position = self.position_manager.get_position_status(symbol, timeframe)
            if position is not None:
                logger.debug(f"Found active position via PositionManager: {symbol} {timeframe}")
                return True
            
            # Check 2: Check all direction combinations
            for direction in ["LONG", "SHORT"]:
                position_id = f"{symbol}_{timeframe}_{direction}"
                if position_id in self.position_manager.positions:
                    pos_data = self.position_manager.positions[position_id]
                    if pos_data.get('status') == 'ACTIVE':
                        logger.debug(f"Found active position by ID: {position_id}")
                        return True
            
            # Check 3: In active_positions set
            if symbol in self.active_positions:
                logger.debug(f"Found in active_positions set: {symbol}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking active position: {e}")
            return True  # Return True on error to prevent duplicate

    @ErrorHandler.service_error_handler("SignalDetector") 
    def _handle_signal_position_fixed(
        self, symbol: str, timeframe: str, signals: Dict, current_price: float, risk_levels: Dict
    ) -> bool:
        """Handle position creation - only blocks if active position exists"""
        try:
            # Check if we have a valid signal
            has_signal = signals.get("buy") or signals.get("short")
        
            if not has_signal:
                return False
        
            # Check if active position exists (no cooldown check)
            if symbol in self.active_positions:
                logger.warning(f"‚ö†Ô∏è {symbol} already in active positions set")
                return False

            # Check via PositionManager strictly
            existing_position = self.position_manager.get_position_status(symbol, timeframe)
            if existing_position:
                logger.warning(f"‚ö†Ô∏è Position already exists: {symbol} {timeframe}")
                return False
        
            # Check all possible position IDs
            direction = "LONG" if signals.get("buy") else "SHORT"
            for dir_check in ["LONG", "SHORT"]:
                check_id = f"{symbol}_{timeframe}_{dir_check}"
                if check_id in self.position_manager.positions:
                    if self.position_manager.positions[check_id].get('status') == 'ACTIVE':
                        logger.warning(f"‚ö†Ô∏è Found active {dir_check} position: {symbol} {timeframe}")
                        return False
        
            # Create signal data for position creation
            signal_data = {
                "symbol": symbol,
                "timeframe": timeframe,
                "direction": direction,
                "current_price": current_price,
                "signal_strength": self._calculate_signal_strength_improved(signals)
            }
        
            # Create position using PositionManager
            position_id = self.position_manager.create_position(signal_data)
        
            if position_id:
                logger.info(f"‚úÖ Created {direction} position: {symbol} {timeframe} @ {current_price}")
            
                # Update tracking (no cooldown timestamp)
                self.active_positions.add(symbol)
            
                return True
            else:
                logger.warning(f"Failed to create position for {symbol} {timeframe}")
                return False
            
        except Exception as e:
            logger.error(f"Error handling signal position: {e}")
            return False

    def _detect_signals_improved_fixed(self, analysis: Dict, timeframe: str = "1d", df=None, trend_1d=None) -> Dict[str, bool]:
        """
        1D: CDC ActionZone (EMA 12/26 Crossover)
        4H: RSI + MACD + Enhanced filters + STRONG MOMENTUM MODE + PULLBACK MODE
        """
        try:
            import pandas as pd

            # Validate dataframe
            if df is None or 'close' not in df.columns:
                logger.warning("Invalid dataframe")
                return {"buy": False, "short": False, "sell": False, "cover": False}
            
            # ========================================
            # 1D: CDC ACTIONZONE (EMA CROSSOVER)
            # ========================================
            if timeframe == "1d":
                if len(df) < 30:
                    logger.warning(f"Insufficient data: {len(df)} candles")
                    return {"buy": False, "short": False, "sell": False, "cover": False, "ema12": 0, "ema26": 0}
                
                # Calculate EMA 12 and 26
                df['ema12'] = df['close'].ewm(span=12, adjust=False).mean()
                df['ema26'] = df['close'].ewm(span=26, adjust=False).mean()
                
                # Current values
                ema12_curr = df['ema12'].iloc[-1]
                ema26_curr = df['ema26'].iloc[-1]
                ema12_prev = df['ema12'].iloc[-2]
                ema26_prev = df['ema26'].iloc[-2]
                price_curr = df['close'].iloc[-1]
                
                # CDC ActionZone conditions
                buy_signal = (ema12_curr > ema26_curr) and (price_curr > ema12_curr)
                short_signal = (ema12_curr < ema26_curr) and (price_curr < ema12_curr)
                
                # Log
                if buy_signal:
                    logger.info(
                        f"üü¢ 1D CDC BUY | "
                        f"EMA12: {ema12_curr:.2f} > EMA26: {ema26_curr:.2f} | "
                        f"Price: {price_curr:.2f} > EMA12"
                    )
                elif short_signal:
                    logger.info(
                        f"üî¥ 1D CDC SELL | "
                        f"EMA12: {ema12_curr:.2f} < EMA26: {ema26_curr:.2f} | "
                        f"Price: {price_curr:.2f} < EMA12"
                    )
                
                # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° EMA values ‡πÉ‡∏ô return
                return {
                    "buy": buy_signal,
                    "short": short_signal,
                    "sell": False,
                    "cover": False,
                    "ema12": float(ema12_curr),
                    "ema26": float(ema26_curr)
                }
            
            # ========================================
            # 4H: IMPROVED SIGNALS
            # ========================================
            else:
                if len(df) < 30:
                    logger.warning(f"Insufficient data: {len(df)} candles")
                    return {"buy": False, "short": False, "sell": False, "cover": False}
                
                # Calculate RSI
                from ta.momentum import RSIIndicator
                rsi_indicator = RSIIndicator(df['close'], window=14)
                df['rsi'] = rsi_indicator.rsi()
                df['rsi_ma'] = df['rsi'].rolling(window=14).mean()
                
                rsi_current = df['rsi'].iloc[-1]
                rsi_ma_current = df['rsi_ma'].iloc[-1]
                rsi_prev = df['rsi'].iloc[-2]
                rsi_ma_prev = df['rsi_ma'].iloc[-2]
                
                # MACD values
                macd_data = analysis.get("macd", {})
                macd_cross = macd_data.get("cross_direction", "NONE")
                macd_line = macd_data.get("macd_line", 0)
                
                # Calculate MACD previous value
                from ta.trend import MACD
                macd_indicator = MACD(df['close'], window_slow=17, window_fast=8, window_sign=9)
                df['macd'] = macd_indicator.macd()
                macd_prev = df['macd'].iloc[-2] if len(df) > 1 else macd_line
                
                # Squeeze
                squeeze_data = analysis.get("squeeze", {})
                squeeze_off = squeeze_data.get("squeeze_off", False)
                
                # Check NaN
                if any(pd.isna([rsi_current, rsi_ma_current, rsi_prev, rsi_ma_prev])):
                    logger.warning("NaN values in RSI")
                    return {"buy": False, "short": False, "sell": False, "cover": False}
                
                # RSI Crossovers
                rsi_cross_up = (rsi_prev <= rsi_ma_prev) and (rsi_current > rsi_ma_current)
                rsi_cross_down = (rsi_prev >= rsi_ma_prev) and (rsi_current < rsi_ma_current)
                
                # ========================================
                # üî• ORIGINAL SIGNALS (Crossover Based)
                # ========================================
                original_buy = (
                    rsi_cross_up and 
                    macd_cross == "UP" and 
                    macd_line > -20 and        # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß: -20 ‡πÅ‡∏ó‡∏ô 0
                    squeeze_off
                )
                
                original_short = (
                    rsi_cross_down and 
                    macd_cross == "DOWN" and 
                    macd_line < 20 and         # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß: 20 ‡πÅ‡∏ó‡∏ô 0
                    squeeze_off
                )
                
                # ========================================
                # üî• STRONG MOMENTUM MODE
                # ========================================
                strong_momentum_buy = (
                    rsi_current > 65 and           # ‚úÖ ‡∏•‡∏î‡∏à‡∏≤‡∏Å 70
                    rsi_current > rsi_prev and
                    macd_line > 80 and             # ‚úÖ ‡∏•‡∏î‡∏à‡∏≤‡∏Å 100
                    macd_line > macd_prev and
                    squeeze_off
                )
                
                strong_momentum_short = (
                    rsi_current < 35 and           # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 30
                    rsi_current < rsi_prev and
                    macd_line < -80 and            # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å -100
                    macd_line < macd_prev and
                    squeeze_off
                )
                
                # ========================================
                # üÜï PULLBACK MODE
                # ========================================
                pullback_buy = (
                    macd_line > 50 and
                    rsi_current > 45 and rsi_current < 55 and
                    rsi_current > rsi_prev and
                    squeeze_off
                )
                
                pullback_short = (
                    macd_line < -50 and
                    rsi_current > 45 and rsi_current < 55 and
                    rsi_current < rsi_prev and
                    squeeze_off
                )
                
                # ========================================
                # ‡∏£‡∏ß‡∏°‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏ó‡∏±‡πâ‡∏á 3 ‡πÇ‡∏´‡∏°‡∏î ‚úÖ
                # ========================================
                buy_signal = original_buy or strong_momentum_buy or pullback_buy
                short_signal = original_short or strong_momentum_short or pullback_short
                
                # ========================================
                # Multi-Timeframe Filter
                # ========================================
                if trend_1d:
                    raw_buy = buy_signal
                    raw_short = short_signal
                    
                    buy_signal = buy_signal and trend_1d.get("buy", False)
                    short_signal = short_signal and trend_1d.get("short", False)
                    
                    # Log ‡∏ñ‡πâ‡∏≤‡πÇ‡∏î‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å
                    if raw_buy and not buy_signal:
                        logger.info(f"üö´ LONG Blocked by 1D Trend (CDC is RED)")
                    if raw_short and not short_signal:
                        logger.info(f"üö´ SHORT Blocked by 1D Trend (CDC is GREEN)")
                
                # ========================================
                # üìä LOGGING
                # ========================================
                if original_buy:
                    logger.info(
                        f"üü¢ 4H LONG (Crossover) | "
                        f"RSI: {rsi_prev:.2f}‚Üí{rsi_current:.2f} | "
                        f"MACD: {macd_cross} ({macd_line:.6f}) | "
                        f"Squeeze: OFF"
                    )
                elif strong_momentum_buy:
                    logger.info(
                        f"üî• 4H LONG (Strong Momentum) | "
                        f"RSI: {rsi_current:.2f} (rising, >65) | "
                        f"MACD: {macd_line:.6f} (rising, >80) | "
                        f"Squeeze: OFF"
                    )
                elif pullback_buy:
                    logger.info(
                        f"üìà 4H LONG (Pullback) | "
                        f"RSI: {rsi_current:.2f} (mid, rising) | "
                        f"MACD: {macd_line:.6f} (>50) | "
                        f"Squeeze: OFF"
                    )
                elif original_short:
                    logger.info(
                        f"üî¥ 4H SHORT (Crossover) | "
                        f"RSI: {rsi_prev:.2f}‚Üí{rsi_current:.2f} | "
                        f"MACD: {macd_cross} ({macd_line:.6f}) | "
                        f"Squeeze: OFF"
                    )
                elif strong_momentum_short:
                    logger.info(
                        f"üî• 4H SHORT (Strong Momentum) | "
                        f"RSI: {rsi_current:.2f} (falling, <35) | "
                        f"MACD: {macd_line:.6f} (falling, <-80) | "
                        f"Squeeze: OFF"
                    )
                elif pullback_short:
                    logger.info(
                        f"üìâ 4H SHORT (Pullback) | "
                        f"RSI: {rsi_current:.2f} (mid, falling) | "
                        f"MACD: {macd_line:.6f} (<-50) | "
                        f"Squeeze: OFF"
                    )
                else:
                    logger.debug(
                        f"4H No signal | RSI: {rsi_current:.2f}, "
                        f"MACD: {macd_cross} ({macd_line:.6f}), Squeeze: {squeeze_off}"
                    )
                
                return {
                    "buy": buy_signal,
                    "short": short_signal,
                    "sell": False,
                    "cover": False
                }
        
        except Exception as e:
            logger.error(f"Error detecting signals: {e}", exc_info=True)
            return {"buy": False, "short": False, "sell": False, "cover": False}

    def _detect_rebound_signals_15m(self, df, current_price: float) -> Dict[str, bool]:
        """
        15m Rebound Strategy: RSI + Bollinger Bands Combo
        LONG: RSI < 35 AND Price <= Lower BB
        SHORT: RSI > 65 AND Price >= Upper BB
        """
        try:
            import pandas as pd
            from ta.momentum import RSIIndicator
            from ta.volatility import BollingerBands
            
            if df is None or len(df) < 30:
                logger.warning("Insufficient data for rebound analysis")
                return {"buy": False, "short": False, "sell": False, "cover": False}
            
            # Calculate RSI
            rsi_indicator = RSIIndicator(df['close'], window=14)
            df['rsi'] = rsi_indicator.rsi()
            rsi_current = df['rsi'].iloc[-1]
            
            # Calculate Bollinger Bands
            bb_indicator = BollingerBands(df['close'], window=20, window_dev=2.0)
            df['bb_upper'] = bb_indicator.bollinger_hband()
            df['bb_lower'] = bb_indicator.bollinger_lband()
            df['bb_middle'] = bb_indicator.bollinger_mavg()
            
            bb_upper = df['bb_upper'].iloc[-1]
            bb_lower = df['bb_lower'].iloc[-1]
            bb_middle = df['bb_middle'].iloc[-1]
            
            # Check for NaN
            if pd.isna(rsi_current) or pd.isna(bb_lower) or pd.isna(bb_upper):
                logger.warning("NaN values in rebound indicators")
                return {"buy": False, "short": False, "sell": False, "cover": False}
            
            # LONG Signal: RSI oversold + Price at lower BB
            buy_signal = (
                rsi_current < 35 and 
                current_price <= bb_lower and
                current_price > bb_lower * 0.998  # ‡πÑ‡∏°‡πà‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            )
            
            # SHORT Signal: RSI overbought + Price at upper BB
            short_signal = (
                rsi_current > 65 and 
                current_price >= bb_upper and
                current_price < bb_upper * 1.002  # ‡πÑ‡∏°‡πà‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            )
            
            # Logging
            if buy_signal:
                logger.info(
                    f"üü° 15m REBOUND LONG | "
                    f"RSI: {rsi_current:.1f} (<35) | "
                    f"Price: {current_price:.2f} <= BB_Lower: {bb_lower:.2f}"
                )
            elif short_signal:
                logger.info(
                    f"üü° 15m REBOUND SHORT | "
                    f"RSI: {rsi_current:.1f} (>65) | "
                    f"Price: {current_price:.2f} >= BB_Upper: {bb_upper:.2f}"
                )
            else:
                logger.debug(
                    f"15m No rebound | RSI: {rsi_current:.1f}, "
                    f"Price: {current_price:.2f}, BB: [{bb_lower:.2f}, {bb_upper:.2f}]"
                )
            
            return {
                "buy": buy_signal,
                "short": short_signal,
                "sell": False,
                "cover": False,
                "rsi": float(rsi_current),
                "bb_upper": float(bb_upper),
                "bb_lower": float(bb_lower),
                "bb_middle": float(bb_middle)
            }
            
        except Exception as e:
            logger.error(f"Error in rebound signal detection: {e}", exc_info=True)
            return {"buy": False, "short": False, "sell": False, "cover": False}

    def analyze_rebound(self, kline_data: Dict) -> Optional[Dict]:
        """
        Analyze 15m rebound signals from WebSocket data
        Entry: Use real-time price from WebSocket stream
        """
        try:
            if not kline_data.get('is_closed'):
                return None
            
            symbol = kline_data['symbol']
            timeframe = kline_data['timeframe']
            
            if timeframe != '15m':
                logger.warning(f"analyze_rebound called with wrong timeframe: {timeframe}")
                return None
            
            # Use real-time price from WebSocket
            current_price = float(kline_data.get('close', 0))
            
            logger.info(f"üîç 15m Rebound analysis: {symbol} @ {current_price}")
            
            # Get historical data for indicators (100 candles)
            df = self.data_manager.get_klines(symbol, timeframe, limit=100)
            
            if df is None or not self.data_converter.validate_dataframe(df):
                logger.warning(f"Invalid data for {symbol} {timeframe}")
                return None
            
            # Detect rebound signals
            signals = self._detect_rebound_signals_15m(df, current_price)
            
            if not (signals.get("buy") or signals.get("short")):
                return None
            
            # Calculate risk levels
            risk_levels = self._calculate_risk_levels(current_price, timeframe, signals, symbol)
            
            # Handle position creation
            position_created = self._handle_signal_position_fixed(
                symbol, timeframe, signals, current_price, risk_levels
            )
            
            # Create result
            result = {
                "symbol": symbol,
                "timeframe": timeframe,
                "timestamp": datetime.now().isoformat(),
                "current_price": current_price,
                "version": "2.2-rebound",
                
                "indicators": {
                    "rsi": {
                        "value": signals.get("rsi", 50),
                        "status": "Oversold" if signals.get("rsi", 50) < 35 else "Overbought" if signals.get("rsi", 50) > 65 else "Neutral"
                    },
                    "bb": {
                        "upper": signals.get("bb_upper", 0),
                        "lower": signals.get("bb_lower", 0),
                        "middle": signals.get("bb_middle", 0)
                    }
                },
                
                "signals": signals,
                "risk_levels": risk_levels,
                "signal_strength": 100,  # Rebound signals are binary
                "recommendation": "LONG" if signals.get("buy") else "SHORT",
                "position_created": position_created,
                "has_active_position": self._has_active_position_strict(symbol, timeframe),
            }
            
            result = self.data_converter.sanitize_signal_data(result)
            
            if result.get('recommendation'):
                logger.info(f"‚úÖ 15m Rebound signal: {symbol} {result['recommendation']}")
                if position_created:
                    logger.info(f"üÜï Created 15m rebound position: {symbol}")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in rebound analysis: {e}", exc_info=True)
            return None

    def _check_market_trend_enhanced(self, df) -> str:
        """Conservative trend detection using MA20 and MA50"""
        try:
            close = df['close']
            
            # Calculate MAs
            ma_20 = close.rolling(20).mean()
            ma_50 = close.rolling(50).mean() if len(close) >= 50 else None
            
            current_price = close.iloc[-1]
            ma_20_current = ma_20.iloc[-1]
            
            # Case 1: Have MA50 - strict check
            if ma_50 is not None:
                ma_50_current = ma_50.iloc[-1]
                
                # Uptrend: Price > MA20 AND MA20 > MA50
                if current_price > ma_20_current and ma_20_current > ma_50_current:
                    return "UP"
                
                # Downtrend: Price < MA20 AND MA20 < MA50
                elif current_price < ma_20_current and ma_20_current < ma_50_current:
                    return "DOWN"
                
                else:
                    return "NEUTRAL"
            
            # Case 2: No MA50 - use only MA20
            else:
                if current_price > ma_20_current:
                    return "UP"
                elif current_price < ma_20_current:
                    return "DOWN"
                else:
                    return "NEUTRAL"
                    
        except Exception as e:
            logger.error(f"Error checking market trend: {e}")
            return "NEUTRAL"

    def _get_recommendation_improved(self, signals: Dict[str, bool]) -> str:
        """Generate recommendation based on signals"""
        if signals.get("buy"):
            return "LONG"
        elif signals.get("short"):
            return "SHORT"
        else:
            return ""

    def _calculate_signal_strength_improved(self, signals: Dict[str, bool]) -> int:
        """Calculate signal strength (0-100)"""
        if signals.get("buy") or signals.get("short"):
            return 100  # Signals that pass conservative conditions = 100%
        else:
            return 0

    def _calculate_risk_levels(self, current_price: float, timeframe: str, signals: Dict, symbol: str) -> Dict:
        """Calculate Stop Loss and Take Profit levels"""
        try:
            risk_config = self.risk_management.get(
                timeframe, self.risk_management.get("4h", {})
            )

            tp_percentages = risk_config.get("tp_levels", [3.0, 5.0, 7.0])
            
            # --- üÜï ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡∏°‡πà: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì SL ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏¥‡πà‡∏á (Volatility) ---
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 10 ‡πÅ‡∏ó‡πà‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏°‡∏≤‡∏î‡∏π‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏´‡∏ß‡∏µ‡πà‡∏¢‡∏á (‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å signals ‡∏´‡∏£‡∏∑‡∏≠ symbol ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á)
            symbol = symbol
            df_recent = self.data_manager.get_klines(symbol, timeframe, limit=10)
            
            if df_recent is not None and not df_recent.empty:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏ß‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏õ‡πá‡∏ô % (High-Low)
                volatility = ((df_recent['high'] - df_recent['low']) / df_recent['close']).mean() * 100
                # ‡πÉ‡∏ä‡πâ 1.5 ‡πÄ‡∏ó‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏ß‡∏µ‡πà‡∏¢‡∏á ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 2% ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5%
                sl_percentage = min(max(volatility * 1.5, 2.0), 5.0)
                logger.info(f"üõ°Ô∏è Dynamic SL set at {sl_percentage:.2f}% (Volatility: {volatility:.2f}%)")
            else:
                # ‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Config ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°
                sl_percentage = risk_config.get("sl_level", 3.0)
            # -----------------------------------------------------------

            risk_levels = {"timeframe": timeframe, "entry_price": current_price}

            # Determine signal direction
            is_long_signal = signals.get("buy", False)
            is_short_signal = signals.get("short", False)

            # Calculate levels based on signal direction
            if is_long_signal:
                risk_levels.update({
                    "direction": "LONG",
                    "stop_loss": current_price * (1 - sl_percentage / 100),
                    "take_profit_1": current_price * (1 + tp_percentages[0] / 100),
                    "take_profit_2": current_price * (1 + tp_percentages[1] / 100),
                    "take_profit_3": current_price * (1 + tp_percentages[2] / 100),
                    "risk_reward_ratio": tp_percentages[0] / sl_percentage,
                })

            elif is_short_signal:
                risk_levels.update({
                    "direction": "SHORT",
                    "stop_loss": current_price * (1 + sl_percentage / 100),
                    "take_profit_1": current_price * (1 - tp_percentages[0] / 100),
                    "take_profit_2": current_price * (1 - tp_percentages[1] / 100),
                    "take_profit_3": current_price * (1 - tp_percentages[2] / 100),
                    "risk_reward_ratio": tp_percentages[0] / sl_percentage,
                })

            return risk_levels

        except Exception as e:
            logger.error(f"Error calculating risk levels: {e}")
            return {"error": "Failed to calculate risk levels"}

    def scan_multiple_symbols(self, symbols: List[str], timeframes: List[str] = None) -> List[Dict]:
        """Scan multiple symbols for signals across different timeframes"""
        if timeframes is None:
            timeframes = ["4h", "1d"]

        results = []

        for symbol in symbols:
            for timeframe in timeframes:
                logger.info(f"üîç Scanning {symbol} on {timeframe}")
                result = self.analyze_symbol(symbol, timeframe)
                
                # ========================================
                # üÜï Check 1D signal history before adding
                # ========================================
                if result and timeframe == "1d":
                    signals = result.get("signals", {})
                    current_price = result.get("current_price", 0)
                    
                    if signals.get("buy"):
                        signal_type = "LONG"
                    elif signals.get("short"):
                        signal_type = "SHORT"
                    else:
                        signal_type = None
                    
                    # Check if should notify
                    if signal_type:
                        should_notify = self.signal_history.should_notify(
                            symbol, timeframe, signal_type, current_price
                        )
                        
                        if should_notify:
                            # Record signal
                            self.signal_history.record_signal(
                                symbol, timeframe, signal_type, current_price
                            )
                            # Clear opposite signal
                            self.signal_history.clear_opposite_signal(
                                symbol, timeframe, signal_type
                            )
                            # Add to results
                            results.append(result)
                            logger.info(f"‚úÖ NEW 1D signal: {symbol} {signal_type}")
                        else:
                            logger.debug(f"‚è≠Ô∏è SKIP 1D signal: {symbol} {signal_type} (already notified)")
                    else:
                        # No signal, still add to results for tracking
                        results.append(result)
                else:
                    # 4H or other timeframes - add normally
                    if result:
                        results.append(result)
                
                time.sleep(0.2)

        return results
        
    def get_active_signals(self, symbols: List[str], timeframes: List[str] = None) -> List[Dict]:
        """Get only signals with active recommendations"""
        if timeframes is None:
            timeframes = ["4h", "1d"]

        all_results = self.scan_multiple_symbols(symbols, timeframes)

        # Filter only results with actual recommendations
        active_signals = []
        for result in all_results:
            if "signals" in result and result.get("recommendation"):
                signals = result["signals"]
                if signals.get("buy") or signals.get("short"):
                    active_signals.append(result)

        logger.info(f"Found {len(active_signals)} active signals out of {len(all_results)} scans")
        return active_signals

    def scan_all_symbols(self, symbols: List[str] = None, timeframes: List[str] = None) -> List[Dict]:
        """Scan all symbols and return all results"""
        if symbols is None:
            symbols = ["BTCUSDT", "ETHUSDT", "BNBUSDT"]
        if timeframes is None:
            timeframes = ["4h", "1d"]
            
        return self.scan_multiple_symbols(symbols, timeframes)

    def validate_signal_quality(self, analysis: Dict) -> Dict:
        """Validate signal quality and reliability"""
        try:
            quality_score = 0
            quality_factors = []

            indicators = analysis.get("indicators", {})
            signals = analysis.get("signals", {})

            # Check squeeze momentum quality
            squeeze = indicators.get("squeeze", {})
            if squeeze.get("squeeze_off"):
                quality_score += 30
                quality_factors.append("Squeeze breakout confirmed")

                # Check momentum strength
                details = squeeze.get("details", {})
                momentum_value = abs(details.get("momentum_value", 0))
                if momentum_value > 0.001:
                    quality_score += 10
                    quality_factors.append("Strong momentum")

            # Check MACD quality
            macd = indicators.get("macd", {})
            if macd.get("cross_direction") != "NONE":
                quality_score += 25
                quality_factors.append("MACD cross confirmed")

                # Check if MACD is above/below zero line
                macd_details = macd.get("details", {})
                if macd_details.get("macd_above_zero") and signals.get("buy"):
                    quality_score += 10
                    quality_factors.append("MACD above zero line")
                elif not macd_details.get("macd_above_zero") and signals.get("short"):
                    quality_score += 10
                    quality_factors.append("MACD below zero line")

            # Check RSI quality
            rsi = indicators.get("rsi", {})
            rsi_value = rsi.get("value", 50)
            
            if rsi_value < 40 or rsi_value > 60:
                quality_score += 20
                level = "oversold" if rsi_value < 40 else "overbought"
                quality_factors.append(f"RSI {level} level")

                # Check RSI trend alignment
                rsi_details = rsi.get("details", {})
                rsi_trend = rsi_details.get("rsi_trend", "NEUTRAL")
                if (rsi_value < 40 and rsi_trend == "RISING") or (rsi_value > 60 and rsi_trend == "FALLING"):
                    quality_score += 5
                    quality_factors.append("RSI trend alignment")

            # Signal grade bonus
            if signals.get("buy") or signals.get("short"):
                quality_score += 15
                quality_factors.append("Strong signal grade")

            # Risk-reward assessment
            risk_levels = analysis.get("risk_levels", {})
            risk_reward = risk_levels.get("risk_reward_ratio", 0)
            if risk_reward >= 1.0:
                quality_score += 5
                quality_factors.append("Favorable risk-reward ratio")

            # Cap quality score at 100
            quality_score = min(quality_score, 100)

            return {
                "quality_score": quality_score,
                "quality_factors": quality_factors,
                "risk_reward_ratio": risk_reward,
                "signal_reliability": (
                    "HIGH" if quality_score >= 80
                    else "MEDIUM" if quality_score >= 60 
                    else "LOW"
                ),
            }

        except Exception as e:
            logger.error(f"Error validating signal quality: {e}")
            return {
                "quality_score": 0,
                "quality_factors": [],
                "risk_reward_ratio": 0,
                "signal_reliability": "UNKNOWN",
            }

    # Position Management Integration Methods
    def get_position_summary(self) -> Dict:
        """Get positions summary from PositionManager"""
        try:
            return self.position_manager.get_positions_summary()
        except Exception as e:
            logger.error(f"Error getting position summary: {e}")
            return {"error": str(e)}
    
    def get_position_status(self, symbol: str, timeframe: str) -> Dict:
        """Get specific position status from PositionManager"""
        try:
            position = self.position_manager.get_position_status(symbol, timeframe)
            return {
                "position_found": position is not None,
                "position": position,
                "symbol": symbol,
                "timeframe": timeframe
            }
        except Exception as e:
            logger.error(f"Error getting position status: {e}")
            return {"error": str(e), "position_found": False}
    
    def force_close_position(self, symbol: str, timeframe: str, reason: str = "MANUAL") -> Dict:
        """Force close a position via PositionManager"""
        try:
            # Create position_id in the format expected by PositionManager
            position_id = f"{symbol}_{timeframe}_LONG"  # Try LONG first
            success = self.position_manager.close_position(position_id, reason)
            
            if not success:
                # Try SHORT if LONG doesn't exist
                position_id = f"{symbol}_{timeframe}_SHORT"
                success = self.position_manager.close_position(position_id, reason)
            
            if success:
                # Remove from tracking
                if symbol in self.active_positions:
                    self.active_positions.remove(symbol)
                
                return {
                    "success": True, 
                    "message": f"Closed position for {symbol} {timeframe}",
                    "reason": reason
                }
            else:
                return {
                    "success": False, 
                    "message": f"No active position found for {symbol} {timeframe}"
                }
                
        except Exception as e:
            logger.error(f"Error force closing position: {e}")
            return {"success": False, "error": str(e)}
    
    def update_all_positions(self, current_prices: Dict[str, float]) -> List[Dict]:
        """Update all positions with current prices via PositionManager"""
        try:
            updates = self.position_manager.update_positions()
            
            # Format results for compatibility
            results = []
            for position_id, update_info in updates.items():
                # Extract symbol from position_id (format: SYMBOL_TIMEFRAME_DIRECTION)
                parts = position_id.split('_')
                if len(parts) >= 3:
                    symbol = parts[0]
                    timeframe = parts[1]
                    
                    result = {
                        "symbol": symbol,
                        "timeframe": timeframe,
                        "position_id": position_id,
                        "update_info": update_info
                    }
                    results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"Error updating all positions: {e}")
            return []

    def get_data_storage_stats(self) -> Dict:
        """Get data storage statistics from DataManager"""
        try:
            return self.data_manager.get_cache_stats()
        except Exception as e:
            logger.error(f"Error getting data storage stats: {e}")
            return {"error": str(e)}
    
    def force_data_update(self, symbol: str, timeframe: str):
        """Force data update for symbol/timeframe via DataManager"""
        try:
            # Clear cache to force refresh
            self.data_manager.clear_cache()
            logger.info(f"Forced data update for {symbol} {timeframe}")
        except Exception as e:
            logger.error(f"Error forcing data update for {symbol} {timeframe}: {e}")
    
    def clear_position_history(self):
        """Clear position history for testing"""
        self.active_positions.clear()
        logger.info("Cleared position history and tracking")
    
    def shutdown(self):
        """Shutdown SignalDetector and cleanup resources"""
        try:
            logger.info("Shutting down SignalDetector CONSERVATIVE mode...")
            
            # Clear data manager cache
            if hasattr(self.data_manager, 'clear_cache'):
                self.data_manager.clear_cache()
            
            # Cleanup old positions  
            if hasattr(self.position_manager, 'cleanup_old_positions'):
                self.position_manager.cleanup_old_positions()
            
            # Clear tracking
            self.active_positions.clear()
                
            logger.info("SignalDetector shutdown complete")
            
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")

========================================
FILE: app/services/line_notifier.py
========================================

"""LINE Bot notification service for trading signals - REFACTORED for v2.0"""
import logging
import requests
from datetime import datetime
from typing import Dict, Optional

from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError, LineBotApiError
from linebot.models import TextSendMessage

logger = logging.getLogger(__name__)


class LineNotifier:
    """
    REFACTORED LINE Bot service for v2.0
    
    Main responsibilities:
    - Send trading signal notifications
    - Send position update alerts
    - Send daily summaries and error alerts
    - Handle LINE webhook verification
    
    Uses ConfigManager for:
    - LINE channel access token
    - LINE channel secret
    - LINE user ID
    """

    def __init__(self, config: Dict):
        """
        Initialize LINE notifier with ConfigManager config
        
        Args:
            config: Configuration from ConfigManager.get_line_config()
                   Expected keys: 'access_token', 'secret', optionally 'user_id'
        """
        # Configuration from ConfigManager
        self.channel_access_token = config.get("access_token")
        self.channel_secret = config.get("secret")
        self.user_id = config.get("user_id")  # Optional, can be set later

        if not self.channel_access_token or not self.channel_secret:
            logger.warning(
                "LINE credentials not fully configured - notifications disabled"
            )
            self.line_bot_api = None
            self.handler = None
            return

        try:
            self.line_bot_api = LineBotApi(self.channel_access_token)
            self.handler = WebhookHandler(self.channel_secret)
            logger.info("LineNotifier v2.0 initialized successfully")
        except Exception as e:
            logger.error(f"LINE Bot initialization failed: {e}")
            self.line_bot_api = None
            self.handler = None

    def send_signal_alert(self, analysis: Dict) -> bool:
        """‡∏™‡πà‡∏á‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏ó‡∏£‡∏î‡πÑ‡∏õ LINE ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏à‡πà‡∏≤‡πÄ‡∏â‡∏¢"""
        symbol = analysis.get("symbol", "UNKNOWN")
        try:
            # üö® 1. ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏à‡πà‡∏≤‡πÄ‡∏â‡∏¢ (‡∏ó‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏•‡∏¢)
            jachey_url = "https://web-production-82bfc.up.railway.app/callback" # ‡πÄ‡∏ä‡πá‡∏Ñ URL ‡∏≠‡∏µ‡∏Å‡∏ó‡∏µ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö
            try:
                # ‡∏™‡πà‡∏á data ‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô (analysis) ‡πÑ‡∏õ‡πÉ‡∏´‡πâ‡∏à‡πà‡∏≤‡πÄ‡∏•‡∏¢
                requests.post(jachey_url, json=analysis, timeout=5)
                logger.info(f"üëÆ‚Äç‚ôÇÔ∏è [RELAY] ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏∂‡∏á‡∏à‡πà‡∏≤‡πÄ‡∏â‡∏¢‡πÅ‡∏•‡πâ‡∏ß: {symbol}")
            except Exception as e:
                logger.error(f"‚ùå [RELAY] ‡∏™‡πà‡∏á‡∏´‡∏≤‡∏à‡πà‡∏≤‡∏û‡∏•‡∏≤‡∏î: {str(e)}")

            # üö® 2. ‡∏™‡πà‡∏á LINE ‡∏´‡∏≤‡∏û‡∏µ‡πà (‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°)
            if not self.line_bot_api or not self.user_id:
                return False

            signals = analysis.get("signals", {})
            if signals.get("buy") or signals.get("short"):
                message = self._create_entry_signal_message(analysis)
                self.line_bot_api.push_message(self.user_id, TextSendMessage(text=message))
                logger.info(f"‚úÖ LINE ALERT SENT: {symbol}")
                return True
            return False

        except Exception as e:
            logger.error(f"üí• ERROR: {str(e)}")
            return False

    def send_position_update(self, update_data: Dict) -> bool:
        """
        Send position update notification to LINE
        
        Args:
            update_data: Position update data with events and position info
            
        Returns:
            bool: True if message sent successfully
        """
        try:
            if not self.line_bot_api or not self.user_id:
                logger.warning("LINE not properly configured, cannot send position update")
                return False

            # Check if there are significant events to report
            events = update_data.get("events", [])
            if not events:
                return False  # No significant update

            message = self._create_position_update_message(update_data)
            self.line_bot_api.push_message(self.user_id, TextSendMessage(text=message))
            logger.info(f"Position update sent: {', '.join(events)}")
            return True

        except Exception as e:
            logger.error(f"Error sending position update: {e}")
            return False

    def send_daily_summary(self, summary: Dict) -> bool:
        """
        Send daily trading summary
        
        Args:
            summary: Daily summary data
            
        Returns:
            bool: True if message sent successfully
        """
        try:
            if not self.line_bot_api or not self.user_id:
                logger.warning("LINE not properly configured, cannot send daily summary")
                return False

            message = self._create_daily_summary_message(summary)
            self.line_bot_api.push_message(self.user_id, TextSendMessage(text=message))
            logger.info("Daily summary sent")
            return True

        except Exception as e:
            logger.error(f"Error sending daily summary: {e}")
            return False
    
    def _create_entry_signal_message(self, analysis: Dict) -> str:
        """Create formatted message for entry signals - REBOUND ALERT style"""
        symbol = analysis.get("symbol", "UNKNOWN")
        timeframe = analysis.get("timeframe", "4h")
        current_price = analysis.get("current_price", 0)
        signals = analysis.get("signals", {})
        risk_levels = analysis.get("risk_levels", {})
        signal_strength = analysis.get("signal_strength", 0)

        # Get risk levels
        entry_price = risk_levels.get('entry_price', current_price)
        sl_price = risk_levels.get('stop_loss', 0)
        tp1_price = risk_levels.get('take_profit_1', 0)
        tp2_price = risk_levels.get('take_profit_2', 0)
        tp3_price = risk_levels.get('take_profit_3', 0)
        rr_ratio = risk_levels.get('risk_reward_ratio', 0)

        # Determine direction
        if signals.get("buy"):
            direction = "LONG"
            direction_emoji = "üü¢"
            # Calculate percentages for LONG
            sl_pct = ((sl_price - entry_price) / entry_price) * 100
            tp1_pct = ((tp1_price - entry_price) / entry_price) * 100
            tp2_pct = ((tp2_price - entry_price) / entry_price) * 100
            tp3_pct = ((tp3_price - entry_price) / entry_price) * 100
        elif signals.get("short"):
            direction = "SHORT"
            direction_emoji = "üî¥"
            # Calculate percentages for SHORT
            sl_pct = ((sl_price - entry_price) / entry_price) * 100
            tp1_pct = -((entry_price - tp1_price) / entry_price) * 100  # ‚úÖ ‡πÄ‡∏õ‡πá‡∏ô -
            tp2_pct = -((entry_price - tp2_price) / entry_price) * 100  # ‚úÖ ‡πÄ‡∏õ‡πá‡∏ô -
            tp3_pct = -((entry_price - tp3_price) / entry_price) * 100  # ‚úÖ ‡πÄ‡∏õ‡πá‡∏ô -
        else:
            direction = "UNKNOWN"
            direction_emoji = "‚ùì"
            sl_pct = tp1_pct = tp2_pct = tp3_pct = 0

        # Calculate R:R ratios for each TP
        if sl_pct != 0:
            rr1 = abs(tp1_pct / sl_pct)
            rr2 = abs(tp2_pct / sl_pct)
            rr3 = abs(tp3_pct / sl_pct)
        else:
            rr1 = rr2 = rr3 = 0

        # Get indicator values
        indicators = analysis.get("indicators", {})
        squeeze = indicators.get("squeeze", {})
        macd = indicators.get("macd", {})
        rsi = indicators.get("rsi", {})

        # ‚úÖ ‡∏™‡∏µ‡πÅ‡∏•‡∏∞ strategy ‡∏ï‡∏≤‡∏° timeframe
        if timeframe == "1d":
            header_emoji = "üîµ‚ö° CDC ALERT ‚ö°üîµ"
            strategy_name = "1D SWING"
            
            # 1D indicators (CDC ActionZone)
            ema12 = analysis.get("ema12", 0)
            ema26 = analysis.get("ema26", 0)
            
            if ema12 > ema26:
                trend_status = "GREEN Trend"
            else:
                trend_status = "RED Trend"
            
            indicator_line = f"""üìä CDC: {trend_status}
üìä RSI: {rsi.get('value', 50):.1f}"""
            
        elif timeframe == "4h":
            header_emoji = "üü¢‚ö° SQUEEZE ALERT ‚ö°üü¢"
            strategy_name = "4H SWING"
            
            # 4H indicators
            squeeze_status = "OFF ‚úÖ" if squeeze.get('squeeze_off') else "ON ‚ùå"
            momentum = squeeze.get('momentum_direction', 'NEUTRAL')
            macd_cross = macd.get('cross_direction', 'NONE')
            
            indicator_line = f"""üìä Squeeze: {squeeze_status}
üìä Momentum: {momentum}
üìä MACD: {macd_cross} Cross
üìä RSI: {rsi.get('value', 50):.1f}"""
            
        else:  # 15m or other
            header_emoji = "üü°‚ö° REBOUND ALERT ‚ö°üü°"
            strategy_name = "15m SCALP (Rebound)"
            
            # 15m rebound indicators
            rsi_value = rsi.get('value', 50)
            rsi_status = "Oversold" if rsi_value < 35 else "Overbought" if rsi_value > 65 else "Neutral"
            
            # BB values if available
            bb_data = indicators.get("bb", {})
            bb_upper = bb_data.get('upper', 0)
            bb_lower = bb_data.get('lower', 0)
            
            if bb_upper > 0 and bb_lower > 0:
                indicator_line = f"""üìä RSI: {rsi_value:.1f} ({rsi_status})
üìä BB Upper: {bb_upper:.2f}
ÔøΩÔøΩ BB Lower: {bb_lower:.2f}
‚ö†Ô∏è Quick Entry/Exit - Scalp Only!"""
            else:
                indicator_line = f"""üìä RSI: {rsi_value:.1f} ({rsi_status})
‚ö†Ô∏è Quick Entry/Exit - Scalp Only!"""

        # Create formatted message
        message = f"""{header_emoji} REBOUND ALERT {header_emoji}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä Strategy: {strategy_name}
ü™ô {symbol} - {direction} {direction_emoji}
üíµ Entry: {entry_price:,.2f}
üõë SL: {sl_price:,.2f} ({sl_pct:+.1f}%)
üéØ TP1: {tp1_price:,.2f} ({tp1_pct:+.1f}%) [{rr1:.1f}:1]
üéØ TP2: {tp2_price:,.2f} ({tp2_pct:+.1f}%) [{rr2:.1f}:1]
üéØ TP3: {tp3_price:,.2f} ({tp3_pct:+.1f}%) [{rr3:.1f}:1]
{indicator_line}
üïê {datetime.now().strftime('%H:%M:%S')}
ü§ñ SIGNAL-ALERT v2.2
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"""

        return message
    def _create_position_update_message(self, update_data: Dict) -> str:
        """Create formatted message for position updates"""
        # Extract position and update information
        position = update_data.get("position", {})
        updates = update_data.get("updates", {})
        events = update_data.get("events", [])

        symbol = position.get("symbol", "UNKNOWN")
        direction = position.get("direction", "UNKNOWN")
        current_price = position.get("current_price", 0)
        pnl_pct = position.get("pnl_pct", 0)

        # Direction emoji
        direction_emoji = "üü¢" if direction == "LONG" else "üî¥" if direction == "SHORT" else "‚ö´"

        message = f"üìä POSITION UPDATE v2.0\n\n"
        message += f"{direction_emoji} {direction} Position\n"
        message += f"Symbol: {symbol}\n"
        message += f"Current Price: ${current_price:.4f}\n"

        # P&L with color
        pnl_emoji = "üü¢" if pnl_pct > 0 else "üî¥" if pnl_pct < 0 else "‚ö´"
        message += f"P&L: {pnl_emoji} {pnl_pct:+.2f}%\n\n"

        # Report events
        for event in events:
            if "SL hit" in event:
                message += f"üõë {event}\n"
            elif "TP" in event and "hit" in event:
                message += f"üéØ {event}\n"
            elif "Position closed" in event:
                message += f"üèÅ {event}\n"

        message += f"\nüïê {datetime.now().strftime('%H:%M:%S')}"
        message += f"\n#{symbol} #{direction} #Update #v2"

        return message

    def _create_daily_summary_message(self, summary: Dict) -> str:
        """Create formatted daily summary message"""
        total_signals = summary.get("total_signals", 0)
        active_positions = summary.get("active_positions", 0)
        closed_positions = summary.get("closed_positions", 0)
        total_pnl_pct = summary.get("total_pnl_pct", 0)
        win_rate_pct = summary.get("win_rate_pct", 0)
        wins = summary.get("wins", 0)
        losses = summary.get("losses", 0)
        version = summary.get("version", "2.0")

        # P&L with color
        pnl_emoji = "üü¢" if total_pnl_pct > 0 else "üî¥" if total_pnl_pct < 0 else "‚ö´"

        message = f"üìà DAILY SUMMARY {version}\n\n"
        message += f"üö® Signals Today: {total_signals}\n"
        message += f"üìä Active Positions: {active_positions}\n"
        message += f"‚úÖ Closed Positions: {closed_positions}\n"
        message += f"üí∞ Total P&L: {pnl_emoji} {total_pnl_pct:+.2f}%\n"
        message += f"üéØ Win Rate: {win_rate_pct:.1f}% ({wins}W/{losses}L)\n\n"

        # Best/worst performers if available
        best_performer = summary.get("best_performer", "")
        worst_performer = summary.get("worst_performer", "")

        if best_performer:
            message += f"üèÜ Best: {best_performer}\n"
        if worst_performer:
            message += f"üìâ Worst: {worst_performer}\n"

        message += f"\nüìÖ {datetime.now().strftime('%Y-%m-%d')}"
        message += f"\n#DailySummary #SqueezeBot #{version.replace('.', '')}"

        return message

    def send_test_message(self) -> bool:
        """Send test message to verify LINE integration"""
        try:
            if not self.line_bot_api or not self.user_id:
                logger.warning("LINE not properly configured for test")
                return False

            test_message = f"ü§ñ Squeeze Bot Test Message v2.0\n\n"
            test_message += f"‚úÖ LINE integration is working!\n"
            test_message += f"üïê Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            test_message += f"üöÄ Status: Ready for LONG/SHORT signals\n"
            test_message += f"üîß Version: 2.0-refactored"

            self.line_bot_api.push_message(
                self.user_id, TextSendMessage(text=test_message)
            )
            logger.info("Test message sent successfully")
            return True

        except Exception as e:
            logger.error(f"Error sending test message: {e}")
            return False

    def send_error_alert(self, error_message: str, context: str = "") -> bool:
        """Send error alert to LINE"""
        try:
            if not self.line_bot_api or not self.user_id:
                logger.warning("LINE not properly configured, cannot send error alert")
                return False

            message = f"‚ö†Ô∏è SQUEEZE BOT ERROR v2.0\n\n"
            message += f"üö® Error: {error_message}\n"
            if context:
                message += f"üìç Context: {context}\n"
            message += f"\nüïê Time: {datetime.now().strftime('%H:%M:%S')}"

            self.line_bot_api.push_message(self.user_id, TextSendMessage(text=message))
            logger.info("Error alert sent to LINE")
            return True

        except Exception as e:
            logger.error(f"Failed to send error alert: {e}")
            return False

    def verify_webhook_signature(self, body: str, signature: str) -> bool:
        """Verify LINE webhook signature"""
        try:
            if not self.handler:
                return False
            self.handler.handle(body, signature)
            return True
        except InvalidSignatureError:
            logger.error("Invalid LINE webhook signature")
            return False
        except Exception as e:
            logger.error(f"Webhook signature verification error: {e}")
            return False

    def set_user_id(self, user_id: str):
        """Set LINE user ID for notifications"""
        self.user_id = user_id
        logger.info(f"LINE user ID set: {user_id}")

    def is_configured(self) -> bool:
        """Check if LINE notifier is properly configured"""
        return (
            self.line_bot_api is not None
            and self.channel_access_token is not None
            and self.channel_secret is not None
        )

    def is_ready(self) -> bool:
        """Check if LINE notifier is ready to send messages"""
        return self.is_configured() and self.user_id is not None

    def get_status(self) -> Dict:
        """Get LINE notifier status"""
        return {
            "configured": self.is_configured(),
            "ready": self.is_ready(),
            "has_user_id": self.user_id is not None,
            "version": "2.0-refactored",
        }

    def shutdown(self):
        """Shutdown LINE notifier"""
        try:
            logger.info("Shutting down LineNotifier v2.0...")
            # Clean up any resources if needed
            logger.info("LineNotifier shutdown complete")
        except Exception as e:
            logger.error(f"Error during LineNotifier shutdown: {e}")

    # Legacy compatibility methods
    def send_position_alert(self, position_data: Dict) -> bool:
        """Legacy method - redirects to send_position_update"""
        # Convert legacy format to new format
        update_data = {
            "position": position_data,
            "events": position_data.get("events", []),
            "updates": position_data.get("updates", {}),
        }
        return self.send_position_update(update_data)
========================================
FILE: app/services/data_manager.py
========================================

import logging
import requests
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
import pandas as pd
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

from ..utils.core_utils import JSONManager, ErrorHandler
from ..utils.data_types import DataConverter
from .config_manager import ConfigManager

class DataManager:
    """Centralized data management - ‡∏£‡∏ß‡∏° DataUpdater + PriceFetcher"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.config = ConfigManager()
        self.json_manager = JSONManager()
        self.data_converter = DataConverter()
        self.rebound_callback = None  # Callback for 15m candle close
        
        # Cache management
        self.cache = {}
        self.price_cache = {}
        self.last_requests = {}
        
        # Rate limiting
        self.min_request_interval = 0.2  # 200ms between requests
        self.price_cache_timeout = 30    # 30 seconds for price cache
        
        # Setup requests session with connection pooling
        self._setup_session()
        
        self.logger.info("‚úÖ DataManager initialized")
    
    def register_rebound_callback(self, callback):
        """Register callback for 15m candle close events"""
        self.rebound_callback = callback
        self.logger.info("‚úÖ Registered rebound callback for 15m signals")
    
    def _setup_session(self):
        """Setup requests session with connection pooling"""
        self.session = requests.Session()
        
        # Retry strategy
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=retry_strategy
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    @ErrorHandler.api_error_handler
    def get_current_prices(self, symbols: List[str]) -> Dict[str, float]:
        """Get current prices for multiple symbols"""
        try:
            binance_config = self.config.get_binance_config()
            symbols_param = '["' + '","'.join(symbols) + '"]'
            url = f"{binance_config['base_url']}/ticker/price"
            
            response = self.session.get(
                url, 
                params={'symbols': symbols_param}, 
                timeout=binance_config['timeout']
            )
            response.raise_for_status()
            
            prices = {}
            for item in response.json():
                price = float(item['price'])
                if self.data_converter.validate_price_data(price):
                    prices[item['symbol']] = price
            
            # Update cache
            now = time.time()
            for symbol, price in prices.items():
                self.price_cache[f"price_{symbol}"] = {
                    'price': price,
                    'timestamp': now
                }
            
            self.logger.info(f"Fetched {len(prices)} current prices")
            return prices
            
        except Exception as e:
            self.logger.error(f"Error fetching current prices: {e}")
            return {}
    
    def get_current_prices_cached(self, symbols: List[str]) -> Dict[str, float]:
        """Get current prices with intelligent caching"""
        now = time.time()
        fresh_prices = {}
        symbols_to_fetch = []
        
        # Check cache first
        for symbol in symbols:
            cache_key = f"price_{symbol}"
            if cache_key in self.price_cache:
                cached_data = self.price_cache[cache_key]
                if now - cached_data['timestamp'] < self.price_cache_timeout:
                    fresh_prices[symbol] = cached_data['price']
                else:
                    symbols_to_fetch.append(symbol)
            else:
                symbols_to_fetch.append(symbol)
        
        # Fetch missing prices
        if symbols_to_fetch:
            fetched_prices = self.get_current_prices(symbols_to_fetch)
            fresh_prices.update(fetched_prices)
        
        return fresh_prices
    
    def get_single_price(self, symbol: str) -> Optional[float]:
        """Get single symbol price with rate limiting"""
        now = time.time()
        last_request = self.last_requests.get(symbol, 0)
        
        if now - last_request < self.min_request_interval:
            time.sleep(self.min_request_interval - (now - last_request))
        
        try:
            binance_config = self.config.get_binance_config()
            url = f"{binance_config['base_url']}/ticker/price"
            
            response = self.session.get(
                url, 
                params={'symbol': symbol}, 
                timeout=binance_config['timeout']
            )
            response.raise_for_status()
            
            self.last_requests[symbol] = time.time()
            price = float(response.json()['price'])
            
            if self.data_converter.validate_price_data(price):
                # Update cache
                self.price_cache[f"price_{symbol}"] = {
                    'price': price,
                    'timestamp': time.time()
                }
                return price
            
            return None
            
        except Exception as e:
            self.logger.error(f"Error fetching price for {symbol}: {e}")
            return None
    
    def get_klines(self, symbol: str, interval: str, limit: int = 500) -> Optional[pd.DataFrame]:
        """Get klines data with caching"""
        cache_key = f"{symbol}_{interval}"
        
        # Check cache first
        if cache_key in self.cache:
            cached_data = self.cache[cache_key]
            if self._is_cache_valid(cached_data, interval):
                return cached_data['df']
        
        try:
            binance_config = self.config.get_binance_config()
            url = f"{binance_config['base_url']}/klines"
            params = {
                'symbol': symbol,
                'interval': interval,
                'limit': limit
            }
            
            response = self.session.get(
                url, 
                params=params, 
                timeout=binance_config['timeout']
            )
            response.raise_for_status()
            
            data = response.json()
            
            df = pd.DataFrame(data, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_asset_volume', 'number_of_trades',
                'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
            ])
            
            # Convert data types
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()
            
            # Validate DataFrame
            if not self.data_converter.validate_dataframe(df):
                self.logger.warning(f"Invalid DataFrame for {symbol} {interval}")
                return None
            
            # Update cache
            self.cache[cache_key] = {
                'df': df,
                'timestamp': datetime.now()
            }
            
            # Save to file for persistence
            self._save_to_file(symbol, interval, df)
            
            self.logger.debug(f"Loaded {len(df)} candles for {symbol} {interval}")
            return df
            
        except Exception as e:
            self.logger.error(f"Error fetching klines for {symbol}: {e}")
            return self._load_from_file(symbol, interval)
    
    def _is_cache_valid(self, cached_data: Dict, interval: str) -> bool:
        """Check if cached data is still valid"""
        now = datetime.now()
        cache_time = cached_data['timestamp']
        
        # Cache validity periods
        validity_periods = {
            '1m': timedelta(minutes=1),
            '5m': timedelta(minutes=5),
            '15m': timedelta(minutes=15),
            '1h': timedelta(hours=1),
            '4h': timedelta(hours=2),
            '1d': timedelta(hours=6)
        }
        
        validity_period = validity_periods.get(interval, timedelta(minutes=30))
        return (now - cache_time) < validity_period
    
    def _save_to_file(self, symbol: str, interval: str, df: pd.DataFrame):
        """Save data to JSON file"""
        try:
            month_str = datetime.now().strftime("%Y-%m")
            filename = f"data/candles/{symbol}_{interval}_{month_str}.json"
            
            data = {
                'symbol': symbol,
                'interval': interval,
                'timestamp': datetime.now().isoformat(),
                'data': df.to_dict('records')
            }
            
            # Convert numpy types before saving
            data = self.data_converter.convert_numpy_types(data)
            self.json_manager.save_json(data, filename)
            
        except Exception as e:
            self.logger.error(f"Error saving data to file: {e}")
    
    def _load_from_file(self, symbol: str, interval: str) -> Optional[pd.DataFrame]:
        """Load data from JSON file as fallback"""
        try:
            month_str = datetime.now().strftime("%Y-%m")
            filename = f"data/candles/{symbol}_{interval}_{month_str}.json"
            
            data = self.json_manager.load_json(filename)
            if not data or 'data' not in data:
                return None
            
            df = pd.DataFrame(data['data'])
            if df.empty:
                return None
                
            # Convert timestamp column
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Validate loaded data
            if self.data_converter.validate_dataframe(df):
                self.logger.info(f"Loaded fallback data for {symbol} {interval}")
                return df
            
            return None
            
        except Exception as e:
            self.logger.error(f"Error loading data from file: {e}")
            return None
    
    def clear_cache(self):
        """Clear all caches"""
        self.cache.clear()
        self.price_cache.clear()
        self.logger.info("Cache cleared")
    

    def process_websocket_kline(self, kline_data: Dict, signal_detector=None):
        """Process real-time kline from WebSocket"""
        self.logger.info("üéØ DataManager.process_websocket_kline() called")
        try:
            symbol = kline_data['symbol']
            timeframe = kline_data['timeframe']
            cache_key = f"{symbol}_{timeframe}_realtime"
            
            # Update real-time cache
            self.cache[cache_key] = {
                'data': kline_data,
                'timestamp': datetime.now()
            }
            
            # Debug log every update
            self.logger.info(
                f"üìä {symbol} {timeframe} | "
                f"C: {kline_data['close']:.2f} | "
                f"Closed: {kline_data.get('is_closed')}"
            )
            
            # Trigger rebound callback for 15m candle close
            if kline_data.get('is_closed') and timeframe == '15m' and self.rebound_callback:
                try:
                    self.logger.info(f"üü° 15m candle closed: {symbol} @ {kline_data['close']:.2f}")
                    self.rebound_callback(kline_data)
                except Exception as e:
                    self.logger.error(f"Error in rebound callback: {e}")
            
            # When candle closes
            if kline_data.get('is_closed'):
                self.logger.info(
                    f"üìä Candle closed: {symbol} {timeframe} "
                    f"C: {kline_data['close']:.2f}"
                )
                
                # TODO: Forward to SignalDetector for analysis
            
        except Exception as e:
            self.logger.error(f"Error processing WebSocket kline: {e}")

    def get_cache_stats(self) -> Dict:
        """Get cache statistics"""
        return {
            'klines_cache_size': len(self.cache),
            'price_cache_size': len(self.price_cache),
            'last_requests_count': len(self.last_requests)
        }
========================================
FILE: app/services/position_manager.py
========================================

import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import pandas as pd

from ..utils.core_utils import JSONManager, ErrorHandler
from ..utils.data_types import DataConverter
try:
    from config.settings import RISK_MANAGEMENT
except ImportError:
    # Fallback if settings not available
    RISK_MANAGEMENT = {
        "4h": {"tp_levels": [3.0, 5.0, 7.0], "sl_level": 3.0},
        "1h": {"tp_levels": [2.0, 3.5, 5.0], "sl_level": 2.0},
        "1d": {"tp_levels": [5.0, 8.0, 12.0], "sl_level": 4.0}
    }

class PositionManager:
    """Centralized position management - ‡∏£‡∏ß‡∏° PositionTracker + PriceMonitor logic"""
    
    PRICE_TOLERANCE = 0.005  # 0.1% tolerance for TP/SL detection
    
    def __init__(self, data_manager):
        self.logger = logging.getLogger(__name__)
        self.json_manager = JSONManager()
        self.data_converter = DataConverter()
        self.data_manager = data_manager
        self.positions_file = "data/positions.json"
        self.positions = self._load_positions()
        
        self.logger.info("‚úÖ PositionManager initialized")
    
    @ErrorHandler.service_error_handler("PositionManager")
    def create_position(self, signal_data: Dict) -> Optional[str]:
        """Create new position from signal"""
        try:
            symbol = signal_data['symbol']
            timeframe = signal_data['timeframe']
            direction = signal_data['direction']
            entry_price = signal_data['current_price']
            
            # Validate entry price
            if not self.data_converter.validate_price_data(entry_price):
                self.logger.error(f"Invalid entry price: {entry_price}")
                return None
            
            # Check price sanity (‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö cached price ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
            cached_price = self.data_manager.price_cache.get(symbol)
            if not self.validate_price_sanity(symbol, entry_price, cached_price):
                self.logger.error(f"Invalid entry price: {symbol} = {entry_price}")
                return None
            
            # Check for existing position
            position_id = f"{symbol}_{timeframe}_{direction}"
            if position_id in self.positions:
                existing = self.positions[position_id]
                if existing['status'] == 'ACTIVE':
                    self.logger.info(f"Position {position_id} already exists")
                    return None
            
            # Calculate TP/SL levels
            tp_levels, sl_level = self._calculate_levels(entry_price, direction, timeframe)
            
            position = {
                'id': position_id,
                'symbol': symbol,
                'timeframe': timeframe,
                'direction': direction,
                'entry_price': entry_price,
                'entry_time': datetime.now().isoformat(),
                'status': 'ACTIVE',
                'tp_levels': tp_levels,
                'sl_level': sl_level,
                'tp_hit': {'TP1': False, 'TP2': False, 'TP3': False},
                'sl_hit': False,
                'current_price': entry_price,
                'pnl_pct': 0.0,
                'signal_strength': signal_data.get('signal_strength', 0),
                'created_by': 'signal_detector',
                'last_update': datetime.now().isoformat()
            }
            
            # Sanitize data before saving
            position = self.data_converter.sanitize_signal_data(position)
            
            self.positions[position_id] = position
            self._save_positions()
            
            self.logger.info(f"‚úÖ Created position: {position_id} at {entry_price}")
            return position_id
            
        except Exception as e:
            self.logger.error(f"Error creating position: {e}")
            return None
    
    @ErrorHandler.service_error_handler("PositionManager")
    def update_positions(self) -> Dict[str, Dict]:
        """Update all active positions with current prices"""
        updates = {}
        
        try:
            active_positions = {k: v for k, v in self.positions.items() 
                              if v['status'] == 'ACTIVE'}
            
            if not active_positions:
                return updates
            
            # Get current prices for all active symbols
            symbols = list(set([pos['symbol'] for pos in active_positions.values()]))
            current_prices = self.data_manager.get_current_prices_cached(symbols)
            
            for position_id, position in active_positions.items():
                symbol = position['symbol']
                current_price = current_prices.get(symbol)
                
                if current_price is None:
                    continue
                
                # Update position with current price
                old_price = position['current_price']
                position['current_price'] = current_price
                position['last_update'] = datetime.now().isoformat()
                
                # Calculate P&L
                entry_price = position['entry_price']
                direction = position['direction']
                
                if direction == 'LONG':
                    pnl_pct = ((current_price - entry_price) / entry_price) * 100
                else:  # SHORT
                    pnl_pct = ((entry_price - current_price) / entry_price) * 100
                
                position['pnl_pct'] = round(pnl_pct, 2)
                
                # Check TP/SL hits
                tp_sl_update = self._check_tp_sl_hits(position, old_price, current_price)
                if tp_sl_update:
                    updates[position_id] = tp_sl_update
            
            self._save_positions()
            
            if updates:
                self.logger.info(f"üìä Updated {len(updates)} positions with TP/SL hits")
            
            return updates
            
        except Exception as e:
            self.logger.error(f"Error updating positions: {e}")
            return {}
    
    def _check_tp_sl_hits(self, position: Dict, old_price: float, current_price: float) -> Optional[Dict]:
        """Check if TP/SL levels are hit"""
        direction = position['direction']
        tp_levels = position['tp_levels']
        sl_level = position['sl_level']
        updates = {}
        
        try:
            # Check TP hits
            for tp_name, tp_price in tp_levels.items():
                if not position['tp_hit'][tp_name]:
                    hit = False
                    
                    if direction == 'LONG':
                        tp_threshold = tp_price * (1 - self.PRICE_TOLERANCE)
                        hit = current_price >= tp_threshold
                    else:  # SHORT
                        tp_threshold = tp_price * (1 + self.PRICE_TOLERANCE)
                        hit = current_price <= tp_threshold
                    
                    if hit:
                        position['tp_hit'][tp_name] = True
                        updates[f'{tp_name}_hit'] = {
                            'hit': True,
                            'price': current_price,
                            'target_price': tp_price,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        self.logger.info(f"üéØ {tp_name} hit for {position['symbol']}: {current_price}")
                        
                        # Check if all TPs hit
                        if all(position['tp_hit'].values()):
                            position['status'] = 'CLOSED'
                            position['close_reason'] = 'ALL_TP_HIT'
                            position['close_time'] = datetime.now().isoformat()
                            updates['position_closed'] = True
            
            # Check SL hit
            if not position['sl_hit']:
                sl_hit = False
                
                if direction == 'LONG':
                    sl_threshold = sl_level * (1 + self.PRICE_TOLERANCE)
                    sl_hit = current_price <= sl_threshold
                else:  # SHORT
                    sl_threshold = sl_level * (1 - self.PRICE_TOLERANCE)
                    sl_hit = current_price >= sl_threshold
                
                if sl_hit:
                    position['sl_hit'] = True
                    position['status'] = 'CLOSED'
                    position['close_reason'] = 'SL_HIT'
                    position['close_time'] = datetime.now().isoformat()
                    updates['sl_hit'] = {
                        'hit': True,
                        'price': current_price,
                        'target_price': sl_level,
                        'timestamp': datetime.now().isoformat()
                    }
                    updates['position_closed'] = True
                    
                    self.logger.info(f"üõë SL hit for {position['symbol']}: {current_price}")
            
            return updates if updates else None
            
        except Exception as e:
            self.logger.error(f"Error checking TP/SL hits: {e}")
            return None
    
    def _calculate_levels(self, entry_price: float, direction: str, timeframe: str) -> Tuple[Dict, float]:
        """Calculate TP and SL levels"""
        risk_config = RISK_MANAGEMENT.get(timeframe, RISK_MANAGEMENT['4h'])
        tp_percentages = risk_config['tp_levels']  # [3.0, 5.0, 7.0]
        sl_percentage = risk_config['sl_level']    # 3.0
        
        tp_levels = {}
        
        if direction == 'LONG':
            tp_levels['TP1'] = round(entry_price * (1 + tp_percentages[0] / 100), 8)
            tp_levels['TP2'] = round(entry_price * (1 + tp_percentages[1] / 100), 8)
            tp_levels['TP3'] = round(entry_price * (1 + tp_percentages[2] / 100), 8)
            sl_level = round(entry_price * (1 - sl_percentage / 100), 8)
        else:  # SHORT
            tp_levels['TP1'] = round(entry_price * (1 - tp_percentages[0] / 100), 8)
            tp_levels['TP2'] = round(entry_price * (1 - tp_percentages[1] / 100), 8)
            tp_levels['TP3'] = round(entry_price * (1 - tp_percentages[2] / 100), 8)
            sl_level = round(entry_price * (1 + sl_percentage / 100), 8)
        
        return tp_levels, sl_level
    
    def get_active_positions(self) -> Dict:
        """Get all active positions"""
        return {k: v for k, v in self.positions.items() if v['status'] == 'ACTIVE'}
    
    def get_position_status(self, symbol: str, timeframe: str) -> Optional[Dict]:
        """Get position status for symbol/timeframe"""
        for position in self.positions.values():
            if (position['symbol'] == symbol and 
                position['timeframe'] == timeframe and 
                position['status'] == 'ACTIVE'):
                return position
        return None
    
    @ErrorHandler.service_error_handler("PositionManager")
    def close_position(self, position_id: str, reason: str = 'MANUAL') -> bool:
        """Manually close a position"""
        try:
            if position_id in self.positions:
                self.positions[position_id]['status'] = 'CLOSED'
                self.positions[position_id]['close_reason'] = reason
                self.positions[position_id]['close_time'] = datetime.now().isoformat()
                self._save_positions()
                self.logger.info(f"üîí Closed position {position_id}: {reason}")
                return True
            return False
        except Exception as e:
            self.logger.error(f"Error closing position: {e}")
            return False
    
    def get_positions_summary(self) -> Dict:
        """Get positions summary statistics"""
        all_positions = self.positions
        active_positions = self.get_active_positions()
        closed_positions = [pos for pos in all_positions.values() if pos['status'] == 'CLOSED']
        
        # Calculate P&L stats
        total_pnl = sum(pos.get('pnl_pct', 0) for pos in active_positions.values())
        wins = len([pos for pos in closed_positions if pos.get('pnl_pct', 0) > 0])
        losses = len([pos for pos in closed_positions if pos.get('pnl_pct', 0) < 0])
        win_rate = (wins / len(closed_positions) * 100) if closed_positions else 0
        
        return {
            'total_positions': len(all_positions),
            'active_positions': len(active_positions),
            'closed_positions': len(closed_positions),
            'total_pnl_pct': round(total_pnl, 2),
            'win_rate_pct': round(win_rate, 2),
            'wins': wins,
            'losses': losses
        }
    
    def _load_positions(self) -> Dict:
        """Load positions from JSON file"""
        positions = self.json_manager.load_json(self.positions_file, {})
        self.logger.info(f"üìÇ Loaded {len(positions)} positions")
        return positions
    
    def _save_positions(self):
        """Save positions to JSON file"""
        success = self.json_manager.save_json(self.positions, self.positions_file)
        if not success:
            self.logger.error("‚ùå Failed to save positions")
    
    def cleanup_old_positions(self, days_old: int = 30):
        """Clean up old closed positions"""
        try:
            cutoff_date = datetime.now().timestamp() - (days_old * 24 * 60 * 60)
            
            positions_to_remove = []
            for pos_id, position in self.positions.items():
                if position['status'] == 'CLOSED':
                    close_time = position.get('close_time')
                    if close_time:
                        close_timestamp = pd.to_datetime(close_time).timestamp()
                        if close_timestamp < cutoff_date:
                            positions_to_remove.append(pos_id)
            
            for pos_id in positions_to_remove:
                del self.positions[pos_id]
            
            if positions_to_remove:
                self._save_positions()
                self.logger.info(f"üßπ Cleaned up {len(positions_to_remove)} old positions")
            
        except Exception as e:
            self.logger.error(f"Error cleaning up positions: {e}")
    
    def validate_price_sanity(self, symbol: str, price: float, previous_price: float = None) -> bool:
        """Check if price is within reasonable range"""

        # Check zero or negative
        if price <= 0:
            self.logger.error(f"INVALID PRICE: {symbol} = {price} (zero or negative)")
            return False

        # Check percentage change if we have previous price
        if previous_price and previous_price > 0:
            pct_change = abs((price - previous_price) / previous_price * 100)
            if pct_change > 30:
                self.logger.error(
                    f"SUSPICIOUS PRICE CHANGE: {symbol} {previous_price} -> {price} ({pct_change:.1f}%)"
                )
                return False

        return True

========================================
FILE: app/services/websocket_manager.py
========================================

"""
WebSocket Manager v2.2 - Real-time Binance Data Stream
"""
import logging
import json
import time
import threading
from typing import Callable, Optional, Dict
import websocket

logger = logging.getLogger(__name__)

class WebSocketManager:
    def __init__(self, symbol: str = "btcusdt", timeframe: str = "15m"):
        self.symbol = symbol.lower()
        self.timeframe = timeframe.lower()
        self.base_url = "wss://stream.binance.com:9443/ws"
        self.stream_name = f"{self.symbol}@kline_{self.timeframe}"
        
        self.ws = None
        self.ws_thread = None
        self.is_running = False
        self.reconnect_attempts = 0
        self.max_reconnect_attempts = 10
        self.reconnect_delay = 5
        self.on_kline_callback: Optional[Callable] = None
        
        logger.info(f"WebSocketManager initialized: {self.stream_name}")

    def connect(self):
        if self.is_running:
            logger.warning("WebSocket already running")
            return
        
        self.is_running = True
        self.reconnect_attempts = 0
        
        ws_url = f"{self.base_url}/{self.stream_name}"
        logger.info(f"Connecting to: {ws_url}")
        
        self.ws = websocket.WebSocketApp(
            ws_url,
            on_message=self._on_message,
            on_error=self._on_error,
            on_close=self._on_close,
            on_open=self._on_open
        )
        
        self.ws_thread = threading.Thread(target=self._run_websocket, daemon=True)
        self.ws_thread.start()
        logger.info("WebSocket thread started")
    
    def _run_websocket(self):
        try:
            self.ws.run_forever(ping_interval=20, ping_timeout=10)
        except Exception as e:
            logger.error(f"WebSocket run error: {e}")
            self._attempt_reconnect()
    
    def disconnect(self):
        logger.info("Disconnecting WebSocket...")
        self.is_running = False
        if self.ws:
            self.ws.close()
        logger.info("WebSocket disconnected")

    def _on_message(self, ws, message):
        logger.info("üîî Message received")
        try:
            data = json.loads(message)
            
            if data.get("e") != "kline":
                return
            
            kline = data.get("k", {})
            
            kline_data = {
                "symbol": data.get("s"),
                "timeframe": self.timeframe,
                "open_time": kline.get("t"),
                "close_time": kline.get("T"),
                "open": float(kline.get("o", 0)),
                "high": float(kline.get("h", 0)),
                "low": float(kline.get("l", 0)),
                "close": float(kline.get("c", 0)),
                "volume": float(kline.get("v", 0)),
                "is_closed": kline.get("x", False)
            }
            
            if self.on_kline_callback:
                self.on_kline_callback(kline_data)
            
            if kline_data["is_closed"]:
                logger.info(f"üïØÔ∏è Kline closed: {kline_data['symbol']} {self.timeframe} C: {kline_data['close']:.2f}")
                
        except Exception as e:
            logger.error(f"Error processing message: {e}")
    
    def _on_open(self, ws):
        logger.info(f"‚úÖ WebSocket connected: {self.stream_name}")
        self.reconnect_attempts = 0
    
    def _on_error(self, ws, error):
        logger.error(f"‚ùå WebSocket error: {error}")
    
    def _on_close(self, ws, close_status_code, close_msg):
        logger.warning(f"üîå WebSocket closed: {close_status_code} - {close_msg}")
        logger.warning(f"üîç Debug: is_running={self.is_running}, reconnect_attempts={self.reconnect_attempts}")
        
        if self.is_running:
            logger.info("üîÑ Initiating reconnect...")
            self._attempt_reconnect()
        else:
            logger.warning("‚ö†Ô∏è Not reconnecting (is_running=False)")

    def _attempt_reconnect(self):
        if self.reconnect_attempts >= self.max_reconnect_attempts:
            logger.error(f"Max reconnect attempts ({self.max_reconnect_attempts}) reached. Giving up.")
            self.is_running = False
            return
        
        self.reconnect_attempts += 1
        wait_time = self.reconnect_delay * self.reconnect_attempts
        
        logger.info(f"üîÑ Reconnect attempt {self.reconnect_attempts}/{self.max_reconnect_attempts} in {wait_time}s...")
        
        time.sleep(wait_time)
        
        if self.is_running:
            self.ws = None
            ws_url = f"{self.base_url}/{self.stream_name}"
            logger.info(f"Reconnecting to: {ws_url}")
            
            self.ws = websocket.WebSocketApp(
                ws_url,
                on_message=self._on_message,
                on_error=self._on_error,
                on_close=self._on_close,
                on_open=self._on_open
            )
            
            self.ws_thread = threading.Thread(target=self._run_websocket, daemon=True)
            self.ws_thread.start()

    def set_kline_callback(self, callback: Callable):
        self.on_kline_callback = callback
        logger.info("Kline callback registered")
    
    def get_status(self) -> Dict:
        return {
            "is_running": self.is_running,
            "stream": self.stream_name,
            "reconnect_attempts": self.reconnect_attempts,
            "thread_alive": self.ws_thread.is_alive() if self.ws_thread else False
        }
    
    def change_stream(self, symbol: str, timeframe: str):
        logger.info(f"Changing stream to {symbol}@kline_{timeframe}")
        self.disconnect()
        time.sleep(2)
        self.symbol = symbol.lower()
        self.timeframe = timeframe.lower()
        self.stream_name = f"{self.symbol}@kline_{self.timeframe}"
        self.connect()

def test_websocket():
    def on_kline(data):
        print(f"üìä {data['symbol']} {data['timeframe']}")
        print(f"   Close: {data['close']:.2f}")
        print(f"   Closed: {data['is_closed']}")
        print()
    
    ws_manager = WebSocketManager(symbol="btcusdt", timeframe="15m")
    ws_manager.set_kline_callback(on_kline)
    ws_manager.connect()
    
    try:
        print("WebSocket running... Press Ctrl+C to stop")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nStopping...")
    finally:
        ws_manager.disconnect()

if __name__ == "__main__":
    test_websocket()

========================================
FILE: app/services/scheduler.py
========================================

"""
Auto Scheduler for Signal Detection and Notification - REFACTORED for v2.0
Simplified to use refactored services architecture
"""
import json
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, List
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from config.settings import Config

logger = logging.getLogger(__name__)


class SignalScheduler:
    """
    REFACTORED Signal Scheduler for v2.0
    
    Main responsibilities:
    - Schedule automated signal scanning
    - Coordinate between refactored services
    - Send notifications and log data
    - Prevent duplicate signals with cooldown system
    
    Uses refactored services:
    - SignalDetector (with integrated DataManager + PositionManager)
    - LineNotifier (via ConfigManager)
    - SheetsLogger (via ConfigManager)
    """

    def __init__(self, config: Dict):
        """
        Initialize scheduler with refactored architecture
        
        Args:
            config: Configuration dictionary from ConfigManager
        """
        # Basic configuration
        self.config = config
        self.scheduler = BackgroundScheduler()
        self.running = False
        
        # Services (will be injected)
        self.signal_detector = None
        self.position_manager = None
        self.line_notifier = None
        self.sheets_logger = None
        
        # Signal deduplication system
        self.last_signals = {}  # Store signal history
        self.cooldown_minutes = Config.SIGNAL_COOLDOWN_MINUTES
        self.signal_history_file = "data/signal_history.json"
        
        # Load signal history from file
        self._load_signal_history()
        
        logger.info(f"SignalScheduler v2.0 initialized with {self.cooldown_minutes}min cooldown")

    def _load_signal_history(self):
        """Load signal history from file"""
        try:
            if os.path.exists(self.signal_history_file):
                with open(self.signal_history_file, 'r') as f:
                    data = json.load(f)
                
                # Convert string timestamps back to datetime
                for key, timestamp_str in data.items():
                    self.last_signals[key] = datetime.fromisoformat(timestamp_str)
                
                logger.info(f"Loaded {len(self.last_signals)} signal history records")
            else:
                logger.info("No signal history file found, starting fresh")
                
        except Exception as e:
            logger.error(f"Error loading signal history: {e}")
            self.last_signals = {}

    def _save_signal_history(self):
        """Save signal history to file"""
        try:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(self.signal_history_file), exist_ok=True)
            
            # Convert datetime to string for JSON storage
            data = {}
            for key, timestamp in self.last_signals.items():
                data[key] = timestamp.isoformat()
            
            with open(self.signal_history_file, 'w') as f:
                json.dump(data, f, indent=2)
                
            logger.debug(f"Saved {len(data)} signal history records")
            
        except Exception as e:
            logger.error(f"Error saving signal history: {e}")

    def _is_duplicate_signal(self, symbol: str, timeframe: str, direction: str) -> bool:
        """
        Check if signal is duplicate within cooldown period
        
        Args:
            symbol: Trading symbol
            timeframe: Timeframe
            direction: Signal direction (LONG/SHORT)
            
        Returns:
            bool: True if signal is duplicate
        """
        signal_key = f"{symbol}_{timeframe}_{direction}"
        current_time = datetime.now()
        
        # Check if we've sent this signal recently
        if signal_key in self.last_signals:
            last_time = self.last_signals[signal_key]
            time_diff = current_time - last_time
            
            # If still within cooldown period
            if time_diff.total_seconds() < (self.cooldown_minutes * 60):
                remaining_minutes = self.cooldown_minutes - (time_diff.total_seconds() / 60)
                logger.debug(f"Signal cooldown active for {signal_key}: {remaining_minutes:.1f} minutes remaining")
                return True
        
        # Clean up old data (keep only last 24 hours)
        cutoff_time = current_time - timedelta(hours=24)
        keys_to_remove = [
            key for key, timestamp in self.last_signals.items()
            if timestamp < cutoff_time
        ]
        for key in keys_to_remove:
            del self.last_signals[key]
        
        return False

    def _record_signal(self, symbol: str, timeframe: str, direction: str):
        """Record signal that was sent"""
        signal_key = f"{symbol}_{timeframe}_{direction}"
        self.last_signals[signal_key] = datetime.now()
        
        # Save to file immediately
        self._save_signal_history()
        logger.debug(f"Recorded signal: {signal_key}")

    def set_services(self, signal_detector, position_manager, line_notifier, sheets_logger):
        """
        Inject refactored services
        
        Args:
            signal_detector: SignalDetector instance
            position_manager: PositionManager instance
            line_notifier: LineNotifier instance
            sheets_logger: SheetsLogger instance
        """
        self.signal_detector = signal_detector
        self.position_manager = position_manager
        self.line_notifier = line_notifier
        self.sheets_logger = sheets_logger
        
        logger.info("Refactored services injected into scheduler")

    def start_scheduler(self):
        """Start the automated scheduler"""
        if self.running:
            logger.warning("Scheduler already running")
            return
        
        if not all([self.signal_detector, self.position_manager]):
            logger.error("Required services not set")
            return
        
        # Job 1: Scan 4H signals - ‡∏ó‡∏∏‡∏Å 15 ‡∏ô‡∏≤‡∏ó‡∏µ
        self.scheduler.add_job(
            func=self._scan_4h_signals,
            trigger="cron",
            hour="*",
            minute="*/15",      # ‚Üê ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ! ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏õ‡πá‡∏ô "0"
            id="scan_4h_signals",
            name="4H Signal Scanner v2.0",
            replace_existing=True,
        )

        # Job 2: ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß ‚úÖ
        self.scheduler.add_job(
            func=self._scan_1d_signals,
            trigger="cron",
            hour="0,4,8,12,16,20",
            minute=0,
            id="scan_1d_signals",
            name="1D Signal Scanner v2.0 (Every 4H)",
            replace_existing=True,
        )
        
        # Job 2: Update positions every 2 minutes (via PositionManager)
        self.scheduler.add_job(
            func=self._update_positions_refactored,
            trigger=IntervalTrigger(minutes=2),
            id="update_positions",
            name="Position Tracker v2.0",
            replace_existing=True,
        )
        
        # Job 3: Daily summary at midnight
        self.scheduler.add_job(
            func=self._send_daily_summary,
            trigger="cron",
            hour=0,
            minute=0,
            id="daily_summary",
            name="Daily Summary v2.0",
            replace_existing=True,
        )
        
        # Start scheduler
        self.scheduler.start()
        self.running = True
        
        logger.info("SignalScheduler v2.0 started successfully")
        logger.info("Scheduled jobs:")
        logger.info(" - 4H signals: every 15 minutes")
        logger.info(" - 1D signals: every 4 hours (00:00, 04:00, 08:00, 12:00, 16:00, 20:00 UTC)")
        logger.info(" - Position updates: every 2 minutes (via PositionManager)")
        logger.info(" - Daily summary: daily at 00:00 UTC (07:00 ICT)")
        logger.info(f" - Signal cooldown: {self.cooldown_minutes} minutes")

    def stop_scheduler(self):
        """Stop the scheduler"""
        if not self.running:
            logger.warning("Scheduler not running")
            return
        
        # Save history before stopping
        self._save_signal_history()
        
        self.scheduler.shutdown(wait=False)
        self.running = False
        
        logger.info("SignalScheduler v2.0 stopped")

    def get_scheduler_status(self) -> Dict:
        """Get current scheduler status"""
        if not self.running:
            return {
                "status": "stopped",
                "jobs": [],
                "next_run_times": {},
                "signal_history_count": len(self.last_signals),
                "version": "2.0-refactored"
            }
        
        jobs = []
        next_run_times = {}
        
        for job in self.scheduler.get_jobs():
            job_info = {
                "id": job.id,
                "name": job.name,
                "next_run": (
                    job.next_run_time.isoformat() if job.next_run_time else None
                ),
                "trigger": str(job.trigger),
            }
            jobs.append(job_info)
            next_run_times[job.id] = job_info["next_run"]
        
        return {
            "status": "running",
            "jobs": jobs,
            "next_run_times": next_run_times,
            "scheduler_state": str(self.scheduler.state),
            "signal_history_count": len(self.last_signals),
            "cooldown_minutes": self.cooldown_minutes,
            "version": "2.0-refactored",
            "services_connected": {
                "signal_detector": self.signal_detector is not None,
                "position_manager": self.position_manager is not None,
                "line_notifier": self.line_notifier is not None,
                "sheets_logger": self.sheets_logger is not None
            }
        }

    def _scan_4h_signals(self):
        """Scan 4H signals"""
        try:
            logger.info("Starting 4H signal scan v2.0...")
            
            symbols = Config.DEFAULT_SYMBOLS if hasattr(Config, 'DEFAULT_SYMBOLS') else ["BTCUSDT", "ETHUSDT"]
        
            if not symbols:
                logger.warning("No symbols configured for scanning")
                return
            
            active_signals = self.signal_detector.get_active_signals(symbols, ["4h"])
            logger.info(f"Found {len(active_signals)} active signals on 4H")
            
            processed_count = 0
            for signal in active_signals:
                if self._process_signal_refactored(signal, "4h"):
                    processed_count += 1
            
            logger.info(f"Processed {processed_count}/{len(active_signals)} signals on 4H")
            
        except Exception as e:
            logger.error(f"Error in 4H signal scan: {e}")

    def _scan_1d_signals(self):
        """Scan 1D signals using refactored SignalDetector"""
        try:
            logger.info("Starting 1D signal scan v2.0...")
            
            symbols = Config.DEFAULT_SYMBOLS if hasattr(Config, 'DEFAULT_SYMBOLS') else ["BTCUSDT", "ETHUSDT"]
        
            if not symbols:
                logger.warning("No symbols configured for scanning")
                return
            
            # Use refactored SignalDetector
            active_signals = self.signal_detector.get_active_signals(symbols, ["1d"])
            logger.info(f"Found {len(active_signals)} active signals on 1D")
            
            processed_count = 0
            for signal in active_signals:
                if self._process_signal_refactored(signal, "1d"):
                    processed_count += 1
            
            logger.info(f"Processed {processed_count}/{len(active_signals)} signals on 1D")
            
        except Exception as e:
            logger.error(f"Error in 1D signal scan: {e}")
            if self.line_notifier:
                try:
                    self.line_notifier.send_error_alert(
                        f"1D signal scan failed: {str(e)}", "Scheduler v2.0"
                    )
                except:
                    pass

    def _process_signal_refactored(self, signal: Dict, timeframe: str) -> bool:
        """
        Process signal using refactored architecture
        
        Args:
            signal: Signal data from SignalDetector
            timeframe: Timeframe being processed
            
        Returns:
            bool: True if signal was processed successfully
        """
        try:
            # Extract basic signal information
            symbol = signal.get("symbol")
            signals = signal.get("signals", {})
            signal_strength = signal.get("signal_strength", 0)
            position_created = signal.get("position_created", False)
            
            # Validate basic data
            if not symbol:
                return False
            
            # Check signal strength threshold (75%)
            if signal_strength < 75:
                logger.debug(f"Skipping {symbol} {timeframe} - signal strength {signal_strength} < 75")
                return False
            
            # Determine trading direction
            direction = None
            if signals.get("buy"):
                direction = "LONG"
            elif signals.get("short"):
                direction = "SHORT"
            
            if not direction:
                logger.debug(f"No valid signal direction for {symbol} {timeframe}")
                return False
            
            # Check for duplicate signals
            if self._is_duplicate_signal(symbol, timeframe, direction):
                logger.info(f"‚è≠Ô∏è SKIPPED DUPLICATE: {symbol} {timeframe} {direction}")
                return False
            
            # SignalDetector should have already created position if valid
            if not position_created:
                logger.debug(f"Position not auto-created for {symbol} {timeframe}")
                # Still record to prevent duplicate attempts
                self._record_signal(symbol, timeframe, direction)
                return False
            
            # Send LINE notification for new signals with positions
            if self.line_notifier:
                try:
                    self.line_notifier.send_signal_alert(signal)
                    logger.info(f"Sent LINE notification for {symbol} {timeframe} {direction}")
                except Exception as e:
                    logger.warning(f"Failed to send LINE notification: {e}")
            
            # Log to Google Sheets for new signals with positions
            if self.sheets_logger:
                try:
                    self.sheets_logger.log_trading_journal(signal)
                    logger.info(f"Logged to Google Sheets: {symbol} {timeframe} {direction}")
                except Exception as e:
                    logger.warning(f"Failed to log to Google Sheets: {e}")
            
            # Record signal in history
            self._record_signal(symbol, timeframe, direction)
            
            logger.info(f"Processed new signal: {symbol} {timeframe} {direction} (Strength: {signal_strength})")
            return True
            
        except Exception as e:
            logger.error(f"Error processing signal {signal.get('symbol', 'UNKNOWN')}: {e}")
            return False

    def _update_positions_refactored(self):
        """
        Update positions using refactored PositionManager
        
        This method now simply triggers PositionManager to update all positions
        and handles any notifications/logging for the results
        """
        try:
            if not self.position_manager:
                logger.warning("No PositionManager available for position updates")
                return
            
            # Get current active positions count
            summary = self.position_manager.get_positions_summary()
            active_count = summary.get("active_positions", 0)
            
            logger.info(f"üìç Checking {active_count} active positions...")
            
            if active_count == 0:
                logger.debug("No active positions to update")
                return
            
            # Trigger PositionManager to update all positions
            updates = self.position_manager.update_positions()
            
            # Process any position updates for notifications
            notifications_sent = 0
            sheets_logged = 0
            
            for position_id, update_info in updates.items():
                try:
                    # Check if any important events occurred
                    events = []
                    if update_info.get('position_closed'):
                        events.append("Position closed")
                    
                    for tp_level in ['TP1', 'TP2', 'TP3']:
                        if update_info.get(f'{tp_level}_hit', {}).get('hit', False):
                            events.append(f"{tp_level} hit")
                    
                    if update_info.get('sl_hit', {}).get('hit', False):
                        events.append("SL hit")
                    
                    if events:
                        logger.info(f"Position {position_id}: {', '.join(events)}")
                        
                        # Send LINE notification if available
                        if self.line_notifier:
                            try:
                                # Format update for LINE notification
                                position_data = self.position_manager.positions.get(position_id)
                                if position_data:
                                    notification_data = {
                                        "position": position_data,
                                        "updates": update_info,
                                        "events": events
                                    }
                                    self.line_notifier.send_position_update(notification_data)
                                    notifications_sent += 1
                            except Exception as e:
                                logger.warning(f"Failed to send position update notification: {e}")
                        
                        # Log to Google Sheets if available
                        if self.sheets_logger:
                            try:
                                position_data = self.position_manager.positions.get(position_id)
                                if position_data:
                                    self.sheets_logger.log_position_update({
                                        "position": position_data,
                                        "updates": update_info
                                    })
                                    sheets_logged += 1
                            except Exception as e:
                                logger.warning(f"Failed to log position update: {e}")
                                
                except Exception as e:
                    logger.error(f"Error processing update for {position_id}: {e}")
            
            if notifications_sent > 0 or sheets_logged > 0:
                logger.info(f"Position updates: {notifications_sent} LINE notifications, {sheets_logged} sheets logs")
                
        except Exception as e:
            logger.error(f"Error in refactored position update: {e}")

    def _send_daily_summary(self):
        """Send daily summary using refactored services"""
        try:
            logger.info("Generating daily summary v2.0...")
            
            # Get statistics from sheets logger if available
            if self.sheets_logger:
                try:
                    stats = self.sheets_logger.get_trading_statistics(days=1)
                except:
                    stats = {}
            else:
                stats = {}
            
            # Get position summary from PositionManager
            if self.position_manager:
                try:
                    position_summary = self.position_manager.get_positions_summary()
                except:
                    position_summary = {}
            else:
                position_summary = {}
            
            daily_summary = {
                "date": datetime.now().strftime("%Y-%m-%d"),
                "version": "2.0-refactored",
                "total_signals": stats.get("total_trades", 0),
                "active_positions": position_summary.get("active_positions", 0),
                "closed_positions": position_summary.get("closed_positions", 0),
                "total_positions": position_summary.get("total_positions", 0),
                "win_rate_pct": position_summary.get("win_rate_pct", 0),
                "total_pnl_pct": position_summary.get("total_pnl_pct", 0),
                "wins": position_summary.get("wins", 0),
                "losses": position_summary.get("losses", 0),
                "signal_history_count": len(self.last_signals),
                "best_performer": stats.get("best_performer", ""),
                "worst_performer": stats.get("worst_performer", "")
            }
            
            # Send LINE notification
            if self.line_notifier:
                try:
                    self.line_notifier.send_daily_summary(daily_summary)
                    logger.info("Daily summary sent via LINE")
                except Exception as e:
                    logger.warning(f"Failed to send daily summary via LINE: {e}")
            
            # Log to Google Sheets
            if self.sheets_logger:
                try:
                    self.sheets_logger.log_daily_summary(daily_summary)
                    logger.info("Daily summary logged to sheets")
                except Exception as e:
                    logger.warning(f"Failed to log daily summary: {e}")
            
        except Exception as e:
            logger.error(f"Error sending daily summary: {e}")

    def get_enhanced_status(self) -> Dict:
        """Get detailed status including position summary"""
        try:
            scheduler_status = self.get_scheduler_status()
            
            # Add position summary if available
            if self.position_manager:
                try:
                    position_summary = self.position_manager.get_positions_summary()
                    scheduler_status["position_summary"] = position_summary
                except Exception as e:
                    logger.error(f"Error getting position summary: {e}")
            
            # Add signal history details
            scheduler_status["signal_history"] = self.get_signal_history()
            
            return scheduler_status
            
        except Exception as e:
            logger.error(f"Error getting enhanced status: {e}")
            return self.get_scheduler_status()

    def force_scan_now(self, timeframe: str = "1d") -> Dict:
        """Force immediate signal scan"""
        try:
            if timeframe == "1d":
                self._scan_1d_signals()
                return {"status": "1D scan completed", "version": "2.0-refactored"}
            else:
                return {"error": "Invalid timeframe. Use '1d' only"}
        except Exception as e:
            logger.error(f"Error in force scan: {e}")
            return {"error": str(e)}

    def force_update_positions(self) -> Dict:
        """Force immediate position update"""
        try:
            self._update_positions_refactored()
            return {"status": "Position update completed", "version": "2.0-refactored"}
        except Exception as e:
            logger.error(f"Error in force position update: {e}")
            return {"error": str(e)}

    def clear_signal_history(self):
        """Clear all signal history (for testing)"""
        self.last_signals = {}
        self._save_signal_history()
        logger.info("Signal history cleared")

    def get_signal_history(self) -> Dict:
        """Get signal history with timestamps"""
        history = {}
        for key, timestamp in self.last_signals.items():
            history[key] = {
                "timestamp": timestamp.isoformat(),
                "minutes_ago": (datetime.now() - timestamp).total_seconds() / 60
            }
        return history

    def _process_signal(self, signal: Dict, timeframe: str):
        """Legacy method - redirects to refactored version"""
        return self._process_signal_refactored(signal, timeframe)

    def _update_positions(self):
        """Legacy method - redirects to refactored version"""
        self._update_positions_refactored()
========================================
FILE: app/services/indicators.py
========================================

"""Technical indicators for Squeeze Bot trading system."""

import logging
from typing import Dict, Optional, Tuple

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


class TechnicalIndicators:
    """Technical analysis indicators calculator with layer organization."""

    # ================================================================
    # üéØ LAYER 1: Squeeze Momentum Indicator (‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Squeeze ‡πÅ‡∏•‡∏∞ Momentum)
    # ================================================================

    @staticmethod
    def squeeze_momentum(
        df: pd.DataFrame, length: int = 20, mult_bb: float = 2.0, mult_kc: float = 1.5
    ) -> Tuple[bool, str, Dict]:
        """
        Calculate Squeeze Momentum Indicator.

        This indicator identifies periods of low volatility (squeeze) followed by
        directional moves when the squeeze is released.

        Args:
            df: DataFrame with OHLCV data
            length: Period for calculation (default 20)
            mult_bb: Bollinger Bands multiplier (default 2.0)
            mult_kc: Keltner Channel multiplier (default 1.5)

        Returns:
            Tuple[bool, str, Dict]: (squeeze_off, momentum_direction, details)
        """
        try:
            close = df["close"]
            high = df["high"]
            low = df["low"]

            # Bollinger Bands calculation
            basis = close.rolling(length).mean()
            dev = mult_bb * close.rolling(length).std()
            upper_bb = basis + dev
            lower_bb = basis - dev

            # Keltner Channel calculation (using True Range)
            high_low = high - low
            high_close = np.abs(high - close.shift())
            low_close = np.abs(low - close.shift())
            true_range = np.maximum(high_low, np.maximum(high_close, low_close))
            rangema = true_range.rolling(length).mean()

            # Moving average for Keltner Channel
            ma = close.rolling(length).mean()
            upper_kc = ma + rangema * mult_kc
            lower_kc = ma - rangema * mult_kc

            # Squeeze detection: BB inside KC = squeeze ON, BB outside KC = squeeze OFF
            squeeze_off = (lower_bb.iloc[-1] < lower_kc.iloc[-1]) and (
                upper_bb.iloc[-1] > upper_kc.iloc[-1]
            )

            # Momentum calculation using linear regression
            highest = high.rolling(length).max()
            lowest = low.rolling(length).min()
            mid_point = (highest + lowest) / 2

            momentum_direction = "NEUTRAL"
            momentum_value = 0

            if len(close) >= length:
                # Linear regression of (close - midpoint) for momentum
                x = np.arange(length)
                y = (close - mid_point).iloc[-length:].values

                if len(y) == length and not np.isnan(y).any():
                    # Current momentum slope
                    slope = np.polyfit(x, y, 1)[0]
                    momentum_value = slope

                    # Previous momentum slope for comparison
                    if len(close) > length:
                        prev_y = (close - mid_point).iloc[-length - 1 : -1].values
                        if len(prev_y) == length and not np.isnan(prev_y).any():
                            prev_slope = np.polyfit(x, prev_y, 1)[0]

                            # Determine momentum direction
                            if slope > prev_slope:
                                momentum_direction = "UP"
                            elif slope < prev_slope:
                                momentum_direction = "DOWN"
                            else:
                                momentum_direction = "NEUTRAL"

            # Additional details for analysis
            details = {
                "bb_upper": float(upper_bb.iloc[-1]),
                "bb_lower": float(lower_bb.iloc[-1]),
                "kc_upper": float(upper_kc.iloc[-1]),
                "kc_lower": float(lower_kc.iloc[-1]),
                "momentum_value": float(momentum_value),
                "squeeze_intensity": float(upper_bb.iloc[-1] - lower_bb.iloc[-1])
                / float(upper_kc.iloc[-1] - lower_kc.iloc[-1]),
            }

            # Ensure boolean is Python native type
            squeeze_off = bool(squeeze_off)

            logger.debug(
                f"Squeeze analysis: OFF={squeeze_off}, Direction={momentum_direction}"
            )
            return squeeze_off, momentum_direction, details

        except Exception as e:
            logger.error(f"Error calculating squeeze momentum: {e}")
            return False, "NEUTRAL", {}

    # ================================================================
    # üìà LAYER 2: MACD Uncle Cholok (MACD ‡∏•‡∏∏‡∏á‡πÇ‡∏â‡∏•‡∏Å 8,17,9)
    # ================================================================

    @staticmethod
    def macd_uncle_cholok(
        df: pd.DataFrame, fast: int = 8, slow: int = 17, signal: int = 9
    ) -> Tuple[float, float, str, Dict]:
        """
        MACD ‡∏•‡∏∏‡∏á‡πÇ‡∏â‡∏•‡∏Å (Uncle Cholok) calculation with custom periods (8,17,9).
        ‚úÖ UPDATED: More flexible cross detection
        """
        try:
            close = df["close"]

            # Calculate MACD components
            ema_fast = close.ewm(span=fast, adjust=False).mean()
            ema_slow = close.ewm(span=slow, adjust=False).mean()
            macd_line = ema_fast - ema_slow
            signal_line = macd_line.ewm(span=signal, adjust=False).mean()
            histogram = macd_line - signal_line

            # Get current values
            current_macd = macd_line.iloc[-1]
            current_signal = signal_line.iloc[-1]
            current_histogram = histogram.iloc[-1]

            # ‚úÖ UPDATED: Cross detection with momentum
            cross_direction = "NONE"
            if len(macd_line) > 1:
                prev_macd = macd_line.iloc[-2]
                prev_signal = signal_line.iloc[-2]

                # Bullish signal: MACD crosses above Signal OR trending up
                if current_macd > current_signal:
                    if prev_macd <= prev_signal:
                        # Traditional cross
                        cross_direction = "UP"
                        logger.debug("MACD: Traditional bullish cross detected")
                    elif current_macd > prev_macd:
                        # MACD above signal and increasing
                        cross_direction = "UP"
                        logger.debug("MACD: Bullish momentum detected (above signal + rising)")
            
                # Bearish signal: MACD crosses below Signal OR trending down
                elif current_macd < current_signal:
                    if prev_macd >= prev_signal:
                        # Traditional cross
                        cross_direction = "DOWN"
                        logger.debug("MACD: Traditional bearish cross detected")
                    elif current_macd < prev_macd:
                        # MACD below signal and decreasing
                        cross_direction = "DOWN"
                        logger.debug("MACD: Bearish momentum detected (below signal + falling)")

            # Additional details
            details = {
                "ema_fast": float(ema_fast.iloc[-1]),
                "ema_slow": float(ema_slow.iloc[-1]),
                "histogram": float(current_histogram),
                "macd_above_zero": bool(current_macd > 0),
                "signal_above_zero": bool(current_signal > 0),
                "divergence_strength": float(abs(current_macd - current_signal)),
            }

            logger.debug(
                f"MACD Uncle Cholok: {current_macd:.6f}, Signal: {current_signal:.6f}, Cross: {cross_direction}"
            )
            return float(current_macd), float(current_signal), cross_direction, details

        except Exception as e:
            logger.error(f"Error calculating MACD Uncle Cholok: {e}")
            return 0.0, 0.0, "NONE", {}

    # ================================================================
    # üìä LAYER 3: RSI Extreme (RSI ‡πÇ‡∏ï‡πà‡∏á) - ‡∏õ‡∏£‡∏±‡∏ö default threshold 40/60
    # ================================================================

    @staticmethod
    def rsi_extreme(
        df: pd.DataFrame,
        period: int = 14,
        low_threshold: float = 35,   # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å 40 ‡πÄ‡∏õ‡πá‡∏ô 35 (‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≤‡∏¢‡∏´‡∏ô‡∏±‡∏Å‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ô‡πà‡∏≤‡∏™‡∏∞‡∏™‡∏°)
        high_threshold: float = 65,  # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å 60 ‡πÄ‡∏õ‡πá‡∏ô 65 (‡∏ï‡πâ‡∏≠‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏´‡∏ô‡∏±‡∏Å‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ô‡πà‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á)
    ) -> Tuple[float, str, Dict]:
        """
        RSI ‡πÇ‡∏ï‡πà‡∏á (Extreme RSI) calculation for identifying overbought/oversold conditions.
        ‡∏õ‡∏£‡∏±‡∏ö default threshold ‡πÄ‡∏õ‡πá‡∏ô 40/60 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô

        Args:
            df: DataFrame with OHLCV data
            period: RSI calculation period (default 14)
            low_threshold: Oversold threshold (default 40 - ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å 30)
            high_threshold: Overbought threshold (default 60 - ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å 70)

        Returns:
            Tuple[float, str, Dict]: (rsi_value, extreme_level, details)
        """
        try:
            close = df["close"]

            # Calculate price changes
            delta = close.diff()

            # Separate gains and losses
            gain = (
                (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()
            )
            loss = (
                (-delta.where(delta < 0, 0))
                .rolling(window=period, min_periods=1)
                .mean()
            )

            # Calculate Relative Strength and RSI
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))

            current_rsi = rsi.iloc[-1]

            # Determine extreme level - ‡πÉ‡∏ä‡πâ threshold ‡πÉ‡∏´‡∏°‡πà
            if current_rsi < low_threshold:
                extreme_level = "LOW"
            elif current_rsi > high_threshold:
                extreme_level = "HIGH"
            else:
                extreme_level = "NORMAL"

            # Calculate RSI tren
            rsi_trend = "NEUTRAL"
            if len(rsi) >= 3:
                recent_rsi = rsi.iloc[-3:]
                if recent_rsi.iloc[-1] > recent_rsi.iloc[-2] > recent_rsi.iloc[-3]:
                    rsi_trend = "RISING"
                elif recent_rsi.iloc[-1] < recent_rsi.iloc[-2] < recent_rsi.iloc[-3]:
                    rsi_trend = "FALLING"

            # Additional details
            details = {
                "rsi_14": float(current_rsi),
                "rsi_trend": rsi_trend,
                "distance_to_oversold": float(current_rsi - low_threshold),
                "distance_to_overbought": float(high_threshold - current_rsi),
                "is_diverging": False,  # TODO: Implement divergence detection
                "threshold_low": float(low_threshold),   # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
                "threshold_high": float(high_threshold), # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠ debug
            }

            logger.debug(
                f"RSI Extreme: {current_rsi:.2f}, Level: {extreme_level}, Trend: {rsi_trend}, Thresholds: {low_threshold}/{high_threshold}"
            )
            return float(current_rsi), extreme_level, details

        except Exception as e:
            logger.error(f"Error calculating RSI Extreme: {e}")
            return 50.0, "NORMAL", {}

    # ================================================================
    # üîç LAYER 4: Comprehensive Analysis (‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏ß‡∏°) - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç default values
    # ================================================================

    @staticmethod
    def analyze_all_indicators(df: pd.DataFrame, config: Dict) -> Dict:
        """
        Calculate all indicators and return comprehensive analysis.
        ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ RSI threshold ‡πÉ‡∏´‡∏°‡πà (40/60) ‡πÄ‡∏õ‡πá‡∏ô default

        Args:
            df: DataFrame with OHLCV data
            config: Configuration dictionary with indicator settings

        Returns:
            Dict with all indicator results
        """
        try:
            # Get indicator settings from config
            squeeze_config = config.get("squeeze", {})
            macd_config = config.get("macd", {})
            rsi_config = config.get("rsi", {})

            # Calculate Squeeze Momentum
            squeeze_off, momentum_direction, squeeze_details = (
                TechnicalIndicators.squeeze_momentum(
                    df,
                    length=squeeze_config.get("length", 20),
                    mult_bb=squeeze_config.get("bb_mult", 2.0),
                    mult_kc=squeeze_config.get("kc_mult", 1.5),
                )
            )

            # Calculate MACD Uncle Cholok
            macd_line, signal_line, macd_cross, macd_details = (
                TechnicalIndicators.macd_uncle_cholok(
                    df,
                    fast=macd_config.get("fast", 8),
                    slow=macd_config.get("slow", 17),
                    signal=macd_config.get("signal", 9),
                )
            )

            # Calculate RSI Extreme - ‚≠ê ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç default values ‡πÄ‡∏õ‡πá‡∏ô 40/60
            rsi_value, rsi_extreme, rsi_details = TechnicalIndicators.rsi_extreme(
                df,
                period=rsi_config.get("period", 14),
                low_threshold=rsi_config.get("oversold", 40),   # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 30 ‡πÄ‡∏õ‡πá‡∏ô 40
                high_threshold=rsi_config.get("overbought", 60), # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å 70 ‡πÄ‡∏õ‡πá‡∏ô 60
            )

            # Compile comprehensive analysis
            analysis = {
                "timestamp": pd.Timestamp.now().isoformat(),
                "current_price": float(df["close"].iloc[-1]),
                "squeeze": {
                    "squeeze_off": squeeze_off,
                    "momentum_direction": momentum_direction,
                    "details": squeeze_details,
                },
                "macd": {
                    "macd_line": macd_line,
                    "signal_line": signal_line,
                    "cross_direction": macd_cross,
                    "details": macd_details,
                },
                "rsi": {
                    "value": rsi_value,
                    "extreme_level": rsi_extreme,
                    "details": rsi_details,
                },
            }

            logger.info("Comprehensive indicator analysis completed")
            return analysis

        except Exception as e:
            logger.error(f"Error in comprehensive analysis: {e}")
            return {}

    # ================================================================
    # üõ†Ô∏è LAYER 5: Helper Functions (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠)
    # ================================================================

    @staticmethod
    def validate_dataframe(df: pd.DataFrame) -> bool:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á DataFrame
        Validate DataFrame for required columns and data quality.
        """
        required_columns = ["open", "high", "low", "close", "volume"]
        
        if not all(col in df.columns for col in required_columns):
            logger.error(f"Missing required columns. Required: {required_columns}")
            return False
            
        if df.empty:
            logger.error("DataFrame is empty")
            return False
            
        if df.isnull().any().any():
            logger.warning("DataFrame contains null values")
            
        return True

    @staticmethod
    def get_indicator_summary(analysis: Dict) -> Dict:
        """
        ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ indicators ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        Get indicator summary for display purposes.
        """
        try:
            summary = {
                "timestamp": analysis.get("timestamp"),
                "price": analysis.get("current_price"),
                "squeeze_status": "OFF" if analysis.get("squeeze", {}).get("squeeze_off") else "ON",
                "momentum": analysis.get("squeeze", {}).get("momentum_direction", "NEUTRAL"),
                "macd_cross": analysis.get("macd", {}).get("cross_direction", "NONE"),
                "rsi_value": round(analysis.get("rsi", {}).get("value", 50), 2),
                "rsi_level": analysis.get("rsi", {}).get("extreme_level", "NORMAL"),
                "signals_present": {
                    "squeeze_breakout": analysis.get("squeeze", {}).get("squeeze_off", False),
                    "macd_signal": analysis.get("macd", {}).get("cross_direction") != "NONE",
                    "rsi_extreme": analysis.get("rsi", {}).get("extreme_level") != "NORMAL"
                }
            }
            return summary
        except Exception as e:
            logger.error(f"Error creating indicator summary: {e}")
            return {}

    @staticmethod
    def calculate_signal_confluence(analysis: Dict) -> Dict:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏à‡∏≤‡∏Å indicators ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
        Calculate signal confluence between multiple indicators.
        """
        try:
            squeeze = analysis.get("squeeze", {})
            macd = analysis.get("macd", {})
            rsi = analysis.get("rsi", {})
            
            # Count bullish signals
            bullish_signals = 0
            bearish_signals = 0
            
            # Squeeze momentum
            if squeeze.get("momentum_direction") == "UP":
                bullish_signals += 1
            elif squeeze.get("momentum_direction") == "DOWN":
                bearish_signals += 1
                
            # MACD cross
            if macd.get("cross_direction") == "UP":
                bullish_signals += 1
            elif macd.get("cross_direction") == "DOWN":
                bearish_signals += 1
                
            # RSI extreme
            if rsi.get("extreme_level") == "LOW":
                bullish_signals += 1
            elif rsi.get("extreme_level") == "HIGH":
                bearish_signals += 1
                
            # Squeeze breakout (neutral but important)
            squeeze_breakout = squeeze.get("squeeze_off", False)
            
            confluence = {
                "bullish_count": bullish_signals,
                "bearish_count": bearish_signals,
                "total_signals": bullish_signals + bearish_signals,
                "squeeze_breakout": squeeze_breakout,
                "confluence_strength": "STRONG" if (bullish_signals >= 3 or bearish_signals >= 3) else
                                     "MEDIUM" if (bullish_signals >= 2 or bearish_signals >= 2) else
                                     "WEAK",
                "direction": "BULLISH" if bullish_signals > bearish_signals else
                           "BEARISH" if bearish_signals > bullish_signals else
                           "NEUTRAL"
            }
            
            return confluence
            
        except Exception as e:
            logger.error(f"Error calculating signal confluence: {e}")
            return {}

    # ================================================================
    # üìã LAYER 6: Debug ‡πÅ‡∏•‡∏∞ Monitoring Functions
    # ================================================================

    @staticmethod
    def get_indicator_health(analysis: Dict) -> Dict:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì indicators
        Check the health of indicator calculations.
        """
        try:
            health = {
                "overall_status": "HEALTHY",
                "issues": [],
                "warnings": []
            }
            
            # Check squeeze calculation
            squeeze = analysis.get("squeeze", {})
            if not squeeze.get("details"):
                health["issues"].append("Squeeze calculation incomplete")
                health["overall_status"] = "ERROR"
                
            # Check MACD calculation  
            macd = analysis.get("macd", {})
            if macd.get("macd_line") == 0 and macd.get("signal_line") == 0:
                health["warnings"].append("MACD values are zero - check data quality")
                
            # Check RSI calculation
            rsi = analysis.get("rsi", {})
            rsi_value = rsi.get("value", 50)
            if rsi_value < 0 or rsi_value > 100:
                health["issues"].append(f"RSI value out of range: {rsi_value}")
                health["overall_status"] = "ERROR"
                
            # Check data freshness
            timestamp = analysis.get("timestamp")
            if timestamp:
                from datetime import datetime, timedelta
                analysis_time = pd.Timestamp(timestamp)
                if pd.Timestamp.now() - analysis_time > timedelta(minutes=10):
                    health["warnings"].append("Analysis data is more than 10 minutes old")
                    
            return health
            
        except Exception as e:
            logger.error(f"Error checking indicator health: {e}")
            return {"overall_status": "ERROR", "issues": [str(e)], "warnings": []}
========================================
FILE: app/services/signal_history_manager.py
========================================

"""Signal History Manager - Persistent storage for 1D signals"""

import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional

logger = logging.getLogger(__name__)


class SignalHistoryManager:
    """Manage 1D signal history with persistent JSON storage"""
    
    def __init__(self, data_dir: str = "/data"):
        """Initialize signal history manager
        
        Args:
            data_dir: Directory for storing signal history (default: /data for Railway Volume)
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.history_file = self.data_dir / "signal_history_1d.json"
        self.signal_history = self._load_history()
        
        logger.info(f"‚úÖ SignalHistoryManager initialized (File: {self.history_file})")
    
    def _load_history(self) -> Dict:
        """Load signal history from JSON file"""
        try:
            if self.history_file.exists():
                with open(self.history_file, 'r') as f:
                    data = json.load(f)
                    logger.info(f"üìÇ Loaded {len(data)} signal histories")
                    return data
            else:
                logger.info("üìÇ No existing history file, starting fresh")
                return {}
        except Exception as e:
            logger.error(f"Error loading history: {e}")
            return {}
    
    def _save_history(self):
        """Save signal history to JSON file"""
        try:
            with open(self.history_file, 'w') as f:
                json.dump(self.signal_history, f, indent=2)
            logger.debug(f"üíæ Saved signal history ({len(self.signal_history)} entries)")
        except Exception as e:
            logger.error(f"Error saving history: {e}")
    
    def should_notify(self, symbol: str, timeframe: str, signal_type: str, current_price: float) -> bool:
        """Check if should notify for this signal
        
        Args:
            symbol: Trading symbol (e.g., BTCUSDT)
            timeframe: Timeframe (e.g., 1d)
            signal_type: Signal type (LONG or SHORT)
            current_price: Current price
            
        Returns:
            True if should notify, False if already notified
        """
        key = f"{symbol}_{timeframe}_{signal_type}"
        
        # Check if exists
        if key not in self.signal_history:
            return True  # New signal, should notify
        
        last_signal = self.signal_history[key]
        
        # Check if same signal type
        if last_signal.get("signal_type") != signal_type:
            return True  # Signal type changed, should notify
        
        # Already notified for same signal
        logger.debug(f"‚è≠Ô∏è Skip: {key} already notified on {last_signal.get('date')}")
        return False
    
    def record_signal(self, symbol: str, timeframe: str, signal_type: str, current_price: float):
        """Record that signal was notified
        
        Args:
            symbol: Trading symbol
            timeframe: Timeframe
            signal_type: Signal type (LONG or SHORT)
            current_price: Current price
        """
        key = f"{symbol}_{timeframe}_{signal_type}"
        
        self.signal_history[key] = {
            "symbol": symbol,
            "timeframe": timeframe,
            "signal_type": signal_type,
            "price": current_price,
            "date": datetime.now().isoformat(),
            "notified": True
        }
        
        self._save_history()
        logger.info(f"üìù Recorded: {key} @ {current_price}")
    
    def clear_opposite_signal(self, symbol: str, timeframe: str, signal_type: str):
        """Clear opposite signal when trend changes
        
        Args:
            symbol: Trading symbol
            timeframe: Timeframe
            signal_type: Current signal type (will clear opposite)
        """
        opposite = "SHORT" if signal_type == "LONG" else "LONG"
        key = f"{symbol}_{timeframe}_{opposite}"
        
        if key in self.signal_history:
            del self.signal_history[key]
            self._save_history()
            logger.info(f"üóëÔ∏è Cleared opposite signal: {key}")
    
    def get_history(self, symbol: Optional[str] = None) -> Dict:
        """Get signal history
        
        Args:
            symbol: Optional symbol filter
            
        Returns:
            Dictionary of signal history
        """
        if symbol:
            return {k: v for k, v in self.signal_history.items() if v['symbol'] == symbol}
        return self.signal_history
    
    def clear_history(self):
        """Clear all signal history (for testing)"""
        self.signal_history = {}
        self._save_history()
        logger.info("üóëÔ∏è Cleared all signal history")
    
    def get_stats(self) -> Dict:
        """Get statistics about signal history"""
        total = len(self.signal_history)
        long_count = sum(1 for v in self.signal_history.values() if v['signal_type'] == 'LONG')
        short_count = sum(1 for v in self.signal_history.values() if v['signal_type'] == 'SHORT')
        
        return {
            "total_signals": total,
            "long_signals": long_count,
            "short_signals": short_count,
            "file_path": str(self.history_file),
            "file_exists": self.history_file.exists()
        }
========================================
FILE: app/services/performance_analyzer.py
========================================

"""
=============================================================================
üìä PERFORMANCE ANALYZER ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö TRADING ANALYTICS
=============================================================================
‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£:
1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î‡∏à‡∏≤‡∏Å Google Sheets
2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡πà‡∏≤‡∏á‡πÜ (Win Rate, PnL, Drawdown)
3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
4. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå pattern ‡πÅ‡∏•‡∏∞ performance ‡∏ï‡∏≤‡∏° timeframe/symbol
=============================================================================
"""

import logging
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import statistics

logger = logging.getLogger(__name__)


class PerformanceAnalyzer:
    """
    =======================================================================
    üìà PERFORMANCE ANALYZER CLASS - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î
    =======================================================================
    
    ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà:
    - ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î‡∏à‡∏≤‡∏Å Google Sheets
    - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞ metrics ‡∏ï‡πà‡∏≤‡∏á‡πÜ
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
    - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå patterns ‡πÅ‡∏•‡∏∞ trends
    """

    def __init__(self, config: Dict, sheets_logger=None):
        """
        ===================================================================
        üöÄ INITIALIZATION LAYER
        ===================================================================
        """
        self.config = config
        self.sheets_logger = sheets_logger
        
        # üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å sheets
        self.trading_data = []
        self.signal_data = []
        
        # üèÜ Performance metrics
        self.performance_cache = {}
        self.last_analysis_time = None
        
        logger.info("PerformanceAnalyzer initialized")

    def load_trading_data(self, days: int = 30) -> bool:
        """
        ===================================================================
        üìã DATA LOADING LAYER - ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Google Sheets
        ===================================================================
        
        Args:
            days: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            
        Returns:
            True ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        """
        if not self.sheets_logger or not self.sheets_logger._initialized:
            logger.error("SheetsLogger not available")
            return False
            
        try:
            # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Trading_Journal
            worksheet = self.sheets_logger.spreadsheet.worksheet("Trading_Journal")
            records = worksheet.get_all_records()
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
            cutoff_date = datetime.now() - timedelta(days=days)
            filtered_data = []
            
            for record in records:
                try:
                    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
                    date_str = record.get("Date", "")
                    if date_str:
                        trade_date = datetime.strptime(date_str, "%Y-%m-%d")
                        if trade_date >= cutoff_date:
                            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                            clean_record = self._clean_trading_record(record)
                            if clean_record:
                                filtered_data.append(clean_record)
                                
                except (ValueError, TypeError) as e:
                    logger.warning(f"Invalid date in record: {record}, error: {e}")
                    continue
            
            self.trading_data = filtered_data
            logger.info(f"Loaded {len(self.trading_data)} trading records")
            
            # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• signals ‡∏î‡πâ‡∏ß‡∏¢
            self._load_signal_data(days)
            
            return True
            
        except Exception as e:
            logger.error(f"Error loading trading data: {e}")
            return False

    def _clean_trading_record(self, record: Dict) -> Optional[Dict]:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• trading record"""
        try:
            clean_record = {
                "date": record.get("Date", ""),
                "symbol": record.get("Symbol", ""),
                "direction": record.get("Signal", ""),
                "entry_price": float(record.get("Entry", 0)),
                "sl": float(record.get("SL", 0)),
                "tp1": float(record.get("TP1", 0)),
                "tp2": float(record.get("TP2", 0)),
                "tp3": float(record.get("TP3", 0)),
                "win_loss": record.get("Win/Loss", ""),
                "win_rate": record.get("Win Rate", "")
            }
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            if (clean_record["symbol"] and 
                clean_record["direction"] and 
                clean_record["entry_price"] > 0):
                return clean_record
                
            return None
            
        except (ValueError, TypeError) as e:
            logger.warning(f"Error cleaning record: {e}")
            return None

    def _load_signal_data(self, days: int):
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• signals ‡∏à‡∏≤‡∏Å Signals worksheet"""
        try:
            worksheet = self.sheets_logger.spreadsheet.worksheet("Signals")
            records = worksheet.get_all_records()
            
            cutoff_date = datetime.now() - timedelta(days=days)
            self.signal_data = []
            
            for record in records:
                try:
                    timestamp_str = record.get("Timestamp", "")
                    if timestamp_str:
                        # ‡πÅ‡∏õ‡∏•‡∏á ISO timestamp
                        signal_date = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                        if signal_date.replace(tzinfo=None) >= cutoff_date:
                            self.signal_data.append(record)
                            
                except (ValueError, TypeError):
                    continue
                    
            logger.info(f"Loaded {len(self.signal_data)} signal records")
            
        except Exception as e:
            logger.warning(f"Could not load signal data: {e}")
            self.signal_data = []

    def calculate_basic_metrics(self) -> Dict:
        """
        ===================================================================
        üìä BASIC METRICS LAYER - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        ===================================================================
        """
        if not self.trading_data:
            return {"error": "No trading data available"}
        
        try:
            total_trades = len(self.trading_data)
            closed_trades = [t for t in self.trading_data if t["win_loss"] in ["WIN", "LOSS"]]
            wins = [t for t in closed_trades if t["win_loss"] == "WIN"]
            losses = [t for t in closed_trades if t["win_loss"] == "LOSS"]
            
            metrics = {
                "total_trades": total_trades,
                "closed_trades": len(closed_trades),
                "open_trades": total_trades - len(closed_trades),
                "wins": len(wins),
                "losses": len(losses),
                "win_rate": round((len(wins) / max(len(closed_trades), 1)) * 100, 1),
                "loss_rate": round((len(losses) / max(len(closed_trades), 1)) * 100, 1)
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"Error calculating basic metrics: {e}")
            return {"error": str(e)}

    def calculate_pnl_metrics(self) -> Dict:
        """
        ===================================================================
        üí∞ PNL METRICS LAYER - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Å‡∏≥‡πÑ‡∏£‡∏Ç‡∏≤‡∏î‡∏ó‡∏∏‡∏ô
        ===================================================================
        """
        if not self.trading_data:
            return {"error": "No trading data available"}
            
        try:
            closed_trades = [t for t in self.trading_data if t["win_loss"] in ["WIN", "LOSS"]]
            
            if not closed_trades:
                return {"message": "No closed trades for PnL calculation"}
            
            pnl_data = []
            
            for trade in closed_trades:
                try:
                    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì PnL ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á TP/SL
                    entry = trade["entry_price"]
                    sl = trade["sl"]
                    tp1 = trade["tp1"]
                    
                    if trade["win_loss"] == "WIN":
                        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏ñ‡∏∂‡∏á TP1
                        if trade["direction"] == "LONG":
                            pnl_percent = ((tp1 - entry) / entry) * 100
                        else:  # SHORT
                            pnl_percent = ((entry - tp1) / entry) * 100
                    else:  # LOSS
                        # ‡∏ñ‡∏∂‡∏á SL
                        if trade["direction"] == "LONG":
                            pnl_percent = ((sl - entry) / entry) * 100
                        else:  # SHORT
                            pnl_percent = ((entry - sl) / entry) * 100
                    
                    pnl_data.append(pnl_percent)
                    
                except (ValueError, ZeroDivisionError):
                    continue
            
            if not pnl_data:
                return {"message": "Could not calculate PnL data"}
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
            total_pnl = sum(pnl_data)
            avg_win = statistics.mean([p for p in pnl_data if p > 0]) if any(p > 0 for p in pnl_data) else 0
            avg_loss = statistics.mean([p for p in pnl_data if p < 0]) if any(p < 0 for p in pnl_data) else 0
            
            metrics = {
                "total_pnl_percent": round(total_pnl, 2),
                "average_pnl_percent": round(statistics.mean(pnl_data), 2),
                "median_pnl_percent": round(statistics.median(pnl_data), 2),
                "best_trade_percent": round(max(pnl_data), 2),
                "worst_trade_percent": round(min(pnl_data), 2),
                "average_win_percent": round(avg_win, 2),
                "average_loss_percent": round(avg_loss, 2),
                "profit_factor": round(abs(avg_win / avg_loss), 2) if avg_loss != 0 else 0,
                "total_trades_analyzed": len(pnl_data)
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"Error calculating PnL metrics: {e}")
            return {"error": str(e)}

    def analyze_by_direction(self) -> Dict:
        """
        ===================================================================
        üß≠ DIRECTION ANALYSIS LAYER - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° LONG/SHORT
        ===================================================================
        """
        if not self.trading_data:
            return {"error": "No trading data available"}
            
        try:
            directions = {}
            
            for direction in ["LONG", "SHORT"]:
                trades = [t for t in self.trading_data if t["direction"] == direction]
                closed_trades = [t for t in trades if t["win_loss"] in ["WIN", "LOSS"]]
                wins = len([t for t in closed_trades if t["win_loss"] == "WIN"])
                
                directions[direction.lower()] = {
                    "total_trades": len(trades),
                    "closed_trades": len(closed_trades),
                    "wins": wins,
                    "losses": len(closed_trades) - wins,
                    "win_rate": round((wins / max(len(closed_trades), 1)) * 100, 1)
                }
            
            return directions
            
        except Exception as e:
            logger.error(f"Error analyzing by direction: {e}")
            return {"error": str(e)}

    def analyze_by_symbol(self, top_n: int = 10) -> Dict:
        """
        ===================================================================
        üè∑Ô∏è SYMBOL ANALYSIS LAYER - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏° Symbol
        ===================================================================
        """
        if not self.trading_data:
            return {"error": "No trading data available"}
            
        try:
            symbols = {}
            
            # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° symbol
            for trade in self.trading_data:
                symbol = trade["symbol"]
                if symbol not in symbols:
                    symbols[symbol] = {
                        "total_trades": 0,
                        "closed_trades": 0,
                        "wins": 0,
                        "losses": 0,
                        "win_rate": 0
                    }
                
                symbols[symbol]["total_trades"] += 1
                
                if trade["win_loss"] in ["WIN", "LOSS"]:
                    symbols[symbol]["closed_trades"] += 1
                    if trade["win_loss"] == "WIN":
                        symbols[symbol]["wins"] += 1
                    else:
                        symbols[symbol]["losses"] += 1
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì win rate
            for symbol in symbols:
                closed = symbols[symbol]["closed_trades"]
                if closed > 0:
                    symbols[symbol]["win_rate"] = round(
                        (symbols[symbol]["wins"] / closed) * 100, 1
                    )
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° total trades ‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà top N
            sorted_symbols = sorted(
                symbols.items(), 
                key=lambda x: x[1]["total_trades"], 
                reverse=True
            )[:top_n]
            
            return dict(sorted_symbols)
            
        except Exception as e:
            logger.error(f"Error analyzing by symbol: {e}")
            return {"error": str(e)}

    def analyze_signal_quality(self) -> Dict:
        """
        ===================================================================
        üéØ SIGNAL QUALITY LAYER - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
        ===================================================================
        """
        if not self.signal_data:
            return {"message": "No signal data available"}
            
        try:
            total_signals = len(self.signal_data)
            
            # ‡∏ô‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
            signal_types = {}
            timeframes = {}
            
            for signal in self.signal_data:
                # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
                signal_type = signal.get("Signal", "UNKNOWN")
                signal_types[signal_type] = signal_types.get(signal_type, 0) + 1
                
                # Timeframe
                timeframe = signal.get("Timeframe", "UNKNOWN")
                timeframes[timeframe] = timeframes.get(timeframe, 0) + 1
            
            return {
                "total_signals": total_signals,
                "signal_types": signal_types,
                "timeframes": timeframes,
                "signals_per_day": round(total_signals / max(30, 1), 1)  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ 30 ‡∏ß‡∏±‡∏ô
            }
            
        except Exception as e:
            logger.error(f"Error analyzing signal quality: {e}")
            return {"error": str(e)}

    def generate_performance_report(self, days: int = 30) -> Dict:
        """
        ===================================================================
        üìã REPORT GENERATION LAYER - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
        ===================================================================
        """
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
        if not self.load_trading_data(days):
            return {"error": "Could not load trading data"}
        
        try:
            report = {
                "report_date": datetime.now().isoformat(),
                "analysis_period_days": days,
                "basic_metrics": self.calculate_basic_metrics(),
                "pnl_metrics": self.calculate_pnl_metrics(),
                "direction_analysis": self.analyze_by_direction(),
                "symbol_analysis": self.analyze_by_symbol(top_n=10),
                "signal_analysis": self.analyze_signal_quality()
            }
            
            # Cache ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            self.performance_cache = report
            self.last_analysis_time = datetime.now()
            
            logger.info(f"Generated performance report for {days} days")
            return report
            
        except Exception as e:
            logger.error(f"Error generating performance report: {e}")
            return {"error": str(e)}

    def get_summary_stats(self) -> Dict:
        """
        ===================================================================
        üìä SUMMARY STATS LAYER - ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠
        ===================================================================
        """
        if not self.trading_data:
            if not self.load_trading_data(7):  # ‡πÇ‡∏´‡∏•‡∏î 7 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                return {"error": "No data available"}
        
        try:
            basic = self.calculate_basic_metrics()
            
            return {
                "total_trades": basic.get("total_trades", 0),
                "win_rate": basic.get("win_rate", 0),
                "active_positions": basic.get("open_trades", 0),
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            logger.error(f"Error getting summary stats: {e}")
            return {"error": str(e)}

    def compare_timeframes(self) -> Dict:
        """
        ===================================================================
        ‚è∞ TIMEFRAME COMPARISON LAYER - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance ‡∏ï‡∏≤‡∏° timeframe
        ===================================================================
        """
        if not self.signal_data:
            return {"message": "No signal data for timeframe analysis"}
            
        try:
            timeframe_performance = {}
            
            # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà signals ‡∏Å‡∏±‡∏ö trades (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ)
            for signal in self.signal_data:
                timeframe = signal.get("Timeframe", "UNKNOWN")
                symbol = signal.get("Symbol", "")
                
                if timeframe not in timeframe_performance:
                    timeframe_performance[timeframe] = {
                        "total_signals": 0,
                        "signal_strength": []
                    }
                
                timeframe_performance[timeframe]["total_signals"] += 1
                
                # ‡πÄ‡∏Å‡πá‡∏ö signal strength ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
                strength = signal.get("Signal_Strength", 0)
                if strength:
                    timeframe_performance[timeframe]["signal_strength"].append(float(strength))
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ signal strength
            for tf in timeframe_performance:
                strengths = timeframe_performance[tf]["signal_strength"]
                if strengths:
                    timeframe_performance[tf]["avg_signal_strength"] = round(
                        statistics.mean(strengths), 1
                    )
                else:
                    timeframe_performance[tf]["avg_signal_strength"] = 0
            
            return timeframe_performance
            
        except Exception as e:
            logger.error(f"Error comparing timeframes: {e}")
            return {"error": str(e)}

    def get_recent_performance(self, days: int = 7) -> Dict:
        """‡∏î‡∏π‡∏ú‡∏•‡∏á‡∏≤‡∏ô N ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        return self.generate_performance_report(days)

    def export_data_for_analysis(self) -> Dict:
        """
        ===================================================================
        üì§ DATA EXPORT LAYER - Export ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        ===================================================================
        """
        try:
            export_data = {
                "trading_data": self.trading_data,
                "signal_data": self.signal_data,
                "export_timestamp": datetime.now().isoformat(),
                "data_summary": {
                    "total_trades": len(self.trading_data),
                    "total_signals": len(self.signal_data),
                    "date_range": {
                        "oldest_trade": min([t["date"] for t in self.trading_data]) if self.trading_data else None,
                        "newest_trade": max([t["date"] for t in self.trading_data]) if self.trading_data else None
                    }
                }
            }
            
            return export_data
            
        except Exception as e:
            logger.error(f"Error exporting data: {e}")
            return {"error": str(e)}
========================================
FILE: app/services/config_manager.py
========================================

import os
import logging
from typing import Dict, Any
from ..utils.core_utils import ConfigValidator


class ConfigManager:
    """Centralized configuration management"""
    
    _instance = None
    _config = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._config is None:
            self._load_config()
    
    def _load_config(self):
        """Load and validate all configuration"""
        # Required environment variables
        required_env_vars = [
            # 'GOOGLE_SHEETS_ID',  <-- üéØ ‡πÉ‡∏™‡πà # ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Å)
            'LINE_CHANNEL_ACCESS_TOKEN',
            'LINE_CHANNEL_SECRET',
            'LINE_USER_ID'
        ]
        
        try:
            self._config = ConfigValidator.validate_required_env_vars(required_env_vars)
            
            # Add optional configs with defaults
            self._config.update({
                'DEBUG': os.getenv('DEBUG', 'false').lower() == 'true',
                'PORT': int(os.getenv('PORT', '8080')),
                'BINANCE_BASE_URL': 'https://api.binance.com/api/v3',
                'VERSION': os.getenv('VERSION', '2.0-refactored'),
                'GOOGLE_APPLICATION_CREDENTIALS': os.getenv('GOOGLE_APPLICATION_CREDENTIALS', '/app/credentials.json')
            })
            
            # Validate configuration
            self._validate_config()
            
            logging.info("‚úÖ Configuration loaded successfully")
            
        except Exception as e:
            logging.error(f"‚ùå Configuration error: {e}")
            raise
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value"""
        return self._config.get(key, default)
    
    def get_all(self) -> Dict[str, Any]:
        """Get all configuration"""
        return self._config.copy()
    
    def _validate_config(self):
        """Validate configuration values"""
        # Validate port range
        port = self._config.get('PORT')
        if not (1024 <= port <= 65535):
            # ‡∏ñ‡πâ‡∏≤‡∏û‡∏≠‡∏£‡πå‡∏ï‡∏ú‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô 8080 ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏∞‡πÄ‡∏ö‡∏¥‡∏î‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á
            self._config['PORT'] = 8080
            logging.warning(f"‚ö†Ô∏è Invalid port: {port}. Defaulting to 8080")
        
        # --- ‡∏õ‡∏¥‡∏î‡∏î‡πà‡∏≤‡∏ô‡∏ï‡∏£‡∏ß‡∏à Google Sheets ---
        # sheets_id = self._config.get('GOOGLE_SHEETS_ID')
        # if not sheets_id or len(sheets_id) < 20:
        #     raise ValueError("Invalid Google Sheets ID")
        
        # --- ‡∏õ‡∏¥‡∏î‡∏î‡πà‡∏≤‡∏ô‡∏ï‡∏£‡∏ß‡∏à LINE tokens (‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ß‡πâ‡∏ñ‡πâ‡∏≤‡∏°‡∏∂‡∏á‡πÉ‡∏™‡πà Token ‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡πâ‡∏ß) ---
        # line_token = self._config.get('LINE_CHANNEL_ACCESS_TOKEN')
        # if not line_token or len(line_token) < 50:
        #     raise ValueError("Invalid LINE Channel Access Token")
        
        logging.info("‚úÖ Configuration validation passed (Strict mode disabled)")
    
    def is_debug_mode(self) -> bool:
        """Check if debug mode is enabled"""
        return self._config.get('DEBUG', False)
    
    def get_binance_config(self) -> Dict[str, str]:
        """Get Binance API configuration"""
        return {
            'base_url': self._config['BINANCE_BASE_URL'],
            'timeout': 30,
            'rate_limit': 1200
        }
    
    def get_google_config(self) -> Dict[str, str]:
        """Get Google API configuration"""
        return {
            'sheets_id': self._config['GOOGLE_SHEETS_ID'],
            'credentials_path': self._config['GOOGLE_APPLICATION_CREDENTIALS']
        }
    
    def get_line_config(self) -> Dict[str, str]:
        """Get LINE Bot configuration"""
        return {
            'access_token': self._config['LINE_CHANNEL_ACCESS_TOKEN'],
            'secret': self._config['LINE_CHANNEL_SECRET'],
            'user_id': self._config.get('LINE_USER_ID')
        }
========================================
FILE: app/services/sheets_logger.py
========================================

"""
Google Sheets Integration for Trading Signal Logging - REFACTORED for v2.0
Simplified to use ConfigManager for configuration
FIXED: worksheet attribute error + Base64 credentials support
"""

import base64
import json
import logging
import os
from datetime import datetime
from typing import Any, Dict, List, Optional

try:
    import gspread
    from google.oauth2.service_account import Credentials
    SHEETS_AVAILABLE = True
except ImportError:
    SHEETS_AVAILABLE = False
    gspread = None
    Credentials = None

logger = logging.getLogger(__name__)


class SheetsLogger:
    """
    REFACTORED Google Sheets Logger for v2.0
    
    Main responsibilities:
    - Connect to Google Sheets using ConfigManager settings
    - Log trading signals and results
    - Track Trading Journal with Win/Loss tracking
    - Calculate Win Rate automatically
    - Generate trading statistics
    
    Uses ConfigManager for:
    - Google Sheets ID
    - Credentials path/content
    """

    def __init__(self, config: Dict):
        """
        Initialize SheetsLogger with ConfigManager config
        
        Args:
            config: Configuration from ConfigManager.get_google_config()
                   Expected keys: 'sheets_id', 'credentials_path'
        """
        # Configuration from ConfigManager
        self.credentials_path = config.get("credentials_path")
        self.spreadsheet_id = config.get("sheets_id")
        
        # Connection state
        self.gc = None                    # Google Sheets client
        self.spreadsheet = None           # Spreadsheet object
        self._cached_worksheet = None     # Current worksheet cache (FIXED)
        self._initialized = False         # Initialization status

        # Show configuration status
        logger.info("Initializing SheetsLogger v2.0...")
        logger.info(f"Credentials configured: {bool(self.credentials_path)}")
        logger.info(f"Spreadsheet ID configured: {bool(self.spreadsheet_id)}")

        # Check dependencies
        if not SHEETS_AVAILABLE:
            logger.warning("Google Sheets dependencies not installed")
            return

        # Check configuration
        if not self.credentials_path or not self.spreadsheet_id:
            logger.warning("Google Sheets credentials or spreadsheet ID not configured")
            logger.warning(f"   Credentials: {'available' if self.credentials_path else 'missing'}")
            logger.warning(f"   Spreadsheet ID: {'available' if self.spreadsheet_id else 'missing'}")
            return

        # Attempt connection
        try:
            self._initialize_connection()
            self._initialized = True
            logger.info("SheetsLogger v2.0 initialization completed successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Google Sheets connection: {e}")
            self._initialized = False

    @property
    def worksheet(self):
        """
        Property to safely access Trading_Journal worksheet
        Returns None if not initialized or worksheet not available
        """
        if not self._initialized or not self.spreadsheet:
            return None
        
        try:
            # Try to get cached worksheet first
            if self._cached_worksheet:
                return self._cached_worksheet
            
            # Get worksheet from spreadsheet
            worksheet = self.spreadsheet.worksheet("Trading_Journal")
            self._cached_worksheet = worksheet
            return worksheet
            
        except Exception as e:
            logger.error(f"Error accessing Trading_Journal worksheet: {e}")
            return None

    def _initialize_connection(self):
        """Initialize connection to Google Sheets"""
        if not SHEETS_AVAILABLE:
            return

        try:
            # Define OAuth permissions
            scope = [
                "https://spreadsheets.google.com/feeds",
                "https://www.googleapis.com/auth/drive",
            ]

            logger.info(f"Loading credentials type: {type(self.credentials_path)}")

            credentials_str = str(self.credentials_path).strip()

            # ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô JSON string ‡∏Å‡πà‡∏≠‡∏ô
            if credentials_str.startswith('{'):
                try:
                    logger.info("Loading credentials from JSON string")
                    creds_info = json.loads(credentials_str)
                    creds = Credentials.from_service_account_info(creds_info, scopes=scope)
                    logger.info("‚úÖ Successfully loaded credentials from JSON string")
                except json.JSONDecodeError as e:
                    logger.error(f"Invalid JSON in credentials: {e}")
                    raise
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå
            elif os.path.isfile(credentials_str):
                logger.info(f"Loading credentials from file: {credentials_str}")
                creds = Credentials.from_service_account_file(
                    credentials_str, scopes=scope
                )
                logger.info("‚úÖ Successfully loaded credentials from file")
            else:
                logger.error(f"Invalid credentials format (not JSON or file): {credentials_str[:100]}")
                raise ValueError(f"Invalid credentials format")

            # Create authorized client
            self.gc = gspread.authorize(creds)
            
            # Connect to spreadsheet
            self.spreadsheet = self.gc.open_by_key(self.spreadsheet_id)

            # Try to access or create the main worksheet
            try:
                self._cached_worksheet = self.spreadsheet.worksheet("Trading_Journal")
                logger.info("Connected to existing Trading_Journal worksheet")
            except gspread.WorksheetNotFound:
                # Create Trading_Journal worksheet if it doesn't exist
                headers = ["Date", "Symbol", "Signal", "Entry", "SL", "TP1", "TP2", "TP3", "Win/Loss", "Win Rate"]
                self._cached_worksheet = self.spreadsheet.add_worksheet(
                    title="Trading_Journal", rows=1000, cols=len(headers)
                )
                self._cached_worksheet.append_row(headers)
                logger.info("Created new Trading_Journal worksheet")

            logger.info("Google Sheets connection established")
            logger.info(f"Spreadsheet: {self.spreadsheet.title}")
            logger.info(f"Spreadsheet ID: {self.spreadsheet_id}")

        except Exception as e:
            logger.error(f"Google Sheets initialization error: {e}")
            logger.error(f"Credentials path: {self.credentials_path}")
            logger.error(f"Spreadsheet ID: {self.spreadsheet_id}")
            raise   

    def _ensure_worksheet_exists(self, worksheet_name: str, headers: List[str]) -> Optional[Any]:
        """Ensure worksheet exists with proper headers"""
        if not self.spreadsheet:
            logger.error("Spreadsheet not initialized")
            return None

        try:
            # Try to find existing worksheet
            try:
                worksheet = self.spreadsheet.worksheet(worksheet_name)
                logger.info(f"Found existing worksheet: {worksheet_name}")

                # Check headers
                existing_headers = worksheet.row_values(1)
                if not existing_headers or existing_headers != headers:
                    worksheet.clear()
                    worksheet.append_row(headers)
                    logger.info(f"Updated headers for worksheet: {worksheet_name}")

                return worksheet

            # Create new worksheet if not found
            except gspread.WorksheetNotFound:
                logger.info(f"Creating new worksheet: {worksheet_name}")
                worksheet = self.spreadsheet.add_worksheet(
                    title=worksheet_name, 
                    rows=1000,
                    cols=len(headers)
                )
                worksheet.append_row(headers)
                logger.info(f"Created new worksheet: {worksheet_name}")
                return worksheet

        except Exception as e:
            logger.error(f"Error ensuring worksheet exists: {e}")
            return None

    def _determine_signal_type(self, signals: Dict) -> str:
        """Determine signal type from signals dictionary"""
        # Strong signals (highest priority)
        if signals.get("strong_buy"):
            return "STRONG_BUY"
        elif signals.get("strong_short"):
            return "STRONG_SHORT"
        
        # Medium signals
        elif signals.get("medium_buy"):
            return "MEDIUM_BUY"
        elif signals.get("medium_short"):
            return "MEDIUM_SHORT"
        
        # Weak signals
        elif signals.get("weak_buy"):
            return "WEAK_BUY"
        elif signals.get("weak_short"):
            return "WEAK_SHORT"
        
        # Experimental signals
        elif signals.get("experimental_buy"):
            return "EXPERIMENTAL_BUY"
        elif signals.get("experimental_short"):
            return "EXPERIMENTAL_SHORT"
        
        # Basic signals (legacy support)
        elif signals.get("buy"):
            return "BUY"
        elif signals.get("short"):
            return "SHORT"
        elif signals.get("sell"):
            return "SELL"
        elif signals.get("cover"):
            return "COVER"
        
        # No signal
        else:
            return "NONE"

    def _has_tradeable_signal(self, signals: Dict) -> bool:
        """Check if signals contain tradeable signals"""
        return any([
            # Strong signals
            signals.get("strong_buy", False),
            signals.get("strong_short", False),
            
            # Medium signals  
            signals.get("medium_buy", False),
            signals.get("medium_short", False),
            
            # Weak signals
            signals.get("weak_buy", False),
            signals.get("weak_short", False),
            
            # Experimental signals
            signals.get("experimental_buy", False),
            signals.get("experimental_short", False),
            
            # Basic signals (legacy)
            signals.get("buy", False),
            signals.get("short", False)
        ])

    def _get_trade_direction(self, signals: Dict) -> Optional[str]:
        """Get trade direction from signals"""
        # LONG/BUY signals
        if any([
            signals.get("strong_buy", False),
            signals.get("medium_buy", False), 
            signals.get("weak_buy", False),
            signals.get("experimental_buy", False),
            signals.get("buy", False)
        ]):
            return "LONG"
        
        # SHORT signals
        elif any([
            signals.get("strong_short", False),
            signals.get("medium_short", False),
            signals.get("weak_short", False), 
            signals.get("experimental_short", False),
            signals.get("short", False)
        ]):
            return "SHORT"
        
        # No clear direction
        else:
            return None

    def log_signal(self, analysis: Dict) -> bool:
        """
        Log detailed signal information to Signals worksheet
        
        Args:
            analysis: Complete analysis from SignalDetector
            
        Returns:
            bool: True if logged successfully
        """
        # Check connection status
        if not self._initialized:
            logger.warning("SheetsLogger not properly initialized, skipping detailed signal log")
            return False
            
        if not self.spreadsheet:
            logger.warning("Google Sheets not initialized, skipping detailed signal log")
            return False

        try:
            symbol = analysis.get("symbol", "UNKNOWN")
            timeframe = analysis.get("timeframe", "UNKNOWN")
            logger.info(f"Attempting to log detailed signal: {symbol} ({timeframe})")
            
            # Define headers for detailed logging
            headers = [
                "Timestamp", "Symbol", "Timeframe", "Price", "Signal", "Recommendation",
                "Squeeze_Off", "Momentum", "MACD_Cross", "RSI_Value", "RSI_Level", 
                "Signal_Strength", "Entry_Price", "Stop_Loss", "TP1", "TP2", "TP3", "Risk_Reward"
            ]

            # Prepare worksheet
            logger.info("Ensuring Signals worksheet exists...")
            worksheet = self._ensure_worksheet_exists("Signals", headers)
            if not worksheet:
                logger.error("Failed to create/access Signals worksheet")
                return False

            # Extract data from analysis
            signals = analysis.get("signals", {})
            indicators = analysis.get("indicators", {})
            risk_levels = analysis.get("risk_levels", {})

            # Determine signal type
            signal_type = self._determine_signal_type(signals)
            logger.info(f"Signal type determined: {signal_type}")

            # Prepare row data
            try:
                row_data = [
                    analysis.get("timestamp", datetime.now().isoformat()),
                    symbol,
                    timeframe,
                    float(analysis.get("current_price", 0)),
                    signal_type,
                    analysis.get("recommendation", ""),
                    bool(indicators.get("squeeze", {}).get("squeeze_off", False)),
                    str(indicators.get("squeeze", {}).get("momentum_direction", "")),
                    str(indicators.get("macd", {}).get("cross_direction", "")),
                    float(indicators.get("rsi", {}).get("value", 0)),
                    str(indicators.get("rsi", {}).get("extreme_level", "")),
                    float(analysis.get("signal_strength", 0)),
                    float(risk_levels.get("entry_price", 0)),
                    float(risk_levels.get("stop_loss", 0)),
                    float(risk_levels.get("take_profit_1", 0)),
                    float(risk_levels.get("take_profit_2", 0)),
                    float(risk_levels.get("take_profit_3", 0)),
                    float(risk_levels.get("risk_reward_ratio", 0)),
                ]
                
                logger.info(f"Row data prepared: {len(row_data)} columns")
                
            except Exception as e:
                logger.error(f"Error preparing row data: {e}")
                return False

            # Append to worksheet
            logger.info("Appending row to worksheet...")
            worksheet.append_row(row_data)

            logger.info(f"Detailed signal logged successfully: {symbol} - {signal_type}")
            return True

        except Exception as e:
            logger.error(f"Error logging detailed signal to sheets: {e}")
            return False

    def log_trading_journal(self, analysis: Dict) -> bool:
        """
        Log tradeable signals to Trading_Journal worksheet
        
        Args:
            analysis: Analysis from SignalDetector
            
        Returns:
            bool: True if logged successfully
        """
        # Check connection status
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping trading journal log")
            return False

        try:
            signals = analysis.get("signals", {})
            
            # Check for tradeable signals
            if not self._has_tradeable_signal(signals):
                logger.debug(f"No tradeable signals found for {analysis.get('symbol', 'UNKNOWN')}")
                return False
            
            # Headers for Trading Journal
            headers = [
                "Date", "Symbol", "Signal", "Entry", "SL", 
                "TP1", "TP2", "TP3", "Win/Loss", "Win Rate"
            ]

            # Prepare worksheet (use main worksheet if available)
            worksheet = self.worksheet
            if not worksheet:
                worksheet = self._ensure_worksheet_exists("Trading_Journal", headers)
                if not worksheet:
                    logger.error("Failed to create/access Trading_Journal worksheet")
                    return False

            risk_levels = analysis.get("risk_levels", {})
            
            # Get trade direction
            trade_direction = self._get_trade_direction(signals)
            if not trade_direction:
                logger.debug(f"No clear trade direction for {analysis.get('symbol', 'UNKNOWN')}")
                return False
            
            # Check for duplicates before logging
            symbol = analysis.get("symbol", "")
            records = worksheet.get_all_records()
            today = datetime.now().strftime("%Y-%m-%d")

            for rec in records:
                if (rec.get("Date") == today and 
                    rec.get("Symbol") == symbol and 
                    rec.get("Signal") == trade_direction and
                    not rec.get("Win/Loss")):
                    logger.warning(f"Duplicate signal blocked: {symbol} {trade_direction}")
                    return False

            # Prepare row data
            row_data = [
                datetime.now().strftime("%Y-%m-%d"),
                analysis.get("symbol", ""),
                trade_direction,
                float(risk_levels.get("entry_price", 0)),
                float(risk_levels.get("stop_loss", 0)),
                float(risk_levels.get("take_profit_1", 0)),
                float(risk_levels.get("take_profit_2", 0)),
                float(risk_levels.get("take_profit_3", 0)),
                "",
                ""
            ]

            # Append to worksheet
            worksheet.append_row(row_data)
            logger.info(f"Trading journal logged: {analysis.get('symbol')} - {trade_direction} (Signal: {self._determine_signal_type(signals)})")
            return True

        except Exception as e:
            logger.error(f"Error logging trading journal: {e}")
            return False

    def log_tp_hit(self, position_data: Dict, tp_info: Dict) -> bool:
        """
        Log Take Profit hit to Google Sheets
        
        Args:
            position_data: Position information
            tp_info: TP hit information
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping TP hit log")
            return False

        try:
            symbol = position_data.get("symbol", "")
            entry_price = position_data.get("entry_price", 0)
            tp_price = tp_info.get("target_price", 0)
            current_price = tp_info.get("price", 0)
            
            # Determine which TP was hit from the price
            tp_levels = position_data.get("tp_levels", {})
            tp_level = "TP1"  # Default
            
            for tp_name, tp_target in tp_levels.items():
                if abs(tp_target - tp_price) < 0.001:  # Account for floating point precision
                    tp_level = tp_name
                    break
            
            return self.update_trading_result(symbol, entry_price, f"take_profit_{tp_level[-1]}", current_price)

        except Exception as e:
            logger.error(f"Error logging TP hit: {e}")
            return False

    def log_sl_hit(self, position_data: Dict, sl_info: Dict) -> bool:
        """
        Log Stop Loss hit to Google Sheets
        
        Args:
            position_data: Position information
            sl_info: SL hit information
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping SL hit log")
            return False

        try:
            symbol = position_data.get("symbol", "")
            entry_price = position_data.get("entry_price", 0)
            current_price = sl_info.get("price", 0)
            
            return self.update_trading_result(symbol, entry_price, "stop_loss", current_price)

        except Exception as e:
            logger.error(f"Error logging SL hit: {e}")
            return False

    def log_position_close(self, position_data: Dict) -> bool:
        """
        Log position closure to Google Sheets
        
        Args:
            position_data: Position information
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping position close log")
            return False

        try:
            symbol = position_data.get("symbol", "")
            entry_price = position_data.get("entry_price", 0)
            close_reason = position_data.get("close_reason", "MANUAL")
            
            # Determine if it's a win or loss based on close reason
            if close_reason in ["ALL_TP_HIT", "TP3_HIT"]:
                result_type = "WIN"
            elif close_reason == "SL_HIT":
                result_type = "LOSS"
            else:
                result_type = "MANUAL_CLOSE"
            
            # Update the trading journal
            return self._update_position_status(symbol, entry_price, result_type)

        except Exception as e:
            logger.error(f"Error logging position close: {e}")
            return False

    def update_trading_result(self, symbol: str, entry_price: float, triggered_level: str, triggered_price: float) -> bool:
        """
        Update trading result with TP/SL marks
        
        Args:
            symbol: Trading symbol
            entry_price: Entry price to match
            triggered_level: Level that was triggered (e.g., "take_profit_1", "stop_loss")
            triggered_price: Price at which level was triggered
            
        Returns:
            bool: True if updated successfully
        """
        if not self._initialized or not self.spreadsheet:
            logger.warning("SheetsLogger not initialized, skipping result update")
            return False

        try:
            # Access Trading Journal worksheet - FIXED: use safe access
            worksheet = self.worksheet
            if not worksheet:
                logger.error("Cannot access Trading_Journal worksheet")
                return False

            records = worksheet.get_all_records()

            # Find matching row
            for i, record in enumerate(records, start=2):  # start=2 because row 1 is header
                if (record.get("Symbol") == symbol and 
                    abs(float(record.get("Entry", 0)) - entry_price) < 0.001 and  # Account for floating point precision
                    not record.get("Win/Loss")):  # Not yet updated
                    
                    # Update based on triggered level
                    if triggered_level == "stop_loss":
                        # Mark SL column (column E = 5)
                        current_sl = worksheet.cell(i, 5).value
                        worksheet.update_cell(i, 5, f"‚ùå {current_sl}")
                        worksheet.update_cell(i, 9, "LOSS")  # Win/Loss column
                        logger.info(f"Updated LOSS: {symbol} hit SL at {triggered_price}")
                        
                    elif triggered_level.startswith("take_profit"):
                        # Mark appropriate TP column
                        if triggered_level == "take_profit_1":
                            col = 6  # TP1 column
                        elif triggered_level == "take_profit_2":
                            col = 7  # TP2 column
                        elif triggered_level == "take_profit_3":
                            col = 8  # TP3 column
                        else:
                            continue
                        
                        current_tp = worksheet.cell(i, col).value
                        worksheet.update_cell(i, col, f"‚úÖ {current_tp}")
                        worksheet.update_cell(i, 9, "WIN")  # Win/Loss column
                        logger.info(f"Updated WIN: {symbol} hit {triggered_level} at {triggered_price}")
                    
                    # Update Win Rate
                    self._update_win_rate(worksheet)
                    
                    logger.info(f"Trading result updated successfully: {symbol} - {triggered_level}")
                    return True
            
            logger.warning(f"No matching trade found for {symbol} at {entry_price}")
            return False

        except Exception as e:
            logger.error(f"Error updating trading result: {e}")
            return False

    def _update_position_status(self, symbol: str, entry_price: float, status: str) -> bool:
        """Update position status in trading journal"""
        try:
            worksheet = self.worksheet
            if not worksheet:
                logger.error("Cannot access Trading_Journal worksheet")
                return False

            records = worksheet.get_all_records()

            # Find matching row
            for i, record in enumerate(records, start=2):
                if (record.get("Symbol") == symbol and 
                    abs(float(record.get("Entry", 0)) - entry_price) < 0.001 and
                    not record.get("Win/Loss")):
                    
                    worksheet.update_cell(i, 9, status)  # Win/Loss column
                    self._update_win_rate(worksheet)
                    
                    logger.info(f"Updated position status: {symbol} - {status}")
                    return True
            
            return False

        except Exception as e:
            logger.error(f"Error updating position status: {e}")
            return False

    def _update_win_rate(self, worksheet):
        """Calculate and update Win Rate"""
        try:
            # Get all records
            records = worksheet.get_all_records()
            
            # Filter completed trades
            completed_trades = [r for r in records if r.get("Win/Loss") in ["WIN", "LOSS"]]
            
            if not completed_trades:
                logger.debug("No completed trades found for win rate calculation")
                return
            
            # Calculate Win Rate
            total = len(completed_trades)
            wins = len([r for r in completed_trades if r.get("Win/Loss") == "WIN"])
            win_rate = f"{round(wins / total * 100, 1)}%"
            
            logger.info(f"Win Rate calculated: {wins}/{total} = {win_rate}")
            
            # Update Win Rate in the last row with data
            for i in range(len(records), 1, -1):  # From bottom to top
                if records[i-2].get("Win/Loss"):  # i-2 because records start from 0
                    worksheet.update_cell(i, 10, win_rate)  # Win Rate column (column J = 10)
                    logger.info(f"Win Rate updated in row {i}: {win_rate}")
                    break
                    
        except Exception as e:
            logger.error(f"Error updating win rate: {e}")

    def test_connection(self) -> bool:
        """Test Google Sheets connection"""
        try:
            if not self.spreadsheet:
                logger.error("Spreadsheet object is None")
                return False

            title = self.spreadsheet.title
            worksheet_count = len(self.spreadsheet.worksheets())
            
            logger.info(f"Google Sheets connection test successful!")
            logger.info(f"  Spreadsheet: {title}")
            logger.info(f"  Worksheets: {worksheet_count}")
            logger.info(f"  URL: https://docs.google.com/spreadsheets/d/{self.spreadsheet_id}")
            
            return True

        except Exception as e:
            logger.error(f"Google Sheets connection test failed: {e}")
            return False

    def get_trading_statistics(self, days: int = 30) -> Dict:
        """
        Generate trading statistics
        
        Args:
            days: Number of days to analyze (simplified - uses all data for now)
            
        Returns:
            Dict with trading statistics
        """
        if not self._initialized or not self.spreadsheet:
            return {}

        try:
            worksheet = self.worksheet
            if not worksheet:
                return {}

            records = worksheet.get_all_records()
            
            # Filter completed trades (simplified - use all for now)
            completed_trades = [r for r in records if r.get("Win/Loss") in ["WIN", "LOSS"]]
            
            if not completed_trades:
                return {"total_trades": 0, "win_rate": 0, "total_pnl": 0}
            
            # Calculate statistics
            total_trades = len(completed_trades)
            wins = len([r for r in completed_trades if r.get("Win/Loss") == "WIN"])
            win_rate = round(wins / total_trades * 100, 1) if total_trades > 0 else 0
            
            return {
                "total_trades": total_trades,
                "wins": wins,
                "losses": total_trades - wins,
                "win_rate": win_rate,
                "total_pnl": 0,  # TODO: Calculate from actual prices
                "best_performer": "",
                "worst_performer": "",
                "version": "2.0-refactored"
            }
            
        except Exception as e:
            logger.error(f"Error getting trading statistics: {e}")
            return {}

    def log_daily_summary(self, summary_data: Dict) -> bool:
        """
        Log daily summary to Google Sheets
        
        Args:
            summary_data: Summary data dictionary
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            return False

        try:
            headers = [
                "Date", "Total_Signals", "Active_Positions", "Closed_Positions", 
                "Total_PnL", "Win_Rate", "Best_Performer", "Worst_Performer", "Version"
            ]

            worksheet = self._ensure_worksheet_exists("Daily_Summary", headers)
            if not worksheet:
                return False

            row_data = [
                summary_data.get("date", ""),
                summary_data.get("total_signals", 0),
                summary_data.get("active_positions", 0),
                summary_data.get("closed_positions", 0),
                summary_data.get("total_pnl", 0),
                summary_data.get("win_rate", 0),
                summary_data.get("best_performer", ""),
                summary_data.get("worst_performer", ""),
                summary_data.get("version", "2.0-refactored"),
            ]

            worksheet.append_row(row_data)
            logger.info(f"Daily summary logged for {summary_data.get('date', 'today')}")
            return True

        except Exception as e:
            logger.error(f"Error logging daily summary: {e}")
            return False

    def log_position_update(self, update_data: Dict) -> bool:
        """
        Log position update information
        
        Args:
            update_data: Position update data
            
        Returns:
            bool: True if logged successfully
        """
        if not self._initialized or not self.spreadsheet:
            return False

        try:
            # Extract position and update information
            position = update_data.get("position", {})
            updates = update_data.get("updates", {})
            
            if not position or not updates:
                return False
            
            symbol = position.get("symbol", "")
            entry_price = position.get("entry_price", 0)
            
            # Process TP hits
            for tp_level in ['TP1', 'TP2', 'TP3']:
                tp_key = f'{tp_level}_hit'
                if updates.get(tp_key, {}).get('hit', False):
                    tp_info = updates[tp_key]
                    self.log_tp_hit(position, tp_info)
            
            # Process SL hits
            if updates.get('sl_hit', {}).get('hit', False):
                sl_info = updates['sl_hit']
                self.log_sl_hit(position, sl_info)
            
            # Process position closure
            if updates.get('position_closed', False):
                self.log_position_close(position)
            
            return True

        except Exception as e:
            logger.error(f"Error logging position update: {e}")

            return False

    def shutdown(self):
        """Shutdown SheetsLogger"""
        try:
            logger.info("Shutting down SheetsLogger v2.0...")
            # Clean up any resources if needed
            self._cached_worksheet = None
            logger.info("SheetsLogger shutdown complete")
        except Exception as e:
            logger.error(f"Error during SheetsLogger shutdown: {e}")
========================================
FILE: app/utils/core_utils.py
========================================

import json
import os
import logging
from typing import Dict, Any, Optional
from datetime import datetime
from functools import wraps

class JSONManager:
    """Centralized JSON file operations"""
    
    @staticmethod
    def ensure_directory(file_path: str):
        """Ensure directory exists for file path"""
        directory = os.path.dirname(file_path)
        if directory:
            os.makedirs(directory, exist_ok=True)
    
    @staticmethod
    def save_json(data: Dict[Any, Any], file_path: str) -> bool:
        """Safe JSON save with backup"""
        try:
            JSONManager.ensure_directory(file_path)
            
            # Create backup if file exists
            if os.path.exists(file_path):
                backup_path = f"{file_path}.backup"
                os.rename(file_path, backup_path)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
            return True
        except Exception as e:
            logging.error(f"Error saving JSON {file_path}: {e}")
            return False
    
    @staticmethod
    def load_json(file_path: str, default: Dict[Any, Any] = None) -> Dict[Any, Any]:
        """Safe JSON load with fallback"""
        if default is None:
            default = {}
            
        if not os.path.exists(file_path):
            return default
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logging.error(f"Error loading JSON {file_path}: {e}")
            # Try backup
            backup_path = f"{file_path}.backup"
            if os.path.exists(backup_path):
                try:
                    with open(backup_path, 'r', encoding='utf-8') as f:
                        return json.load(f)
                except:
                    pass
            return default

class ErrorHandler:
    """Centralized error handling patterns"""
    
    @staticmethod
    def api_error_handler(func):
        """Decorator for API error handling"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logging.error(f"API Error in {func.__name__}: {e}")
                return {"error": str(e), "timestamp": datetime.now().isoformat()}
        return wrapper
    
    @staticmethod
    def service_error_handler(service_name: str):
        """Decorator factory for service error handling"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logging.error(f"Service Error [{service_name}] in {func.__name__}: {e}")
                    return None
            return wrapper
        return decorator

class ConfigValidator:
    """Configuration validation utilities"""
    
    @staticmethod
    def validate_required_env_vars(required_vars: list) -> Dict[str, str]:
        """Validate and return required environment variables"""
        config = {}
        missing_vars = []
        
        for var in required_vars:
            value = os.getenv(var)
            if not value:
                missing_vars.append(var)
            else:
                config[var] = value
        
        if missing_vars:
            raise EnvironmentError(f"Missing required environment variables: {missing_vars}")
        
        return config
========================================
FILE: app/utils/data_types.py
========================================

import numpy as np
from typing import Any, Dict, List
import pandas as pd

class DataConverter:
    """Centralized data type conversion utilities"""
    
    @staticmethod
    def convert_numpy_types(data: Any) -> Any:
        """Convert numpy types to Python native types"""
        if isinstance(data, np.integer):
            return int(data)
        elif isinstance(data, np.floating):
            return float(data)
        elif isinstance(data, np.bool_):
            return bool(data)
        elif isinstance(data, np.ndarray):
            return data.tolist()
        elif isinstance(data, dict):
            return {k: DataConverter.convert_numpy_types(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [DataConverter.convert_numpy_types(item) for item in data]
        else:
            return data
    
    @staticmethod
    def validate_dataframe(df: pd.DataFrame, min_rows: int = 50) -> bool:
        """Validate DataFrame for technical analysis"""
        if df is None or df.empty:
            return False
        
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in df.columns for col in required_columns):
            return False
            
        if len(df) < min_rows:
            return False
            
        # Check for null values
        if df[required_columns].isnull().any().any():
            return False
            
        return True
    
    @staticmethod
    def sanitize_signal_data(signal_data: Dict) -> Dict:
        """Sanitize signal data for JSON serialization"""
        sanitized = {}
        
        for key, value in signal_data.items():
            if isinstance(value, (pd.Timestamp, np.datetime64)):
                sanitized[key] = value.isoformat() if hasattr(value, 'isoformat') else str(value)
            else:
                sanitized[key] = DataConverter.convert_numpy_types(value)
        
        return sanitized
    
    @staticmethod
    def validate_price_data(price: Any) -> bool:
        """Validate price data"""
        if price is None:
            return False
        
        try:
            price_float = float(price)
            return price_float > 0
        except (ValueError, TypeError):
            return False
    
    @staticmethod
    def format_percentage(value: float, decimals: int = 2) -> str:
        """Format percentage with proper decimals"""
        try:
            return f"{round(float(value), decimals):.{decimals}f}%"
        except (ValueError, TypeError):
            return "0.00%"
========================================
FILE: requirements.txt
========================================

# Core Flask Application
Flask==3.0.3
requests==2.32.3
python-dotenv==1.0.1

# Job Scheduling
APScheduler==3.10.4

# Google Services Integration
gspread==6.1.4
google-auth==2.37.0
google-auth-oauthlib==1.2.1
google-auth-httplib2==0.2.0
google-api-python-client==2.158.0

# LINE Bot Integration
line-bot-sdk==3.13.0

# Technical Analysis
ta==0.11.0
pandas==2.2.3
numpy==2.0.2

# Data Processing
pytz==2024.2

# Development & Testing
pytest==8.3.4
pytest-cov==6.0.0

# Web Server
click==8.1.8
gunicorn==22.0.0
websocket-client==1.7.0
websocket-client==1.7.0

========================================
FILE: Dockerfile
========================================

FROM python:3.11-slim

WORKDIR /app

# Copy requirements first
COPY requirements.txt .

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Create directories
RUN mkdir -p data/logs storage

# Railway inject PORT ‡πÄ‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á set ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
EXPOSE 8080

CMD ["python", "-m", "app.main"]
========================================
FILE: .env.example
========================================


========================================
FILE: version.txt
========================================

24